---
title: PauseAI-Vorschlag
description: Einführung einer vorübergehenden Pause bei der Entwicklung der leistungsfähigsten allgemeinen KI-Systeme.
---

**Einführung einer vorübergehenden Pause bei der Entwicklung der leistungsfähigsten allgemeinen KI-Systeme**, bis wir wissen, wie wir sie sicher entwickeln und unter demokratischer Kontrolle halten können.

_Version: 4. Februar 2025_

Einzelne Länder können und sollten diese Maßnahme _sofort_ umsetzen.
Insbesondere die USA (oder Kalifornien im Speziellen) sollten eine Pause einlegen, da sie Heimat fast aller führenden KI-Unternehmen sind.
Viele Wissenschaftler und Branchenführer [stimmen darin überein, dass eine Pause notwendig ist](https://futureoflife.org/open-letter/pause-giant-ai-experiments/), und die (US-amerikanische) Öffentlichkeit unterstützt eine Pause ebenfalls stark ([64%](https://www.campaignforaisafety.org/usa-ai-x-risk-perception-tracker/) - [69%](https://today.yougov.com/topics/technology/survey-results/daily/2023/04/03/ad825/2)).

Allerdings können wir nicht erwarten, dass Länder oder Unternehmen ihre Wettbewerbsvorteile riskieren, indem sie KI-Trainingsläufe für längere Zeit pausieren, wenn andere Länder oder Unternehmen dies nicht auch tun.
Deshalb benötigen wir eine **globale Pause**.

Lassen Sie uns tiefer in die Details eintauchen, um dorthin zu gelangen.

## Auf dem Weg zu einem Vertrag {#getting-to-a-treaty}

Ein internationales Abkommen wird typischerweise durch einen Gipfel etabliert, auf dem die Führer der Länder zusammenkommen, um das Thema zu diskutieren und eine Entscheidung zu treffen.
Wir hatten bereits [drei KI-Sicherheitsgipfel](/summit).

Das primäre Ziel dieser Gipfel sollte ein **Vertrag** sein.
Bisher waren die Gipfel jedoch nicht effektiv darin, etwas rechtlich Bindendes zu produzieren.
Und der Vertragsaufbau neigt dazu, langsam und anfällig für Vetos zu sein.
Wir [haben möglicherweise nicht die Zeit](/urgency), auf traditionelle Vertragsverhandlungsprozesse zu warten.

Also benötigen wir einen neuen **Vertragsverhandlungsprozess**:

- Die Beteiligung sowohl der **USA als auch Chinas** ist entscheidend.
- Er muss **unempfindlich gegen Vetos** durch ein einzelnes Land sein.
- Er muss **schnell** sein. Normale Vertragsverhandlungsprozesse dauern Jahre, und wir [haben möglicherweise nicht diese Zeit](/urgency).
- Der Umfang dieses Vertragsverhandlungsprozesses ist beispiellos, und er benötigt die Zustimmung aller Länder.

Der Vertrag selbst sollte die folgenden **Maßnahmen** enthalten:

### Vertragsmaßnahmen {#treaty-measures}

- **Einrichtung einer internationalen KI-Sicherheitsagentur**, ähnlich der IAEO. Diese Agentur wird verantwortlich sein für:
  - Die Erteilung von Genehmigungen für _Einsätze_. Dies wird auch Red-Teaming-/Modellevaluierungen umfassen.
  - Die Erteilung von Genehmigungen für _neue Trainingsläufe_ von KI-Modellen oberhalb einer bestimmten Größe (z.B. 1 Milliarde Parameter).
  - Regelmäßige Treffen, um den Fortschritt der KI-Sicherheitsforschung zu diskutieren.

- **Nur die Entwicklung von allgemeinen KI-Systemen erlauben, wenn ihre Sicherheit garantiert werden kann**.
  - Mit allgemeinen KI-Modellen meinen wir Modelle, die entweder 1) größer als 10^12 Parameter sind, 2) mehr als 10^25 FLOPs für das Training verwenden oder 3) Fähigkeiten haben, die eine Punktzahl von 86% auf dem MMLU-Benchmark überschreiten. Beachten Sie, dass dies nicht _spezialisierte_ KI-Systeme wie Bilderkennung für die Krebsdiagnose betrifft.
  - Es ist möglich, dass das KI-Alignmentsproblem _nie gelöst wird_ - es könnte unlösbar sein. In diesem Fall sollten wir niemals die Entwicklung solcher Systeme erlauben.
  - **Überprüfen**, dass diese gefährlichen Trainingsläufe nicht stattfinden. Dies kann auf [zahlreiche Weise](https://arxiv.org/abs/2408.16074) erfolgen: [GPU-Tracking](https://arxiv.org/abs/2303.11341), Anreize für Whistleblower, Energieüberwachung, Inspektionen von Rechenzentren, Finanzanalyse, Inspektionen von Halbleiterherstellungsanlagen, Inspektionen von KI-Entwicklern, Chip-Ortung und chipbasierte Berichterstattung. Die [KI-Chip-Lieferkette](https://www.governance.ai/post/computing-power-and-the-governance-of-ai) ist stark zentralisiert, was eine globale Überwachung ermöglicht.
  - Erfordern [Überwachung während der Trainingsläufe](https://www.alignmentforum.org/posts/Zfk6faYvcf5Ht7xDx/compute-thresholds-proposed-rules-to-mitigate-risk-of-a-lab).
  - Selbst wenn wir kontrollierbare, sichere KI bauen können, sollten wir eine solche Technologie nur mit **starker demokratischer Kontrolle** bauen und einsetzen. Eine Superintelligenz ist zu mächtig, um von einem einzelnen Unternehmen oder Land kontrolliert zu werden.

- **Nur den Einsatz von Modellen erlauben, nachdem keine [gefährlichen Fähigkeiten](/dangerous-capabilities) vorhanden sind**. (Vor-Einsatz-Evaluierung)
  - Wir werden Standards und unabhängige Red-Teaming-Verfahren benötigen, um zu bestimmen, ob ein Modell gefährliche Fähigkeiten hat.
  - Die Liste der gefährlichen Fähigkeiten kann sich im Laufe der Zeit ändern, wenn die KI-Fähigkeiten wachsen.
  - Beachten Sie, dass die ausschließliche Verwendung von Modellevaluierungen [nicht ausreichend ist](/4-levels-of-ai-regulation).

Die Einführung einer Pause _kann_ nach hinten losgehen, wenn sie nicht ordnungsgemäß durchgeführt wird.
Lesen Sie mehr darüber, [wie diese Risiken minimiert werden können](/mitigating-pause-failures).

Für weitere Details darüber, wie die KI-Chip-Lieferkette für eine globale Überwachung genutzt werden kann, lesen Sie [Building the Pause Button](/building-the-pause-button).

## Weitere Maßnahmen, die den Fortschritt effektiv verlangsamen {#other-measures-that-effectively-slow-down}

- **Verbot des Trainings von KI-Systemen mit urheberrechtlich geschütztem Material**. Dies hilft bei Urheberrechtsproblemen, verlangsamt die wachsende Ungleichheit und verlangsamt den Fortschritt in Richtung Superintelligenz.
- **KI-Modellersteller für kriminelle Handlungen haftbar machen**, die mit ihren KI-Systemen begangen werden. Dies gibt Modellerstellern mehr Anreize, sicherzustellen, dass ihre Modelle sicher sind.

## Langfristige Politik {#long-term-policy}

Zum Zeitpunkt des Schreibens kostet das Training eines GPT-3-Modells Millionen von Dollar.
Dies macht es sehr schwierig, solche Modelle zu trainieren, und erleichtert die Kontrolle des Trainings durch GPU-Tracking.
Allerdings sinken die Kosten für das Training eines Modells aufgrund von Hardware-Verbesserungen und neuen Trainingsalgorithmen exponentiell.

Es wird einen Punkt geben, an dem potenziell superintelligente KI-Modelle für einige Tausend Dollar oder weniger trainiert werden können, vielleicht sogar auf Consumer-Hardware.
Wir müssen darauf vorbereitet sein.
Wir sollten die folgenden Politiken in Betracht ziehen:

- **Begrenzung der Veröffentlichung von Trainingsalgorithmen / Laufzeitverbesserungen**. Manchmal wird ein neuer Algorithmus veröffentlicht, der das Training viel effizienter macht. Die Transformer-Architektur ermöglichte beispielsweise nahezu alle jüngsten Fortschritte in der KI. Diese Art von Fähigkeits-Sprüngen kann jederzeit passieren, und wir sollten die Veröffentlichung solcher Algorithmen begrenzen, um das Risiko eines plötzlichen Fähigkeits-Sprungs zu minimieren. Es gibt auch Innovationen, die [dezentralisierte Trainingsläufe](https://www.primeintellect.ai/blog/opendiloco) ermöglichen. Ebenso könnten einige Laufzeitinnovationen drastisch ändern, was mit bestehenden Modellen möglich ist. Ein Verbot der Veröffentlichung solcher Algorithmen kann durch ähnliche Mittel wie das Verbot anderer Formen von Informationen, wie illegale pornografische Medien, umgesetzt werden.
- **Begrenzung der Fähigkeitsfortschritte von Rechenressourcen**. Wenn das Training einer Superintelligenz auf Consumer-Hardware möglich wird, sind wir in Schwierigkeiten. Wir sollten die Fähigkeitsfortschritte von Hardware (z.B. durch Beschränkungen der Lithografie, Chip-Design und neuartige Rechenparadigmen wie photonische Chips und Quantencomputing) begrenzen.

## Helfen Sie uns, dies zu erreichen {#help-us-achieve-this}

[Treten Sie](/join) der Bewegung bei, um zusammenzuarbeiten oder [ergreifen Sie Maßnahmen](/action) auf eigene Faust!
