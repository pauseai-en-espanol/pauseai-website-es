---
title: Die schwierige Psychologie des existenziellen Risikos
description: Über das Ende der Welt nachzudenken ist schwer.
---

Die meisten Menschen reagieren anfangs auf das Thema des existenziellen Risikos durch künstliche Intelligenz mit einer Mischung aus Spott, Leugnung und Unglauben.
Die Angst setzt oft erst nach langer Zeit des Nachdenkens ein.

Die Psychologie des existenziellen Risikos ist ein Thema, das im Vergleich zu den technischen Aspekten der KI-Sicherheit selten diskutiert wird.
Es könnte jedoch genauso wichtig sein.
Schließlich können wir nichts dagegen tun, wenn wir die Menschen nicht dazu bringen können, das Thema ernst zu nehmen und entsprechend zu handeln.

Es ist schwer, das Thema **anzusprechen**, schwer zu **glauben**, schwer zu **verstehen** und schwer, **darauf zu reagieren**.
Ein besseres Verständnis dafür, warum diese Dinge so schwierig sind, kann uns helfen, überzeugender, effektiver und empathischer zu sein.

## Schwierig anzusprechen {#difficult-to-bring-up}

Das existenzielle Risiko ist ein schwieriges Thema, das in Gesprächen anzusprechen ist, insbesondere wenn man Politiker ist.
Die Menschen könnten denken, dass man verrückt ist, und man könnte sich nicht wohl dabei fühlen, über dieses technisch komplexe Thema zu sprechen.

### Angst vor Spott {#fear-of-being-ridiculed}

Die erste Reaktion auf das existenzielle Risiko ist oft, es einfach abzutun.
Wir haben auch gesehen, dass dies im Weißen Haus passiert ist, als das existenzielle Risiko zum ersten Mal angesprochen wurde.
Dies macht es wiederum schwieriger, das Thema erneut anzusprechen, da andere befürchten, verspottet zu werden, wenn sie es ansprechen.

Profis können befürchten, dass ihre Reputation geschädigt wird, wenn sie ihre Bedenken teilen.

> "Es war fast gefährlich aus karrieretechnischer Sicht, zuzugeben, dass man besorgt war", - [Jeff Clune sagte](https://www.theglobeandmail.com/business/article-i-hope-im-wrong-why-some-experts-see-doom-in-ai/)

Das Eintreten für vernünftige politische Maßnahmen (wie eine Pause) kann als "extremistisch" oder "alarmistisch" angesehen werden, was die Glaubwürdigkeit oder den Ruf senken kann.

### Angst, als Rassist/Sektenmitglied/Verschwörungstheoretiker bezeichnet zu werden {#fear-of-being-called-racistcultistconspiracy-theorist}

In den letzten Monaten sind verschiedene Verschwörungstheorien aufgetaucht.
Einige Personen haben behauptet, dass [alle KI-Sicherheitsleute Rassisten sind](https://medium.com/%2540emilymenonbender/talking-about-a-schism-is-ahistorical-3c454a77220f) und dass [KI-Sicherheit eine Sekte ist](https://www.cnbc.com/2023/06/06/ai-doomers-are-a-cult-heres-the-real-threat-says-marc-andreessen.html).
Einige haben behauptet, dass KI-"Doomer" Teil einer [Verschwörung von Big Tech sind, um KI zu "hype"]](https://www.latimes.com/business/technology/story/2023-03-31/column-afraid-of-ai-the-startups-selling-it-want-you-to-be).
Diese lächerlichen Anschuldigungen können besorgte Menschen dazu bringen, ihre Bedenken nicht zu teilen.

Es ist jedoch wichtig, sich daran zu erinnern, dass diese Anschuldigungen das Ergebnis von Angst und Leugnung sein können (siehe unten).
Die Anerkennung der Gefahren von KI ist beängstigend, und es kann einfacher sein, den Boten abzulehnen, als die Botschaft zu internalisieren.

### Ein komplexes Thema, über das zu diskutieren ist {#a-complex-topic-to-argue-about}

Die Menschen sprechen gerne über Dinge, über die sie Bescheid wissen.
Die technische Schwierigkeit der KI-Sicherheit macht es zu einem einschüchternden Thema für die meisten Menschen.
Es dauert Zeit und Mühe, die Argumente zu verstehen.
Als Politiker möchte man nicht dabei erwischt werden, etwas Falsches zu sagen, also könnte man das Thema ganz vermeiden.

## Schwierig zu glauben {#difficult-to-believe}

Selbst wenn es eine Diskussion über das existenzielle Risiko gibt, ist es schwierig, die Menschen davon zu überzeugen, dass es ein reales Problem ist.
Es gibt verschiedene Gründe, warum die meisten Menschen die Idee sofort ablehnen.

### Normalitätsbias {#normalcy-bias}

Wir alle kennen die Bilder von Katastrophen in Filmen, oder?
Die Menschen schreien und rennen in Panik.
Es stellt sich jedoch heraus, dass das Gegenteil oft wahr ist: etwa 80% der Menschen zeigen Symptome von [Normalitätsbias](https://de.wikipedia.org/wiki/Normalit%C3%A4tsbias) während Katastrophen: Sie suchen nicht nach Schutz während eines Tornados, ignorieren Regierungs warnungen und schütteln weiter Hände in den frühen COVID-Tagen.
Der Normalitätsbias beschreibt unsere Tendenz, die Möglichkeit einer Katastrophe zu unterschätzen und zu glauben, dass das Leben weiterhin normal verläuft, selbst angesichts erheblicher Bedrohungen oder Krisen.
Dies passiert [derzeit mit KI](https://lethalintelligence.ai/post/category/warning-signs/).

> Die Menschen zögern, nach Meinungen zu fragen, weil sie hören wollen, dass alles in Ordnung ist. Sie werden weiter fragen und zögern, bis sie die Antwort erhalten, die sie hören wollen.

> Während des 11. Septembers zum Beispiel betrug die durchschnittliche Wartezeit unter den Überlebenden, um die Türme zu evakuieren, 6 Minuten, wobei einige bis zu einer halben Stunde warteten, um zu gehen. Etwa 1000 Menschen nahmen sich sogar die Zeit, ihre Computer herunterzufahren und andere Büroarbeiten zu erledigen, eine Strategie, um während einer unbekannten Situation weiterhin normale Aktivitäten auszuführen.

_Aus ["Die gefrorene Ruhe des Normalitätsbias"](https://gizmodo.com/the-frozen-calm-of-normality-bias-486764924)_

Ein weiteres Beispiel dafür ist die Challenger-Raumfähren-Katastrophe im Jahr 1986.
Roger Boisjoly war ein Ingenieur, der vorhersagte, dass sie explodieren würde, aber keiner seiner Manager wollte glauben, dass es möglich war:

> Wir wussten alle, dass die Dichtungen versagen würden, wenn die Raumfähre explodieren würde.
> Ich kämpfte wie verrückt, um den Start zu stoppen. Ich bin so zerrissen, dass ich kaum darüber sprechen kann, selbst jetzt.
> Wir sprachen mit den richtigen Leuten, wir sprachen mit den Leuten, die die Macht hatten, den Start zu stoppen.

_Aus ["Erinnerungen an Roger Boisjoly"](https://www.npr.org/sections/thetwo-way/2012/02/06/146490064/remembering-roger-boisjoly-he-tried-to-stop-shuttle-challenger-launch)_

Eine Erklärung dafür, warum unser Gehirn sich weigert zu glauben, dass Gefahr droht, ist die kognitive Dissonanz.

### Kognitive Dissonanz {#cognitive-dissonance}

Wenn wir mit neuen Informationen konfrontiert werden, versucht unser Gehirn, sie mit dem zu vereinbaren, was wir bereits wissen.
Ideen, die bereits mit unseren bestehenden Überzeugungen übereinstimmen, werden leicht in unser Weltbild integriert.
Ideen, die zu unterschiedlich von dem sind, was wir bereits glauben, verursachen _kognitive Dissonanz_ - wir fühlen uns unwohl und versuchen, die Ideen abzulehnen oder alternative Erklärungen für das zu finden, was wir hören.

Viele Überzeugungen, die die meisten Menschen haben, werden durch die Idee des existenziellen Risikos in Frage gestellt:

- Die Technologie ist da, um uns zu dienen und kann leicht kontrolliert werden
- Es gibt intelligente Menschen, die dafür sorgen, dass alles in Ordnung ist
- Ich werde wahrscheinlich alt werden, und meine Kinder auch

Viele dieser Gedanken werden durch die Idee in Frage gestellt, dass KI ein existenzielles Risiko darstellt.
Unser Gehirn sucht nach alternativen Erklärungen dafür, warum Wissenschaftler [vor diesem Risiko warnen](/quotes):

- Sie werden von Big Tech bezahlt
- Sie sind Teil einer Verschwörung oder Sekte
- Sie wollen nur Aufmerksamkeit oder Macht

Die Internalisierung, dass _Wissenschaftler uns warnen, weil sie glauben, dass wir in Gefahr sind_, steht im Widerspruch zu unseren bestehenden Überzeugungen und verursacht zu viel kognitive Dissonanz.

### Das Ende der Welt ist noch nie passiert {#the-end-of-the-world-has-never-happened}

Sehen ist glauben (siehe: [_Verfügbarkeitsheuristik_](https://de.wikipedia.org/wiki/Verf%C3%BCgbarkeitsheuristik)).
Das ist ein Problem für das Aussterberisiko, weil wir es nie sehen werden, bevor es zu spät ist.

Auf der anderen Seite haben wir tonnenweise Beweise für das Gegenteil.
Das Ende der Zeiten wurde von vielen Menschen vorhergesagt, und jeder einzelne von ihnen hat sich bisher geirrt.

Wenn also Menschen von existenziellem Risiko hören, denken sie, dass es nur eine weitere dieser Weltuntergangs-Kult-Vorhersagen ist.
Versuchen Sie, diese Sichtweise zu verstehen und seien Sie nicht zu hart zu Menschen, die so denken.
Sie haben wahrscheinlich nicht die gleichen Informationen wie Sie.

### Wir möchten glauben, dass wir besonders sind {#we-like-to-think-that-we-are-special}

Sowohl auf kollektiver als auch auf individueller Ebene möchten wir glauben, dass wir besonders sind.

Auf kollektiver Ebene möchten wir die Menschen als etwas sehr Unterschiedliches von Tieren sehen - Darwins Idee, dass wir von Affen abstammen, war für die meisten fast undenkbar.
Die meisten Religionen haben Geschichten über den Himmel oder die Wiedergeburt, in denen die Menschen (oder zumindest die Gläubigen) auf irgendeine Weise ewig leben werden.
Die Idee, dass die Menschheit eines Tages nicht mehr existieren könnte, ist sehr verstörend und schwer zu internalisieren.
Wir möchten glauben, dass wir _Plot-Panzer_ haben - dass wir die Hauptfiguren in einer Geschichte sind und dass die Geschichte ein glückliches Ende haben wird.
Die Menschen können es rational betrachten, aber sie werden es nicht _fühlen_.
Ein Video von Robert Miles mit dem Titel ["Es gibt keine Regel, die sagt, dass wir es schaffen werden"](https://www.youtube.com/watch?v=JD_iA7imAPs) erklärt dies sehr gut.

Auf individueller Ebene sind wir stolz auf unsere einzigartigen intellektuellen Fähigkeiten.
Viele wollten nie glauben, dass ein Computer eines Tages in der Lage sein könnte, Kunst zu schaffen, Bücher zu schreiben oder uns sogar beim Schach zu schlagen.
Der Gedanke, dass unsere eigene Intelligenz nur ein Produkt der Evolution ist und dass sie von einer Maschine repliziert werden kann, ist etwas, das viele Menschen schwer akzeptieren können.
Dies macht es schwierig zu akzeptieren, dass ein Computer intelligenter sein könnte als wir.

### Fiktion hat uns konditioniert, ein glückliches Ende zu erwarten {#fiction-has-conditioned-us-to-expect-a-happy-ending}

Die meisten Dinge, die wir über existenzielles Risiko wissen, stammen aus der Fiktion.
Dies hilft wahrscheinlich nicht, weil fiktive Geschichten nicht geschrieben werden, um realistisch zu sein: Sie werden geschrieben, um unterhaltsam zu sein.

In der Fiktion gibt es oft einen Helden, Konflikt, Hoffnung und schließlich ein glückliches Ende.
Wir sind konditioniert, einen Kampf zu erwarten, den wir gewinnen können.
In Science-Fiction werden Computer oft sehr anthropomorph dargestellt - als böse, als menschlich, als ihre Ziele ändernd.
All dies passt nicht zu dem, worüber KI-Sicherheitsexperten besorgt sind.

Und in den meisten Geschichten gewinnt der Held.
Der Computer macht einen dummen Fehler, und der Held findet einen Weg, die Sache zu überlisten, die viel intelligenter sein soll.
Der Held ist durch _Plot-Panzer_ geschützt.
In realistischeren KI-Untergangsszenarien gibt es keinen Helden, keinen Plot-Panzer, keinen Kampf, kein menschliches Überlisten einer Superintelligenz und kein glückliches Ende.

### Der Fortschritt war immer (meistens) gut {#progress-has-always-been-mostly-good}

Viele der Technologien, die in unsere Gesellschaft eingeführt wurden, waren größtenteils vorteilhaft für die Menschheit.
Wir haben Krankheiten geheilt, unsere Lebenserwartung erhöht und unser Leben komfortabler gemacht.
Und jedes Mal, wenn wir dies getan haben, gab es Menschen, die sich diesen Innovationen widersetzten und vor den Gefahren warnten.
Die Ludditen zerstörten die Maschinen, die ihre Arbeitsplätze nahmen, und die Menschen hatten Angst vor den ersten Zügen und Autos.
Diese Menschen haben sich immer geirrt.

### Wir möchten nicht an unseren Tod denken {#we-dont-like-to-think-about-our-death}

Der menschliche Geist mag keine schlechten Nachrichten, und er hat verschiedene Bewältigungsmechanismen, um damit umzugehen.
Die wichtigsten, wenn es um existenzielles Risiko geht, sind [Verleugnung](https://de.wikipedia.org/wiki/Verleugnung) und [Kompartimentalisierung](<https://de.wikipedia.org/wiki/Kompartimentalisierung_(Psychologie)>).
Wenn es um unseren eigenen Tod geht, sind wir sehr anfällig für Verleugnung.
Bücher wurden über die [Verleugnung des Todes](https://de.wikipedia.org/wiki/Die_Verleugnung_des_Todes) geschrieben.

Diese Bewältigungsmechanismen schützen uns vor dem Schmerz, akzeptieren zu müssen, dass die Welt nicht so ist, wie wir dachten.
Sie können uns jedoch auch daran hindern, auf eine Bedrohung angemessen zu reagieren.

Wenn Sie bemerken, dass jemand diese Bewältigungsmechanismen verwendet, versuchen Sie, empathisch zu sein.
Sie tun es nicht absichtlich, und sie sind nicht dumm.
Es ist eine natürliche Reaktion auf schlechte Nachrichten, und wir alle tun es in gewissem Maße.

### Es ist schwer, zuzugeben, dass die eigene Arbeit gefährlich ist {#admitting-your-work-is-dangerous-is-hard}

Für diejenigen, die an KI-Fähigkeiten gearbeitet haben, ist es noch schwieriger, die Gefahren zu akzeptieren.

Nehmen Sie Yoshua Bengio zum Beispiel.
Yoshua Bengio hat einen brillanten Geist und ist einer der Pioniere auf dem Gebiet der KI.
KI-Sicherheitsexperten warnen seit Jahren vor den potenziellen Gefahren von KI, aber es dauerte lange, bis er ihre Warnungen ernst nahm.
In einem [Interview](https://youtu.be/0RknkWgd6Ck?t%25253D949) gab er die folgende Erklärung:

> "Warum habe ich nicht früher darüber nachgedacht? Warum hat Geoffrey Hinton nicht früher darüber nachgedacht? [...] Ich glaube, es gibt einen psychologischen Effekt, der immer noch bei vielen Menschen wirksam ist. [...] Es ist sehr schwer, in Bezug auf das eigene Ego und das Gefühl, gut zu sein, zu akzeptieren, dass das, woran man seit Jahrzehnten arbeitet, tatsächlich sehr gefährlich für die Menschheit sein könnte. [...] Ich denke, ich wollte nicht zu viel darüber nachdenken, und das ist wahrscheinlich auch bei anderen der Fall."

Es sollte niemanden überraschen, dass einige der heftigsten KI-Risiko-Leugner KI-Forscher selbst sind.

### Leicht zu entkräften als Verschwörung oder Sekte {#easy-to-dismiss-as-conspiracy-or-cult}

Im letzten Jahr wurde der größte Teil der Bevölkerung mit dem Konzept des existenziellen Risikos durch KI bekannt gemacht.
Wenn Menschen davon hören, suchen sie nach einer Erklärung.
Die richtige Erklärung ist, dass KI tatsächlich gefährlich ist, aber dies zu glauben ist schwierig und beängstigend: Es wird zu viel kognitive Dissonanz verursachen.
Die Menschen werden also fast direkt nach einer anderen Erklärung suchen.
Es gibt zwei alternative Erklärungen, die viel einfacher zu glauben sind:

1. **Es ist alles eine große Verschwörung**. KI-Unternehmen hypen KI, um mehr Geld zu bekommen, und KI-Sicherheitsleute sind Teil dieser Hype-Maschine. Diese Erzählung passt zu verschiedenen Beobachtungen: Unternehmen lügen oft, viele KI-Sicherheitsleute werden von KI-Unternehmen beschäftigt, und es gibt eine Reihe von Milliardären, die KI-Sicherheitsforschung finanzieren. Wir können jedoch auch darauf hinweisen, warum diese Verschwörungsgeschichte einfach nicht wahr ist. Viele der "Alarmisten" sind Wissenschaftler, die nichts zu gewinnen haben. Die Unternehmen könnten auf irgendeine Weise profitieren, aber bis vor kurzem (Mai 2023) haben sie sich fast vollständig still über KI-Risiken gehalten. Dies macht Sinn, da Unternehmen meist nicht davon profitieren, wenn die Menschen Angst vor ihrem Produkt oder ihrer Dienstleistung haben. Wir haben vor Microsoft und OpenAI protestiert, teilweise weil wir wollten, dass sie die Risiken anerkennen.
2. **Es ist eine Sekte**. Die Gruppe, die an KI-Sicherheit glaubt, ist nur eine Gruppe von verrückten religiösen Extremisten, die an das Ende der Welt glauben. Dies scheint auch zu passen, da die Menschen in der KI-Sicherheitsgemeinschaft oft sehr leidenschaftlich über das Thema sind und allerlei Insider-Jargon verwenden. Es fällt jedoch auseinander, wenn man darauf hinweist, dass die Menschen, die vor KI-Risiken warnen, keine einzelne Organisation sind. Es ist eine große, vielfältige Gruppe von Menschen, es gibt keinen einzigen Anführer, es gibt keine Rituale, und es gibt keinen Dogmatismus.

Was diese Erklärungen so überzeugend macht, ist nicht nur, dass sie einfach zu verstehen sind oder dass sie alle Beobachtungen perfekt erklären - der Hauptgrund ist, dass sie tröstlich sind.
Zu glauben, dass die Menschen vor KI warnen, _weil es eine reale Bedrohung gibt_, ist beängstigend und schwer zu akzeptieren.

## Schwierig zu verstehen {#difficult-to-understand}

Die Argumente für das existenzielle Risiko durch KI sind oft sehr technisch, und wir neigen dazu, KI-Systeme zu anthropomorphisieren.

### KI-Alignment ist überraschend schwierig {#ai-alignment-is-surprisingly-hard}

Die Menschen könnten intuitiv das Gefühl haben, dass sie das KI-Alignment-Problem lösen könnten.
Warum nicht einfach einen [Stopp-Knopf](https://www.youtube.com/watch?v=3TYT1QfdfsM&list=PLfHsskCxi_g-c62a_dmsNuHynaXsRQm40&index=10) hinzufügen? Warum nicht den Computer [wie ein Kind aufziehen](https://www.youtube.com/watch?v=eaYIU6YXr3w)? Warum nicht [Asimovs drei Gesetze](https://www.youtube.com/watch?v=7PKx3kS7f4A)?
Im Gegensatz zu den meisten Arten von technischen Problemen haben die Menschen eine Meinung darüber, wie das KI-Alignment-Problem zu lösen ist, und unterschätzen die Schwierigkeit des Problems.
Das Verständnis der tatsächlichen Schwierigkeit erfordert viel Zeit und Mühe.

### Wir anthropomorphisieren {#we-anthropomorphize}

Wir sehen Gesichter in Wolken, und wir sehen menschliche Qualitäten in KI-Systemen.
Millionen Jahre der Evolution haben uns zu hochsozialen Wesen gemacht, aber diese Instinkte sind nicht immer hilfreich.
Wir neigen dazu, Computer als menschliche Ziele und Motivationen zu sehen, Emotionen zu empfinden und ein Gefühl für Moral zu haben.
Wir erwarten von einem sehr intelligenten Computer, dass er auch sehr weise und freundlich ist.
Dies ist einer der Gründe, warum die Menschen intuitiv denken, dass KI-Alignment einfach ist, und warum die [Orthogonalitäts-These](https://www.youtube.com/watch?v=hEUO6pjwFOo) so kontraintuitiv sein kann.

### KI-Sicherheit verwendet komplexe Sprache {#ai-safety-uses-complex-language}

Das Feld der KI-Sicherheit besteht größtenteils aus einer kleinen Gruppe von (intelligenten) Menschen, die ihren eigenen Jargon entwickelt haben.
Das Lesen von LessWrong-Beiträgen kann sich wie das Lesen einer fremden Sprache anfühlen.
Viele Beiträge gehen davon aus, dass der Leser bereits mit mathematischen Konzepten, verschiedenen technischen Konzepten und dem Jargon des Fachgebiets vertraut ist.

## Schwierig, darauf zu reagieren {#difficult-to-act-on}

Selbst wenn die Menschen die Argumente verstehen, ist es schwierig, darauf zu reagieren.
Die Auswirkungen sind zu groß, wir haben Bewältigungsmechanismen, die die Risiken herunterspielen, und wenn wir die Schwere der Situation spüren, können wir uns machtlos fühlen.

### Mangelnde angeborene Angstreaktion {#lack-of-innate-fear-response}

Unsere Gehirne haben sich entwickelt, um Dinge zu fürchten, die gefährlich sind.
Wir fürchten instinktiv Höhen, große Tiere mit scharfen Zähnen, plötzliche laute Geräusche und Dinge, die sich in einer S-Form bewegen.
Ein superintelligenter Computer trifft keine unserer primitiven Ängste.
Darüber hinaus haben wir eine starke Angst vor sozialer Ablehnung oder dem Verlust des sozialen Status, was bedeutet, dass die Menschen tendenziell Angst davor haben, über KI-Risiken zu sprechen.

### Umfangsblindheit {#scope-insensitivity}

> "Ein einzelner Tod ist eine Tragödie; eine Million Tote ist eine Statistik." - Joseph Stalin

Umfangsblindheit ist die menschliche Tendenz, die Auswirkungen großer Zahlen zu unterschätzen.
Wir kümmern uns nicht 10-mal so viel um 1000 Menschen, die sterben, wie um 100 Menschen, die sterben.
Das existenzielle Risiko bedeutet den Tod aller 8 Milliarden Menschen auf der Erde (ohne ihre Nachkommen).

Selbst wenn es nur eine 1-prozentige Chance gibt, dass dies passiert, ist es immer noch ein sehr großes Problem.
Rational sollten wir diese 1-prozentige Chance von 8 Milliarden Toten genauso wichtig nehmen wie den sicheren Tod von 80 Millionen Menschen.

Wenn jemand das Gefühl hat, dass das Ende der Welt nicht so schlimm ist (Sie wären überrascht, wie oft dies passiert), können Sie versuchen, die Dinge persönlicher zu machen.
Die Menschheit ist nicht nur ein abstraktes Konzept, sondern Ihre Freunde, Ihre Familie und Sie selbst.
Alle Menschen, um die Sie sich kümmern, werden sterben.

### Unser Verhalten wird durch unsere Umgebung und primitive Geister geprägt {#our-behavior-is-shaped-by-our-environment-and-primitive-minds}

Unsere Handlungen werden durch das konditioniert, was als normal, gut und vernünftig angesehen wird.
Egal, wie sehr wir in einer Situation handeln möchten, die ein Handeln erfordert, wenn diese Handlungen ungewöhnlich sind, fürchten wir uns oft bewusst oder unbewusst davor, von der Gesellschaft ausgeschlossen zu werden. Und was normal ist, wird uns durch unsere engen sozialen Kreise und Online-Feeds eingeimpft.
Die Menschen, die einfach Dinge tun und über Dinge sprechen, die nichts mit dem zu tun haben, was uns wirklich wichtig ist, werden das, was in unseren Köpfen ist, überschreiben und uns motivieren, täglich andere Dinge zu tun.

Das Aussterberisiko verdient viel mehr unserer Zeit, Energie und Aufmerksamkeit. Unsere Reaktionen darauf sollten eher wie lebensbedrohliche Situationen sein, die uns mit Adrenalin füllen. Aber wegen der abstrakten Natur der Probleme und unserer unangepassten Geister gehen die meisten Menschen, die davon erfahren, einfach weiter mit ihrem Tag, als hätten sie nichts gelernt.

### Bewältigungsmechanismen (Verhinderung von Handlungen) {#coping-mechanisms-preventing-action}

Die gleichen Bewältigungsmechanismen, die die Menschen daran hindern, an das existenzielle Risiko zu _glauben_, hindern sie auch daran, _darauf zu reagieren_.
Wenn Sie in Verleugnung oder Kompartimentalisierung sind, werden Sie nicht das Bedürfnis verspüren, etwas dagegen zu tun.

### Stress und Angst {#stress-and-anxiety}

Während ich dies schreibe, fühle ich mich gestresst und ängstlich.
Es ist nicht nur, weil ich Angst vor dem Ende der Welt habe, sondern auch, weil ich das Gefühl habe, dass ich etwas dagegen tun muss.
Es gibt viel Druck, zu handeln, und es kann überwältigend sein.
Dieser Stress kann ein guter Motivator sein, aber er kann auch lähmend sein.

### Hoffnungslosigkeit und Machtlosigkeit {#hopelessness-and-powerlessness}

Wenn die Menschen das Thema ernst nehmen und die volle Schwere der Situation begreifen, können sie das Gefühl haben, dass alle Hoffnung verloren ist.
Es kann sich wie eine Krebsdiagnose anfühlen: Sie werden früher sterben, als Sie wollten, und es gibt nichts, was Sie dagegen tun können.
Das Problem ist zu groß, um es anzugehen, und Sie sind zu klein.
Die meisten Menschen sind keine KI-Sicherheitsexperten oder erfahrene Lobbyisten, also wie können sie etwas dagegen tun?

## Aber Sie können helfen! {#but-you-can-help}

Es gibt viele [Dinge, die Sie tun können](/action).
Ein Brief zu schreiben, an einer Demonstration teilzunehmen, Geld zu spenden oder einer Gemeinschaft beizutreten, ist nicht so schwer!
Und diese Handlungen haben eine reale Auswirkung.
Selbst wenn wir dem Ende der Welt gegenüberstehen, kann es immer noch Hoffnung und sehr lohnende Arbeit geben.
[Treten Sie PauseAI bei](/join) und werden Sie Teil unserer Bewegung.
