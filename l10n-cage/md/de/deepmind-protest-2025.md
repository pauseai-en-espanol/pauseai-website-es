---
title: 'PauseAI präsentiert: Der Google DeepMind Protest'
slug: deepmind-protest-2025
description: Wir organisieren einen Protest, um Google DeepMind für ihre gebrochenen Versprechen zur Rechenschaft zu ziehen.
date: 2025-06-30T17:00:00.000Z
---

PauseAI hat seinen größten Protest bisher vor dem Londoner Büro von Google DeepMind abgehalten.

## Medienberichterstattung {#media-coverage}

- [Business Insider](https://www.businessinsider.com/protesters-accuse-google-deepmind-breaking-promises-ai-safety-2025-6)
- [Islignton Tribune](https://www.islingtontribune.co.uk/article/stark-warning-from-protesters-calling-for-ai-pause-its-going-to-turn-out-bad)
- [Times of India](https://www.islingtontribune.co.uk/article/stark-warning-from-protesters-calling-for-ai-pause-its-going-to-turn-out-bad)
- [Tech Times](https://www.techtimes.com/articles/311120/20250701/google-deepmind-slammed-protesters-over-broken-ai-safety-promise.htm)

## Google DeepMind hat seine Versprechen gebrochen {#google-deepmind-have-broken-their-promises}

Im Jahr 2024 gab Google auf dem AI-Gipfel in Seoul ein öffentliches Versprechen ab. Das Unternehmen unterzeichnete die Frontier AI Safety Commitments, in denen es sich verpflichtete, seine KI-Modelle streng zu testen und die Ergebnisse von unabhängigen Drittprüfern zu berücksichtigen. Google versprach auch, volle Transparenz über den Prozess, einschließlich der Regierungsbeteiligung, zu gewährleisten.

Im März 2025 veröffentlichte Google dann Gemini 2.5 Pro, sein fortschrittlichstes KI-Modell bisher. Als Sicherheitsexperten nach dem versprochenen Testbericht suchten, fanden sie jedoch nichts. Es gab keine externe Bewertung und keinen Transparenzbericht – nur Schweigen.

Einen Monat später veröffentlichte Google unter Druck eine minimalistische "Modellkarte" mit einigen internen Bewertungen, aber ohne Erwähnung externer Bewertungen. Später fügte das Unternehmen vage Hinweise auf "externe Tester" hinzu, lieferte aber keine Details. Als Fortune direkt fragte, ob Regierungen beteiligt waren, weigerte sich Google, zu antworten – und brach damit sein Transparenzversprechen.

Google machte ähnliche Zusagen gegenüber dem Weißen Haus im Jahr 2023 und unterzeichnete den Hiroshima-Prozess-Internationalen Verhaltenskodex im Jahr 2025. Mit der Veröffentlichung von Gemini 2.5 Pro scheint Google auch diese anderen Sicherheitszusagen, zumindest im Geiste, gebrochen zu haben.

Lesen Sie die gesamte Zeitleiste von Google DeepMinds gebrochenen Versprechen [hier](https://pauseai.info/google-deepmind-broken-promises).

## Unsichere KI-Entwicklung bedroht uns alle {#unsafe-ai-development-threatens-us-all}

Die heutigen KI-Modelle sind zwar nicht gefährlich genug, um Massenzerstörung zu verursachen, aber die KI-Entwicklung beschleunigt sich unvorhersehbar. Wir benötigen strenge Tests jeder Generation, um nicht von plötzlichen Leistungssprüngen überrascht zu werden. Noch wichtiger ist, dass Googles lässige Missachtung von Sicherheitszusagen einen gefährlichen Präzedenzfall schafft.

Wenn KI immer leistungsfähiger wird, werden die Wettbewerbsdrucke zunehmen und die Einsätze höher. Wenn wir es Unternehmen jetzt erlauben, Sicherheitszusagen zu ignorieren, wenn die Risiken relativ gering sind, welche Hoffnung haben wir dann, sie zur Rechenschaft zu ziehen, wenn KI-Systeme existenzielle Bedrohungen darstellen könnten?

Die Normen, die wir heute festlegen, werden die Entwicklung der leistungsfähigsten Technologie in der Geschichte der Menschheit prägen.

## Lasst uns DeepMind zur Rechenschaft ziehen {#lets-hold-deepmind-accountable}

PauseAI ist eine wachsende Bewegung, die sich weigert, die Sicherheit von KI als nachrangig zu betrachten. Am 30. Juni versammeln wir uns vor dem Londoner Hauptsitz von Google DeepMind mit einer einfachen Botschaft: Haltet eure Versprechen ein.

Unsere Forderung ist einfach: Google soll die Versprechen einhalten, die es bereits gemacht hat. Das Unternehmen soll zeitnahe und transparente Berichte über die Ergebnisse seiner Sicherheitstests vor der Bereitstellung veröffentlichen.

Unser ultimatives Ziel ist ein Moratorium für die Entwicklung von Frontier-KI, bis wir sicherstellen können, dass fortschrittliche Systeme sicher sind. Aber wir brauchen jetzt eines: grundlegende Rechenschaftspflicht von einem der mächtigsten KI-Unternehmen der Welt.

Die Zukunft von KI wird durch die Präzedenzfälle geprägt, die wir heute schaffen.

**Wir fordern Google DeepMind auf:**

1. **Klare Definitionen** von "Bereitstellung" festzulegen, die mit dem allgemeinen Verständnis übereinstimmen – wenn ein Modell öffentlich zugänglich ist, ist es bereitgestellt.
2. **Einen spezifischen Zeitplan** für die Veröffentlichung von Sicherheitsbewertungsberichten für alle zukünftigen Modelle zu veröffentlichen.
3. **Eindeutig zu klären**, für jedes Modell-Release, welche Regierungsbehörden und unabhängigen Drittprüfer an den Tests beteiligt sind und die genauen Zeitpläne ihrer Testverfahren.

Schließt euch uns am Montag, dem 30. Juni, in London an, indem ihr euch [hier](https://lu.ma/bvffgzmb) anmeldet.
