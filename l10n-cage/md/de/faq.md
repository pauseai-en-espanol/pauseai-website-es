---
title: Häufig gestellte Fragen
description: Häufig gestellte Fragen zu PauseAI und den Risiken von superintelligenter KI.
---

<script>
    import SimpleToc from '$lib/components/simple-toc/SimpleToc.svelte'
</script>
<style>
    h2 {
        font-size: 1.2rem;
    }
</style>

<SimpleToc />

## Wer sind wir? {#who-are-you}

Wir sind eine Gemeinschaft von Freiwilligen und [lokale Gemeinschaften](/communities), die von einer [gemeinnützigen Organisation](/organization) koordiniert werden, um die [Risiken von KI](/risks) (einschließlich des [Risikos des menschlichen Aussterbens](/xrisk)) zu mildern.
Unser Ziel ist es, unsere Regierungen davon zu überzeugen, einzugreifen und die [Entwicklung von übermenschlicher KI](/proposal) zu stoppen.
Wir tun dies, indem wir die Öffentlichkeit informieren, mit Entscheidungsträgern sprechen und [Veranstaltungen](/events) organisieren.

## Haben Sie soziale Medien? {#do-you-have-social-media}

Sie können uns auf [Discord](https://discord.gg/NuqkHutXW3) (hier passiert die meiste Koordination!), [Twitter](https://twitter.com/PauseAI), [Substack](https://substack.com/@pauseai), [Facebook](https://www.facebook.com/PauseAI), [TikTok](https://www.tiktok.com/@pauseai), [LinkedIn](https://www.linkedin.com/uas/login?session_redirect=/company/97035448/), [YouTube](https://www.youtube.com/@PauseAI), [Instagram](https://www.instagram.com/pause_ai), [Telegram](https://t.me/+UeTsIsNkmt82ZmQ8), [Whatsapp](https://chat.whatsapp.com/JgcAbjqRr8X3tvrXdeQvfj) und [Reddit](https://www.reddit.com/r/PauseAI/) finden.
Sie können uns per E-Mail kontaktieren: [joep@pauseai.info](mailto:joep@pauseai.info).

## Haben Sie Angst vor Veränderungen und neuer Technologie? {#arent-you-just-scared-of-changes-and-new-technology}

Sie werden überrascht sein, dass die meisten Menschen bei PauseAI sich als Techno-Optimisten betrachten.
Viele von ihnen sind in die KI-Entwicklung involviert, sind Gadget-Liebhaber und waren bisher sehr aufgeregt über die Zukunft.
Viele von ihnen waren besonders aufgeregt über das Potenzial von KI, um der Menschheit zu helfen.
Deshalb war für viele von ihnen die traurige Erkenntnis, dass KI ein existenzielles Risiko darstellen könnte, sehr [schwierig zu akzeptieren](/psychology-of-x-risk).

## Wollen Sie alle KI verbieten? {#do-you-want-to-ban-all-ai}

Nein, nur die Entwicklung der größten allgemeinen KI-Systeme, oft "Frontier-Modelle" genannt.
Fast alle derzeit existierenden KI-Systeme wären unter [unserem Vorschlag](/proposal) legal, und die meisten zukünftigen KI-Modelle werden auch legal bleiben.
Wir fordern ein Verbot von leistungsfähigeren allgemeinen KI-Systemen, bis wir wissen, wie wir beweisbar sichere KI bauen können und sie unter demokratischer Kontrolle haben.

## Glauben Sie, dass GPT-4 uns töten wird? {#do-you-believe-gpt-4-is-going-to-kill-us}

Nein, wir denken nicht, dass [aktuelle KI-Modelle](/sota) eine existenzielle Bedrohung darstellen.
Es scheint wahrscheinlich, dass die meisten nächsten KI-Modelle auch keine sein werden.
Aber wenn wir weiterhin immer leistungsfähigere KI-Systeme bauen, werden wir schließlich einen Punkt erreichen, an dem eines eine [existenzielle Bedrohung](/xrisk) darstellen wird.

## Kann ein Pause nach hinten losgehen und die Dinge schlimmer machen? {#can-a-pause-backfire-and-make-things-worse}

Wir haben diese Bedenken in [diesem Artikel](/mitigating-pause-failures) angesprochen.

## Ist ein Pause überhaupt möglich? {#is-a-pause-even-possible}

AGI ist nicht unvermeidlich.
Es erfordert Horden von Ingenieuren mit Millionen-Dollar-Gehältern.
Es erfordert eine voll funktionsfähige und unbeschränkte Lieferkette der komplexesten Hardware.
Es erfordert, dass wir alle es diesen Unternehmen erlauben, mit unserer Zukunft zu spielen.

[Mehr über die Machbarkeit eines Pause](/feasibility).

## Wer zahlt Ihnen? {#who-is-paying-you}

Siehe unsere [Funding-Seite](/funding)

## Was sind Ihre Pläne? {#what-are-your-plans}

Unser Fokus liegt auf [der Bewegung zu vergrößern](/growth-strategy), Proteste zu organisieren, Politiker zu lobbyieren und die Öffentlichkeit zu informieren.

Überprüfen Sie unseren [Zeitplan](/roadmap) für eine detaillierte Übersicht über unsere Pläne und was wir mit mehr Finanzierung tun könnten.

## Wie denken Sie, dass Sie die Regierungen davon überzeugen können, KI zu stoppen? {#how-do-you-think-you-can-convince-governments-to-pause-ai}

Überprüfen Sie unsere [Theorie des Wandels](/theory-of-change) für eine detaillierte Übersicht über unsere Strategie.

## Warum protestieren Sie? {#why-do-you-protest}

- Protestieren zeigt der Welt, dass wir uns um diese Angelegenheit kümmern. Indem wir protestieren, zeigen wir, dass wir bereit sind, unsere Zeit und Energie zu investieren, um die Menschen zum Zuhören zu bringen.
- Proteste können und werden oft [die öffentliche Meinung, das Wahlverhalten, das Verhalten von Unternehmen und die Politik positiv beeinflussen](https://www.socialchangelab.org/_files/ugd/503ba4_052959e2ee8d4924934b7efe3916981e.pdf).
- Die meisten Menschen unterstützen [friedliche/nicht-gewalttätige Proteste](https://today.yougov.com/politics/articles/31718-do-protesters-want-help-or-hurt-america).
- Es gibt [keine Beweise für einen "Backfire"-Effekt](https://journals.sagepub.com/doi/full/10.1177/2378023120925949) [es sei denn, der Protest ist gewalttätig](https://news.stanford.edu/2018/10/12/how-violent-protest-can-backfire/). Unsere Proteste sind friedlich und nicht-gewalttätig.
- Es ist eine soziale Bindungserfahrung. Sie treffen andere Menschen, die Ihre Bedenken und Ihre Bereitschaft, zu handeln, teilen.
- Überprüfen Sie [diesen großartigen Artikel](https://forum.effectivealtruism.org/posts/4ez3nvEmozwPwARr9/a-case-for-the-effectiveness-of-protest) für mehr Einblicke in die Wirksamkeit von Protesten.

Wenn Sie einen [Protest organisieren](/organizing-a-protest) wollen, können wir Ihnen mit Ratschlägen und Ressourcen helfen.

## Wie wahrscheinlich ist es, dass superintelligente KI sehr schlechte Ergebnisse verursacht, wie das menschliche Aussterben? {#how-likely-is-it-that-superintelligent-ai-will-cause-very-bad-outcomes-like-human-extinction}

Wir haben [eine Liste von 'p(doom)'-Werten](/pdoom) (Wahrscheinlichkeit von schlechten Ergebnissen) von verschiedenen bekannten Experten auf dem Gebiet zusammengestellt.

KI-Sicherheitsforscher (die die Experten auf diesem Gebiet sind) sind geteilt in dieser Frage, und Schätzungen [reichen von 2% bis 97% mit einem Durchschnitt von 30%](https://web.archive.org/web/20221013014859/https://www.alignmentforum.org/posts/QvwSr5LsxyDeaPK5s/existential-risk-from-ai-survey-results).
Beachten Sie, dass keine (befragten) KI-Sicherheitsforscher glauben, dass es eine 0%-Chance gibt.
Es könnte jedoch eine Selektionsverzerrung geben: Menschen, die im Bereich der KI-Sicherheit arbeiten, tun dies wahrscheinlich, weil sie glauben, dass die Verhinderung von schlechten KI-Ergebnissen wichtig ist.

Wenn Sie KI-Forscher im Allgemeinen (nicht Sicherheitsspezialisten) fragen, sinkt diese Zahl auf einen [Mittelwert von etwa 14%](https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/), mit einem Median von 5%.
Die überwiegende Mehrheit, 86% von ihnen, glaubt, dass das Alignment-Problem sowohl ein reales als auch ein wichtiges Problem ist.
Beachten Sie, dass es auch hier eine Selektionsverzerrung in die entgegengesetzte Richtung geben könnte: Menschen, die in der KI arbeiten, tun dies wahrscheinlich, weil sie glauben, dass KI vorteilhaft sein wird.

_Stellen Sie sich vor, Sie sind zu einem Testflug in einem neuen Flugzeug eingeladen._
Die Flugzeugingenieure denken, dass es eine 14%-Chance gibt, dass es abstürzt.
Würden Sie in dieses Flugzeug steigen? Denn jetzt steigen wir alle in das KI-Flugzeug ein.

## Wie lange haben wir noch, bis superintelligente KI entsteht? {#how-long-do-we-have-until-superintelligent-ai}

Es könnte Monate dauern, es könnte Jahrzehnte dauern, niemand weiß es genau.
Wir wissen jedoch, dass der Fortschritt in der KI oft stark unterschätzt wird.
Vor gerade drei Jahren dachten wir, dass wir KI-Systeme, die den SAT-Test bestehen, im Jahr 2055 haben würden.
Wir haben es im April 2023 geschafft.
Wir sollten so handeln, als hätten wir sehr wenig Zeit, weil wir nicht überrascht werden wollen.

[Mehr über die Dringlichkeit](/urgency).

## Wenn wir Pause einlegen, was ist mit China? {#if-we-pause-what-about-china}

Zunächst hat China strengere KI-Regulierungen als fast jedes andere Land.
Sie [erlaubten nicht einmal Chatbots](https://www.reuters.com/technology/chinas-slow-ai-roll-out-points-its-tech-sectors-new-regulatory-reality-2023-07-12/) und [verboten das Training auf Internet-Daten](https://cointelegraph.com/news/china-sets-stricter-rules-training-generative-ai-models) bis [September 2023](https://asia.nikkei.com/Business/Technology/China-approves-AI-chatbot-releases-but-will-it-unleash-innovation).
China hat eine kontrollierendere Regierung und hat daher noch mehr Grund, die unkontrollierbaren und unvorhersehbaren Auswirkungen von KI zu fürchten.
Während des UNSC-Treffens zu KI-Sicherheit war China das einzige Land, das die Möglichkeit erwähnte, eine Pause einzulegen.

Beachten Sie auch, dass wir in erster Linie eine _internationale_ Pause fordern, die durch einen Vertrag durchgesetzt wird.
Ein solcher Vertrag muss auch von China unterzeichnet werden.
Wenn der Vertrag garantiert, dass andere Nationen auch stoppen, und es ausreichende Durchsetzungsmechanismen gibt,
sollte dies etwas sein, das China auch sehen will.

## OpenAI und Google sagen, dass sie reguliert werden wollen. Warum protestieren Sie gegen sie? {#openai-and-google-are-saying-they-want-to-be-regulated-why-are-you-protesting-them}

Wir begrüßen [OpenAI](https://openai.com/blog/governance-of-superintelligence) und [Google](https://www.ft.com/content/8be1a975-e5e0-417d-af51-78af17ef4b79) für ihre Forderung nach internationaler Regulierung von KI.
Wir glauben jedoch, dass die aktuellen Vorschläge nicht ausreichen, um eine KI-Katastrophe zu verhindern.
Google und Microsoft haben noch nicht öffentlich etwas über das existenzielle Risiko von KI gesagt.
Nur OpenAI [erwähnt explizit das Risiko des Aussterbens](https://openai.com/blog/governance-of-superintelligence), und wir begrüßen sie dafür, dass sie dieses Risiko ernst nehmen.
Ihre Strategie ist jedoch ziemlich explizit: Eine Pause ist unmöglich, wir müssen zuerst zu Superintelligenz gelangen.
Das Problem damit ist jedoch, dass sie [nicht glauben, dass sie das Alignment-Problem gelöst haben](https://youtu.be/L_Guz73e6fw?t=1478).
Die KI-Unternehmen sind in einem Wettlauf nach unten gefangen, bei dem KI-Sicherheit für einen Wettbewerbsvorteil geopfert wird.
Dies ist einfach das Ergebnis von Marktdynamiken.
Wir brauchen Regierungen, die eingreifen und Politiken (auf internationaler Ebene) umsetzen, die [die schlimmsten Ergebnisse verhindern](/proposal).

## Drängen KI-Unternehmen die existenzielle Risiko-Erzählung, um uns zu manipulieren? {#are-ai-companies-pushing-the-existential-risk-narrative-to-manipulate-us}

Wir können nicht genau wissen, welche Motivationen diese Unternehmen haben, aber wir wissen, dass **das existenzielle Risiko nicht ursprünglich von KI-Unternehmen vorangetrieben wurde - es waren Wissenschaftler, Aktivisten und NGOs**.
Lassen Sie uns auf die Zeitachse schauen.

Es gab viele Menschen, die seit den frühen 2000er Jahren vor existenziellem Risiko gewarnt haben.
Eliezer Yudkowsky, Nick Bostrom, Stuart Russell, Max Tegmark und viele andere.
Sie hatten keine KI-Technologie zu pushen - sie waren einfach besorgt über die Zukunft der Menschheit.

Die KI-Unternehmen erwähnten das existenzielle Risiko erst sehr kürzlich.

Sam Altman ist eine interessante Ausnahme.
Er schrieb über existenzielles KI-Risiko [im Jahr 2015 auf seinem privaten Blog](https://blog.samaltman.com/machine-intelligence-part-1), bevor er OpenAI gründete.
In den Jahren seitdem erwähnte er das existenzielle Risiko explizit nicht wieder.
Während der Senatsanhörung am 16. Mai 2023, als er nach seinem Blog-Beitrag zum existenziellen Risiko gefragt wurde, antwortete er nur, indem er über Jobs und die Wirtschaft sprach.
Er drängte die existenzielle Risiko-Erzählung nicht voran, er vermied sie aktiv.

Im Mai 2023 änderte sich alles:

- Am 1. Mai kündigte der 'Gottvater der KI' Geoffrey Hinton [seinen Job bei Google](https://fortune.com/2023/05/01/godfather-ai-geoffrey-hinton-quit-google-regrets-lifes-work-bad-actors/), um vor existenziellem Risiko zu warnen.
- Am 5. Mai wurde der [erste PauseAI-Protest angekündigt](https://twitter.com/Radlib4/status/1654262421794717696), direkt vor OpenAIs Haustür.
- Am 22. Mai veröffentlichte OpenAI [einen Blog-Beitrag über die Regulierung von Superintelligenz](https://openai.com/blog/governance-of-superintelligence) und erwähnte das existenzielle Risiko zum ersten Mal.
- Am 24. Mai bestätigte der ehemalige Google-CEO Eric Schmidt das existenzielle Risiko.
- Am 30. Mai wurde die [Safe.ai-Erklärung (zum existenziellen Risiko)](https://www.safe.ai/statement-on-ai-risk) veröffentlicht. Diesmal einschließlich Leute von OpenAI, Google und Microsoft.

Diese Unternehmen waren sehr langsam darin, das existenzielle Risiko anzuerkennen, wenn man bedenkt, dass viele ihrer Mitarbeiter sich seit Jahren dessen bewusst sind.
Also sehen wir es so: Die KI-Unternehmen drängen die existenzielle Risiko-Erzählung nicht voran, sie reagieren auf andere, die sie vorantreiben, und haben mit ihrer Antwort gewartet, bis es absolut notwendig war.

Die Geschäftsanreize weisen in die andere Richtung: Unternehmen würden lieber nicht die Risiken ihrer Produkte betonen, um Kunden und Investitionen anzuziehen, anstatt sie zu übertreiben.
Wie viel strenge Regulierung und negative Aufmerksamkeit laden sich die Unternehmen durch das Eingeständnis dieser Gefahren auf?
Und würde ein Unternehmen wie OpenAI [20% seiner Rechenressourcen](https://openai.com/blog/introducing-superalignment) für KI-Sicherheit einsetzen, wenn es nicht an die Risiken glauben würde?

Hier ist unsere Interpretation: Die KI-Unternehmen unterzeichneten die Erklärung, weil _sie wissen, dass das existenzielle Risiko ein Problem ist, das sehr ernst genommen werden muss_.

Ein großer Grund, warum viele andere Menschen nicht glauben wollen, dass das existenzielle Risiko ein reales Anliegen ist?
Weil das Eingeständnis, dass _wir tatsächlich in Gefahr sind_, eine sehr, sehr beängstigende Sache ist.

[Mehr über die Psychologie des existenziellen Risikos](/psychology-of-x-risk).

## Okay, ich will helfen! Was kann ich tun? {#ok-i-want-to-help-what-can-i-do}

Es gibt viele [Dinge, die du tun kannst](/action).
Auf eigene Faust kannst du einen [Brief](/writing-a-letter) schreiben, [Flyer](/flyering) verteilen, [lernen](/learn) und andere informieren, an einem [Protest](/protests) teilnehmen oder [Geld spenden](/donate)!
Aber noch wichtiger: Du kannst [PauseAI beitreten](/join) und dich mit anderen koordinieren, die aktiv werden.
Überprüfe, ob es [lokale Gemeinschaften](/communities) in deiner Nähe gibt.
Wenn du mehr beitragen möchtest, kannst du ein Freiwilliger werden und einem unserer [Teams](/teams) beitreten oder [eine lokale Gemeinschaft gründen](/local-organizing)!

Selbst wenn wir dem Ende der Welt gegenüberstehen, kann es noch Hoffnung und sehr lohnende Arbeit geben.
