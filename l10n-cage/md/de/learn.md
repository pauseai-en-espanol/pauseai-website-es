---
title: Warum KI-Sicherheit wichtig ist
description: Bildungsressourcen (Videos, Artikel, Bücher) über KI-Risiken und KI-Alignment
---

<script>
import NewsletterSignup from '$lib/components/NewsletterSignup.svelte';
</script>

<NewsletterSignup />

## Auf dieser Website {#on-this-website}

- [Risiken](/risks). Eine Zusammenfassung der Risiken von KI.
- [X-Risiko](/xrisk). Warum KI ein existenzielles Risiko darstellt.
- [Übernahme](/ai-takeover). Wie KI die Welt übernehmen könnte.
- [Zitate](/quotes). Zitate zu KI-Risiken und -Governance.
- [Machbarkeit einer Pause](/feasibility). Die Machbarkeit einer Pause in der KI-Entwicklung.
- [Den Pausenknopf bauen](/building-the-pause-button). Was es braucht, um KI zu pausieren.
- [FAQ](/faq). Häufig gestellte Fragen zu KI-Sicherheit und PauseAI.
- [Aktion](/action). Was Sie tun können, um zu helfen (mit Links zu vielen aktionsbezogenen Anleitungen)

## Andere Websites {#other-websites}

- [Das Kompendium](https://www.thecompendium.ai/). Ein umfassendes Wissensbündel darüber, warum das aktuelle KI-Rennen so gefährlich ist und was wir dagegen tun können.
- [Ein schmaler Pfad](https://www.narrowpath.co/). Ein detaillierter Plan für die Schritte, die wir unternehmen müssen, um unsere Chancen zu erhöhen, die nächsten Jahrzehnte zu überleben.
- [Die Zukunft menschlich halten](https://keepthefuturehuman.ai/). Ein Artikel von FLI's Anthony Aguirre darüber, warum und wie wir die Zukunft menschlich halten können. (siehe [Kapitel 8: Wie man keine AGI baut](https://keepthefuturehuman.ai/chapter-8-how-to-not-build-agi/))
- [AISafety.com](https://www.aisafety.com) & [AISafety.info](https://aisafety.info). Die Landingpages für KI-Sicherheit. Erfahren Sie mehr über die Risiken, Gemeinschaften, Veranstaltungen, Jobs, Kurse, Ideen, wie man die Risiken mildern kann, und vieles mehr!
- [Existenzielle Sicherheit](https://existentialsafety.org/). Eine umfassende Liste von Maßnahmen, die wir ergreifen können, um unsere existenzielle Sicherheit vor KI zu erhöhen.
- [AISafety.dance](https://aisafety.dance). Eine unterhaltsame, freundliche und interaktive Einführung in die katastrophalen Risiken von KI!
- [AISafety.world](https://aisafety.world/tiles/). Die gesamte KI-Sicherheitslandschaft mit allen Organisationen, Medien, Foren, Blogs und anderen Akteuren und Ressourcen.
- [IncidentDatabase.ai](https://incidentdatabase.ai/). Datenbank von Vorfällen, bei denen KI-Systeme Schaden verursacht haben.

- [LethalIntelligence.ai](https://lethalintelligence.ai/). Eine Sammlung von Ressourcen zu KI-Risiken und -Alignment.

## Newsletter {#newsletters}

- [PauseAI Substack](https://pauseai.substack.com/): Unser Newsletter.
- [TransformerNews](https://www.transformernews.ai/) Umfassender wöchentlicher Newsletter zu KI-Sicherheit und -Governance.
- [Keine Sorge um die Vase](https://thezvi.substack.com/): Ein Newsletter über KI-Sicherheit, Rationalität und andere Themen.

## Videos {#videos}

- [PauseAI-Playlist](https://www.youtube.com/playlist?list=PLI46NoubGtIJa0JVCBR-9CayxCOmU0EJt) ist eine von uns zusammengestellte YouTube-Playlist mit Videos von 1 Minute bis 1 Stunde in verschiedenen Formaten und von diversen Quellen, die keine Vorkenntnisse erfordern.
- [Robert Miles' YouTube](https://www.youtube.com/watch?v=tlS5Y2vm02c&list=PLfHsskCxi_g-c62a_dmsNuHynaXsRQm40) ist ein großartiger Ort, um die Grundlagen des KI-Alignments zu verstehen.
- [LethalIntelligence's YouTube](https://www.youtube.com/channel/UCLwop3J1O7wL-PNWGjQw8fg)

## Podcasts {#podcasts}

- [DoomDebates](https://www.youtube.com/@DoomDebates) von Liron Shapira, komplett auf KI-Doom fokussiert.
- [For Humanity Podcast](https://www.youtube.com/@ForHumanityPodcast) von Ex-Nachrichtensprecher John Sherman.
- [Future of Life Institute | Connor Leahy über KI-Sicherheit und warum die Welt fragil ist](https://youtu.be/cSL3Zau1X8g?si=0X3EKoxZ80_HN9Rl&t=1803). Interview mit Connor über KI-Sicherheitsstrategien.
- [Lex Fridman | Max Tegmark: Der Fall für das Anhalten der KI-Entwicklung](https://youtu.be/VcVfceTsD0A?t=1547). Interview, das in die Details unserer aktuellen gefährlichen Situation eintaucht.
- [Sam Harris | Eliezer Yudkowsky: KI, Rennen an den Abgrund](https://samharris.org/episode/SE60B0CF4B8). Gespräch über die Natur der Intelligenz, verschiedene Arten von KI, das Alignment-Problem, Ist vs. Soll und vieles mehr. Eine von vielen Episoden, die Making Sense über KI-Sicherheit hat.
- [Connor Leahy, KI-Feueralarm](https://youtu.be/pGjyiqJZPJo?t=2510). Vortrag über die Intelligenzexplosion und warum sie das wichtigste Ereignis wäre, das jemals passieren könnte.
- [Die 80.000 Stunden Podcast empfohlene Episoden zu KI](https://80000hours.org/podcast/on-artificial-intelligence/). Nicht 80.000 Stunden lang, sondern eine Zusammenstellung von Episoden des 80.000 Stunden Podcasts über KI-Sicherheit.
- [Future of Life Institute Podcast-Episoden zu KI](https://futureoflife.org/podcast/?_category_browser=ai). Alle Episoden des FLI-Podcasts über die Zukunft der künstlichen Intelligenz.

Podcasts mit PauseAI-Mitgliedern finden Sie in der [Medienberichterstattung](/press)-Liste.

## Artikel {#articles}

- [Das 'Don't Look Up'-Denken, das uns mit KI zum Untergang bringen könnte](https://time.com/6273743/thinking-that-could-doom-us-with-ai/) (von Max Tegmark)
- [Das Anhalten der KI-Entwicklung reicht nicht aus. Wir müssen sie ganz abschalten](https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/) (von Eliezer Yudkowsky)
- [Der Fall für das Verlangsamen von KI](https://www.vox.com/the-highlight/23621198/artificial-intelligence-chatgpt-openai-existential-risk-china-ai-safety-technology) (von Sigal Samuel)
- [Die KI-Revolution: Der Weg zur Superintelligenz](https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html) (von WaitButWhy)
- [Wie sich Rogue-KIs entwickeln könnten](https://yoshuabengio.org/2023/05/22/how-rogue-ais-may-arise/) (von Yoshua Bengio)

- [Argumente gegen die Ernstnahme von KI-Sicherheit durchdenken](https://yoshuabengio.org/2024/07/09/reasoning-through-arguments-against-taking-ai-safety-seriously/) (von Yoshua Bengio)
- [Das Kompendium](https://www.thecompendium.ai/)
- [Ein schmaler Pfad](https://www.narrowpath.co/)
- [Die Zukunft menschlich halten](https://keepthefuturehuman.ai/)
- Überprüfen Sie den Lesebereich auf [LethalIntelligence.ai](https://lethalintelligence.ai/reading-time/)

Wenn Sie lesen möchten, was Journalisten über PauseAI geschrieben haben, überprüfen Sie die Liste der [Medienberichterstattung](/press).

## Bücher {#books}

- [Wenn jemand es baut, stirbt jeder](https://ifanyonebuildsit.com/) (Eliezer Yudkowsky & Nate Soares, 2025)
- [Unkontrollierbar: Die Bedrohung durch künstliche Superintelligenz und das Rennen, um die Welt zu retten](https://www.goodreads.com/book/show/202416160-uncontrollable) (Darren McKee, 2023). Holen Sie es sich [kostenlos](https://impactbooks.store/cart/47288196366640:1?discount=UNCON-P3SFRS)!

- [Das Alignment-Problem](https://www.goodreads.com/book/show/50489349-the-alignment-problem) (Brian Christian, 2020)
- [Menschlich kompatibel: Künstliche Intelligenz und das Problem der Kontrolle](https://www.goodreads.com/en/book/show/44767248) (Stuart Russell, 2019)
- [Leben 3.0: Mensch sein im Zeitalter der künstlichen Intelligenz](https://www.goodreads.com/en/book/show/34272565) (Max Tegmark, 2017)
- [Superintelligenz: Wege, Gefahren, Strategien](https://www.goodreads.com/en/book/show/20527133) (Nick Bostrom, 2014)

## Papiere {#papers}

- [Eine Zusammenstellung](https://arkose.org/aisafety) von KI-Sicherheitspapieren
- [Eine weitere Zusammenstellung](https://futureoflife.org/resource/introductory-resources-on-ai-risks/#toc-44245428-2) von KI-Sicherheitspapieren
- [Alignment-Fälschung in großen Sprachmodellen](https://www.anthropic.com/news/alignment-faking) aktuelles Papier von Anthropic selbst
- [Management extremer KI-Risiken inmitten schnellen Fortschritts](https://www.science.org/doi/abs/10.1126/science.adn0117) von den Paten des Fachgebiets

## Kurse {#courses}

- [Einführung in transformative KI](https://aisafetyfundamentals.com/intro-to-tai/) (15 Stunden)
- [AGI-Sicherheitsgrundlagen](https://www.agisafetyfundamentals.com/) (30 Stunden)
- [CHAI-Bibliographie empfohlener Materialien](https://humancompatible.ai/bibliography) (50 Stunden+)
- [AISafety.training](https://aisafety.training/): Überblick über Trainingsprogramme, Konferenzen und andere Veranstaltungen

## Organisationen {#organizations}

- [Future of Life Institute](https://futureoflife.org/cause-area/artificial-intelligence/) startete den [offenen Brief](https://futureoflife.org/open-letter/pause-giant-ai-experiments/), geleitet von Max Tegmark.
- [Center for AI Safety](https://www.safe.ai/) (CAIS) ist ein Forschungszentrum an der Tschechischen Technischen Universität in Prag, geleitet von Dan Hendrycks.
- [Conjecture](https://www.conjecture.dev/). Start-up, das an KI-Alignment und KI-Politik arbeitet, geleitet von Connor Leahy.
- [Existenzielles Risiko-Observatorium](https://existentialriskobservatory.org/). Niederländische Organisation, die die Öffentlichkeit über x-Risiken informiert und Kommunikationsstrategien studiert.
- [Zentrum für die Regulierung von KI](https://www.governance.ai/)
- [FutureSociety](https://thefuturesociety.org/about-us/)
- [Zentrum für menschlich kompatible künstliche Intelligenz](https://humancompatible.ai/about/) (CHAI), geleitet von Stuart Russell.
- [Machine Intelligence Research Institute](https://intelligence.org/) (MIRI), das mathematische Forschung zu KI-Sicherheit betreibt, geleitet von Eliezer Yudkowsky.
- [Institut für KI-Politik und -Strategie](https://www.iaps.ai/) (IAPS)
- [Das KI-Politik-Institut](https://theaipi.org/)
- [KI-Sicherheitskommunikationszentrum](https://aiscc.org/2023/11/01/yougov-poll-83-of-brits-demand-companies-prove-ai-systems-are-safe-before-release/)
- [Das Midas-Projekt](https://www.themidasproject.com/) Unternehmensdruckkampagnen für KI-Sicherheit.
- [Das Human Survival Project](https://thehumansurvivalproject.org/)
- [KI-Sicherheitswelt](https://aisafety.world/) Hier finden Sie einen Überblick über die KI-Sicherheitslandschaft.

## Wenn Sie überzeugt sind und handeln möchten {#if-you-are-convinced-and-want-to-take-action}

Es gibt viele [Dinge, die Sie tun können](/action).
Ein Brief schreiben, an einer Demonstration teilnehmen, Geld spenden oder einer Gemeinschaft beitreten ist nicht so schwer!
Und diese Aktionen haben einen realen Einfluss.
Selbst wenn wir dem Ende der Welt gegenüberstehen, kann es noch Hoffnung und sehr lohnende Arbeit geben.

## Oder wenn Sie sich noch nicht ganz sicher sind {#or-if-you-still-dont-feel-quite-sure-of-it}

Das Lernen über die [Psychologie von x-Risiken](/psychology-of-x-risk) könnte Ihnen helfen.
