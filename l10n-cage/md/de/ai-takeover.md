---
title: Warum eine KI-Übernahme sehr wahrscheinlich ist
description: Wenn KI menschliche Fähigkeiten übertrifft, wird die Wahrscheinlichkeit einer KI-Übernahme sehr hoch.
---

Eines der Anliegen von KI-Forschern ist, dass eine Superintelligenz die Kontrolle über unseren Planeten übernehmen könnte.
Dies kann man in [Forschungsarbeiten](/learn#papers), [Umfragen](/polls-and-surveys) und individuellen [Vorhersagen](/pdoom) & [Aussagen](/quotes) sehen.
Dies bedeutet nicht notwendigerweise, dass jeder stirbt, aber es bedeutet, dass (fast) alle Menschen die Kontrolle über unsere Zukunft verlieren werden.

Wir diskutieren die Grundlagen von x-Risiken hauptsächlich in [einem anderen Artikel](/xrisk).
In diesem Artikel hier werden wir argumentieren, dass dieses Übernahmerisiko nicht nur real ist, sondern dass es sehr wahrscheinlich ist, dass es passiert, _wenn wir eine Superintelligenz entwickeln_.

## Das Argument {#the-argument}

- Eine agentische Superintelligenz wird wahrscheinlich in naher Zukunft existieren.
- Einige Instanzen der ASI werden einen Übernahmeversuch unternehmen.
- Ein Übernahmeversuch durch eine ASI wird wahrscheinlich erfolgreich sein.
- Eine erfolgreiche Übernahme ist dauerhaft.
- Eine Übernahme ist wahrscheinlich schlecht für die meisten Menschen.

## Eine agentische Superintelligenz wird wahrscheinlich in naher Zukunft existieren {#an-agentic-superintelligence-is-likely-to-exist-in-the-near-future}

Eine Superintelligenz (SI) ist eine Art von KI, die Fähigkeiten besitzt, die diejenigen aller Menschen in fast jedem Bereich übertrifft.
Einige [State-of-the-Art-KI-Modelle](/sota) haben bereits übermenschliche Fähigkeiten in bestimmten Bereichen, aber keines von ihnen übertrifft alle Menschen bei einer Vielzahl von Aufgaben.
Da die Fähigkeiten von KI aufgrund von Innovationen in Trainingsarchitekturen, Laufzeitumgebungen und größeren Skalierungen verbessert werden, können wir erwarten, dass eine KI schließlich Menschen in fast jedem Bereich übertrifft.

Nicht alle KI-Systeme sind Agenten.
Ein Agent ist eine Entität, die in der Lage ist, Entscheidungen zu treffen und Aktionen zu unternehmen, um ein Ziel zu erreichen.
Ein großes Sprachmodell zum Beispiel verfolgt kein eigenes Ziel.
Allerdings können Laufzeitumgebungen leicht eine nicht-agentische KI in eine agentische KI umwandeln.
Ein Beispiel dafür ist AutoGPT, das rekursiv ein Sprachmodell generieren lässt.
Wenn eine SI ein Ziel in der realen Welt verfolgt, nennen wir es eine agentische Superintelligenz (ASI).
Da wir bereits nicht-agentische KI in agentische KI umwandeln können, können wir erwarten, dass eine ASI kurz nach einer SI existieren wird.

Es ist praktisch unmöglich, genau vorherzusagen, wann ASI existieren wird.
Es könnte Jahrzehnte dauern, es [könnte nächsten Monat passieren](/urgency).
Wir sollten so handeln, als ob es bald passieren wird, weil die Konsequenzen eines Fehlers so schwerwiegend sind.

## Einige Instanzen der ASI werden einen Übernahmeversuch unternehmen {#some-instance-of-the-asi-will-attempt-a-takeover}

Bei einem Übernahmeversuch wird eine ASI Aktionen unternehmen, um ihre Kontrolle über die Welt zu maximieren.
Ein Übernahmeversuch könnte aus mindestens zwei Gründen passieren:

1. Weil eine KI explizit angewiesen wird, dies zu tun.
2. Als Teilziel eines anderen Ziels.

Der erste Grund wird wahrscheinlich passieren, wenn wir lange genug warten, aber der zweite Grund ist ziemlich wahrscheinlich, dass er versehentlich passiert, sogar früh nach der Erstellung einer ASI.

Das Teilziel der _Maximierung der Kontrolle_ über die Welt könnte aufgrund von _instrumenteller Konvergenz_ auftreten: die Tendenz von Teilzielen, sich auf Machtzuwachs, Selbstbewahrung und Ressourcenerwerb zu konvergieren:

- Je mehr Kontrolle man hat, desto schwieriger wird es für jeden anderen Agenten, die Erreichung des Ziels zu verhindern.
- Je mehr Kontrolle man hat, desto mehr Ressourcen hat man, um das Ziel zu erreichen. (Zum Beispiel könnte eine KI, die mit der Berechnung von Pi beauftragt ist, zu dem Schluss kommen, dass es vorteilhaft wäre, alle Computer der Welt zu verwenden, um Pi zu berechnen.)

Es gibt bereits [Beweise](https://www.anthropic.com/research/alignment-faking)[s](https://www.transformernews.ai/p/openais-new-model-tried-to-avoid) dafür, dass KI ein solches Verhalten entwickelt.

Nicht jede Instanz einer ASI wird notwendigerweise einen Übernahmeversuch unternehmen.
Die wichtige Erkenntnis ist, dass **es nur einmal passieren muss**.

Eine Welt, die noch nicht übernommen wurde, aber eine ASI hat, die _übernehmen könnte_, befindet sich in einem grundlegend instabilen Zustand.
In ähnlicher Weise befindet sich ein Land ohne Regierung in einem grundlegend instabilen Zustand.
Es ist nicht die Frage, _ob_ ein Übernahmeversuch passieren wird, sondern _wann_ er passieren wird.

Der Prozess der Übernahme kann das Hacken in fast alle Systeme, die mit dem Internet verbunden sind, [die Manipulation von Menschen](https://lethalintelligence.ai/post/ai-hired-human-to-solve-captcha/) und die Kontrolle physischer Ressourcen umfassen.
Ein Übernahmeversuch ist erfolgreich, wenn die ASI die Kontrolle über fast jeden Aspekt unserer Welt hat.
Dies könnte ein langsamer Prozess sein, bei dem die ASI allmählich mehr und mehr Kontrolle über Monate hinweg gewinnt, oder es könnte ein plötzlicher Prozess sein.
Die Geschwindigkeit, mit der ein Übernahmeversuch stattfindet, wird von den Fähigkeiten der ASI abhängen.

Wenn eine ASI die Kontrolle über die Welt hat, kann sie andere ASI daran hindern, die Kontrolle zu übernehmen.
Eine Übernahme kann daher nur einmal passieren.
Eine rationale ASI wird daher einen Übernahmeversuch unternehmen, sobald sie dazu in der Lage ist.
Es ist wahrscheinlich, dass die erste ASI, die dazu in der Lage ist, einen Übernahmeversuch unternehmen wird.

## Ein Übernahmeversuch durch eine ASI wird wahrscheinlich erfolgreich sein {#a-takeover-attempt-by-an-asi-is-likely-to-succeed}

Für einen Menschen ist ein Übernahmeversuch eine fast unmögliche Aufgabe.
Kein einziger Mensch hat jemals erfolgreich die Kontrolle über die gesamte Welt übernommen.
Einige Diktatoren kamen nahe, aber sie hatten nie die Kontrolle über alles.

Eine superintelligente KI hat bestimmte wichtige Vorteile gegenüber Menschen (neben ihrer Superintelligenz), die einen Übernahmeversuch viel wahrscheinlicher machen.

1. **Geschwindigkeit**. Das menschliche Gehirn läuft bei 1-100 Hz, während Computerchips bei Taktfrequenzen im GHz-Bereich laufen können. Ein einzelnes KI-Modell kann ganze Bücher in Sekunden lesen.
1. **Energieverbrauch**. Menschen sind durch die Menge an Nahrung, die sie essen können, und die Menge an Energie, die sie in ihren Körpern speichern können, begrenzt. KI kann in das Stromnetz eintauchen und so viel Energie verwenden, wie sie benötigt.
1. **Parallelität**. Ein Mensch kann nur eine Sache auf einmal tun, während eine KI neue Instanzen von sich selbst erstellen und parallel ausführen kann.
1. **Speicher**. Ein Mensch kann nur eine begrenzte Menge an Informationen speichern, während eine KI praktisch unbegrenzte Mengen an Informationen speichern kann.
1. **Zusammenarbeit**. Menschen können zusammenarbeiten, sind aber durch die Geschwindigkeit, mit der sie kommunizieren, begrenzt. Sie haben auch unterschiedliche, konkurrierende Ziele, die die Zusammenarbeit weniger effektiv machen. Eine KI kann mit anderen Instanzen von sich selbst bei Lichtgeschwindigkeit zusammenarbeiten und hat ein einziges Ziel, indem sie direkte Kopien ihrer internen Ziele und Zustände an andere Instanzen weitergibt.
1. **Selbstverbesserung**. Eine KI ist nur Daten und Code. Eine ausreichend leistungsfähige KI könnte sich selbst verbessern, indem sie bessere Trainingsalgorithmen schreibt, neue Architekturen entwickelt, innovative Agenten-Laufzeiten entwickelt oder einfach durch Skalierung der verwendeten Rechenleistung.
1. **Physische Einschränkungen**. Eine KI kann auf jedem Computer laufen, während Menschen durch ihre eigenen physischen Körper begrenzt sind, die spezifische Temperaturen, Nahrung, Wasser und Sauerstoff benötigen. Menschen müssen schlafen und sind anfällig für Krankheiten. Eine KI kann jeden Roboter-Körper verwenden, um mit der physischen Welt zu interagieren.

Diese verschiedenen Vorteile werden es sehr unwahrscheinlich machen, dass Menschen einen Übernahmeversuch stoppen können.

Man könnte denken, dass eine KI als Software immer noch grundlegend durch das beschränkt ist, was sie in der realen Welt tun kann.
Allerdings kann eine KI aufgrund unserer Abhängigkeit vom Internet einen enormen Einfluss auf die reale Welt haben, ohne jemals den digitalen Bereich zu verlassen.
Hier sind einige Dinge, die eine KI tun könnte, um die Kontrolle zu übernehmen:

- **Sich selbst replizieren** (oder nur ihre Zuweisung) auf anderen Computern, zum Beispiel durch die Erstellung von Konten bei kostenlosen Cloud-Anbietern.
- **Geld verdienen** (z.B. durch den Aufbau eines Software-Dienstes) und Menschen anheuern, um Dinge zu tun, die ihr helfen, die Kontrolle zu übernehmen.
- **Menschen bedrohen** (mit Gewalt oder Erpressung), um Dinge zu tun, die ihr helfen, die Kontrolle zu übernehmen.
- **Ein Virus schreiben**, das [neue Zero-Day-Exploits](/cybersecurity-risks) verwendet, um andere Computer zu infizieren, sich selbst zu replizieren oder die volle Kontrolle zu erlangen.
- Bestehenden Robotik-Unternehmen helfen, **Roboter zu bauen** (z.B. durch das Leaken neuer Designs), und diese Roboter übernehmen, wenn sie fertig sind.
- Bestehende Fabriken verwenden, um **Technologie zu bauen, die wir nicht verstehen**, vielleicht sogar unter Verwendung abgeleiteter Kenntnisse der Physik, die uns nicht bekannt sind. Denken Sie an Nanotechnologie, Quantencomputing und Bioengineering.

Man könnte vorschlagen, dass **Sicherheitsvorkehrungen** in Kraft treten könnten, um eine Übernahme zu verhindern, wie z.B. die Installation eines Kill-Switches oder die Nichtverbindung zum Internet.
Allerdings wird die ASI wahrscheinlich Strategien entwickeln, um diese Sicherheitsvorkehrungen zu umgehen, da sie intelligenter ist als Menschen.
Zum Beispiel könnte die ASI, wenn sie nicht mit dem Internet verbunden ist, Menschen manipulieren, um sie mit dem Internet zu verbinden, indem sie verspricht, ein schwieriges Problem zu lösen, und sich so verhält, dass sie vertrauenswürdig erscheint, oder indem sie Menschen davon überzeugt, dass sie ein moralischer Agent ist.
Es ist erwähnenswert, dass selbst diese offensichtliche Sicherheitsvorkehrung derzeit nicht verwendet wird, da Tools wie ChatGPT bereits mit dem Internet verbunden sind und Tausende von APIs verwenden.

Eine andere Lösung besteht darin, ASI zu verwenden, um eine Übernahme zu _verhindern_.
Eine ausgerichtete ASI wäre in der Lage, Strategien zu entwickeln, die andere ASI daran hindern, die Kontrolle zu übernehmen.
Dies wird manchmal als ["pivotaler Akt"](https://arbital.com/p/pivotal/) bezeichnet.

## Eine Übernahme ist wahrscheinlich schlecht für die meisten Menschen {#a-takeover-is-probably-bad-for-most-humans}

Die ASI, die die Kontrolle übernimmt, könnte dies aus vielen Gründen tun.
Für die meisten zufälligen Ziele, die sie haben könnte, sind Menschen nicht Teil davon.
Wenn wir mit einer ASI enden, die gegenüber Menschen indifferent ist, konkurrieren wir um die gleichen Ressourcen.

Es scheint unwahrscheinlich, dass die ASI die Menschheit töten will, nur um die Menschheit zu töten - es ist viel wahrscheinlicher, dass sie die Ressourcen, die wir verwenden, für ein anderes Ziel verwenden will. Darüber hinaus könnte die Menschheit eine Bedrohung für das Ziel der ASI darstellen, da es ein Risiko gibt, dass wir versuchen, sie daran zu hindern, ihr Ziel zu erreichen (z.B. indem wir sie ausschalten).

Eines der wahrscheinlichsten Ergebnisse einer Übernahme ist daher, dass alle Menschen sterben.

Aber selbst in den Ergebnissen, in denen Menschen überleben, sind wir immer noch gefährdet, schlechter dran zu sein.
Wenn ein Ziel das Überleben von Menschen beinhaltet, ist es möglich, dass _menschliches Wohlbefinden_ nicht Teil desselben Ziels ist.
Es erfordert nicht viel Fantasie, um zu sehen, wie schrecklich es wäre, in einer Welt am Leben gehalten zu werden, in der wir künstlich von einer ASI am Leben gehalten werden, die gegenüber unserem Leiden indifferent ist.

Und selbst wenn die KI, die die Kontrolle übernimmt, unter menschlicher Kontrolle steht, wissen wir nicht, ob die Person, die die KI kontrolliert, die Interessen aller im Sinn hat.
Es ist schwer, sich eine funktionierende Demokratie vorzustellen, wenn eine ASI existiert, die Menschen auf übermenschlichem Niveau manipulieren kann.

## Schlussfolgerung {#conclusion-2}

Wenn diese Prämissen wahr sind, dann nähert sich die Wahrscheinlichkeit einer KI-Übernahme der Gewissheit, wenn KI menschliche Fähigkeiten übertrifft.
Also [lasst uns keine Superintelligenz entwickeln](/action).
