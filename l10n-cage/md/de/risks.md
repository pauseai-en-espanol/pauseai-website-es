---
title: Risiken der künstlichen Intelligenz
description: KI bedroht unsere Demokratie, unsere Technologie und unsere Spezies.
---

KI ist eine leistungsfähige Technologie, die unsere Welt zunehmend verändert.
Sie bietet enormes Potenzial, aber auch eine enorme Menge an ernsthaften Risiken.
Dies ist ein Versuch, alle Risiken aufzulisten, die durch eine Pause gemildert werden könnten.

## Aktuelle Gefahren {#present-dangers}

### Falschmeldungen, Polarisierung und Bedrohung der Demokratie {#fake-news-polarization-and-threatening-democracy}

Ein großer Teil unserer Gesellschaft basiert auf Vertrauen. Wir vertrauen darauf, dass das Geld auf unserem Bankkonto real ist, dass die Nachrichten, die wir lesen, wahr sind, und dass die Menschen, die Online-Rezensionen posten, existieren.

KI-Systeme sind außergewöhnlich gut darin, gefälschte Medien zu erstellen, auch bekannt als Deepfakes.
Sie können gefälschte Videos, Audioaufnahmen, Texte und Bilder erstellen.
Die Erstellung gefälschter Medien ist nicht neu, aber KI macht es viel billiger und realistischer.
Diese Fähigkeiten verbessern sich rasant.

Vor gerade einmal zwei Jahren haben wir über die lächerlich unrealistischen Dall-E-Bilder gelacht, aber jetzt haben wir [Deepfake-Bilder, die Fotowettbewerbe gewinnen](https://www.theguardian.com/technology/2023/apr/17/photographer-admits-prize-winning-image-was-ai-generated).
Ein KI-generiertes Bild einer Explosion verursachte [Panikverkäufe an der Wall Street](https://www.euronews.com/next/2023/05/23/fake-news-about-an-explosion-at-the-pentagon-spreads-on-verified-accounts-on-twitter).
Ein 10-Sekunden-Audio-Clip oder ein einzelnes Bild kann ausreichen, um ein überzeugendes Deepfake zu erstellen.
Vielleicht noch gefährlicher als die Deepfakes selbst ist, wie die Existenz überzeugender Deepfakes das Vertrauen zerstört.
[Ein reales Bild kann als KI-generiert bezeichnet werden](https://www.axios.com/2024/08/13/trump-crowd-photo-ai-deepfake-truth), und die Menschen werden es glauben.

GPT-4 kann auf eine Weise schreiben, die von Menschen nicht zu unterscheiden ist, aber mit einer viel schnelleren Geschwindigkeit und einem Bruchteil der Kosten.
Wir könnten bald sehen, wie soziale Medien mit gefälschten Diskussionen und Meinungen und gefälschten Nachrichtenartikeln überschwemmt werden, die von echten nicht zu unterscheiden sind.
Auch bekannt als "totes Internet".

Dies führt zu einer Polarisierung zwischen verschiedenen Gruppen von Menschen, die an verschiedene Informationsquellen und Narrative glauben und durch verzerrte Darstellungen dessen, was passiert, ihre Differenzen eskalieren lassen, bis sie in gewalttätige und antidemokratische Reaktionen kulminieren.

Ein Stopp der Entwicklung von Frontier-Modellen (unser [Vorschlag](/proposal)) würde die Modelle, die heute verwendet werden, um gefälschte Medien zu erstellen, nicht stoppen, aber es könnte helfen, zukünftige hochentwickelte Modelle zu verhindern.
Außerdem würde es den Grundstein für zukünftige Regulierungen legen, die darauf abzielen, gefälschte Medien und andere spezifische Probleme, die durch KI verursacht werden, zu mildern. Nicht zu vergessen die Erhöhung der öffentlichen Aufmerksamkeit und des Bewusstseins für diese Gefahren und den Beweis, dass sie angegangen werden können.

### Deepfakes-gestützte Belästigung und Betrug {#deepfakes-powered-harrasment-and-scams}

Deepfakes können nicht nur die Identität berühmter Menschen stehlen und [Desinformation verbreiten](https://time.com/6565446/biden-deepfake-audio/), sondern auch Sie selbst darstellen.
Jeder, der Fotos, Videos oder Audioaufnahmen von jemandem hat (und genug Kenntnisse), kann Deepfakes von ihnen erstellen und sie verwenden, um Betrug zu begehen, sie zu belästigen oder nicht einvernehmliche sexuelle Inhalte zu erstellen.
Etwa [96% aller Deepfake-Inhalte sind sexuelle Inhalte](https://www.technologyreview.com/2019/10/07/132735/deepfake-porn-deeptrace-legislation-california-election-disinformation/).

Sie können eine Zusammenstellung von KI-Vorfällen finden, die hauptsächlich Deepfake-Nachrichten, Betrug und Belästigung sind, [hier](https://incidentdatabase.ai/summaries/incidents/).

Wie im Abschnitt über Falschmeldungen erwähnt, würde unser Vorschlag nicht alle gefälschten Medien verhindern, aber sie könnten in gewissem Umfang reduziert werden.
Ein nicht so kleiner Umfang, wenn man bedenkt, dass KI-Multifunktionssysteme wie Chatbots sehr beliebt geworden sind und wir sie davon abhalten würden, noch leistungsfähiger und beliebter zu werden, was Systeme einschließen könnte, die mit weniger Filtern und trainierbar mit neuen Gesichtern konzipiert sind.

### Vorurteile und Diskriminierung {#biases-and-discrimination}

KI-Systeme werden mit Daten trainiert, und viele der Daten, die wir haben, sind auf irgendeine Weise voreingenommen.
Dies bedeutet, dass KI-Systeme die Vorurteile unserer Gesellschaft erben werden.
Ein automatisiertes Rekrutierungssystem bei Amazon [erbte eine Voreingenommenheit gegen Frauen](https://www.reuters.com/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G).
Schwarze Patienten wurden [weniger wahrscheinlich an einen medizinischen Spezialisten überwiesen](https://www.science.org/doi/full/10.1126/science.aax2342).
Voreingenommene Systeme, die in der Strafverfolgung verwendet werden, wie zum Beispiel Vorhersagealgorithmen für Polizeieinsätze, könnten zu einer unfairen Zielsetzung bestimmter Gruppen führen.
Generative KI-Modelle kopieren nicht nur die Vorurteile aus ihren Trainingsdaten, [sie verstärken sie](https://www.bloomberg.com/graphics/2023-generative-ai-bias/).
Diese Vorurteile treten oft auf, ohne dass die Ersteller des KI-Systems sich dessen bewusst sind.

### Arbeitsplatzverlust, wirtschaftliche Ungleichheit und Instabilität {#job-loss-economic-inequality-and-instability}

Während der industriellen Revolution verloren viele Menschen ihre Arbeitsplätze an Maschinen.
Allerdings wurden neue (oft bessere) Arbeitsplätze geschaffen, und die Wirtschaft wuchs.
Diesmal könnte es anders sein.

KI ersetzt nicht nur unsere Muskeln, wie die Dampfmaschine es tat, sondern auch unsere Gehirne.
Normale Menschen könnten nichts mehr anzubieten haben, was die Wirtschaft benötigt.
Bildgenerierungsmodelle (die stark auf urheberrechtlich geschütztem Material von professionellen Künstlern trainiert werden) wirken sich bereits [auf die Kreativbranche aus](https://cointelegraph.com/news/artists-face-a-choice-with-ai-adapt-or-become-obsolete).
Schriftsteller [streiken](https://www.newscientist.com/article/2373382-why-use-of-ai-is-a-major-sticking-point-in-the-ongoing-writers-strike/).
GPT-4 hat [die Anwaltsprüfung bestanden](https://law.stanford.edu/2023/04/19/gpt-4-passes-the-bar-exam-what-that-means-for-artificial-intelligence-tools-in-the-legal-industry/), kann exzellente schriftliche Inhalte erstellen und kann Code schreiben (wiederum teilweise trainiert auf [urheberrechtlich geschütztem Material](https://www.ischool.berkeley.edu/news/2023/new-research-prof-david-bamman-reveals-chatgpt-seems-be-trained-copyrighted-books)).

Die Menschen, die diese KI-Systeme besitzen, werden in der Lage sein, sie zu kapitalisieren, aber die Menschen, die ihre Arbeitsplätze an sie verlieren, werden es nicht.
Es ist schwierig vorherzusagen, welche Arbeitsplätze als erste ersetzt werden.
Sie könnten Sie arbeitslos und ohne Einkommen lassen, egal wie viel Zeit, Geld und Energie Sie in die Erlangung der Erfahrung und Kenntnisse investiert haben, die Sie haben, und wie wertvoll sie vor einem Moment waren.
Die Art und Weise, wie wir Reichtum in unserer Gesellschaft verteilen, ist nicht auf diese Situation vorbereitet.

Politische Maßnahmen wie ein universelles Grundeinkommen könnten die schlimmsten wirtschaftlichen Folgen verhindern, aber es ist unklar, ob sie rechtzeitig umgesetzt werden.
Sobald unsere Arbeitsplätze ersetzt sind, könnten wir ohne Verhandlungsmacht bleiben, um soziale Netze zu fordern.

Und selbst wenn es uns gelingt, die Probleme im Zusammenhang mit Ungleichheit und Instabilität richtig zu meistern, könnten wir in einer Welt enden, in der unser Sinn für Zweck verloren geht.
Viele Künstler fühlen sich bereits so, da sie sehen, dass ihre Arbeit durch KI ersetzt wird.
Bald könnten wir alle so fühlen.

### Psychische Gesundheit, Sucht und Entfremdung zwischen Menschen {#mental-health-addiction-and-disconnection-between-people}

Soziale Medienunternehmen haben KI-Systeme verwendet, um ihren Profit zu maximieren, während sie unsere Primatengehirne ausnutzen, um unsere psychische Gesundheit zu schädigen.
KI-Chatbots, die Benutzern eine romantische Beziehung anbieten, haben in den letzten Jahren ein enormes Wachstum erlebt, mit mehr als 3 Milliarden Suchergebnissen für "KI-Freundin" auf Google.
Diese KI-Beziehungs-Apps sind [als süchtig machend erwiesen](https://onlinelibrary.wiley.com/doi/10.1002/mar.21899), insbesondere für "einsame, verletzliche Menschen".

Die Unternehmen, die diese Apps kontrollieren, sind motiviert, sie so süchtig machend wie möglich zu machen, und haben eine enorme Macht, indem sie das Verhalten und die Meinungen dieser Modelle prägen.

Eine Pause bei den größten Modellen könnte verhindern, dass sie zu multifunktionalen Chatbots werden, die unsere Bedürfnisse perfekt erfüllen, ohne dass die Menschen die langfristigen Auswirkungen verstehen.

### Automatisierte Ermittlungen (Verlust der Privatsphäre) {#automated-investigation-loss-of-privacy}

Wir hinterlassen viele Spuren im Internet.
Die Verbindung der Punkte ist schwierig und zeitaufwendig, aber KI kann dies jetzt viel billiger machen.
Große Sprachmodelle können jetzt autonom das Internet durchsuchen und sind gut genug, um große Mengen an Daten zu analysieren und interessante Details zu finden.
Dies kann verwendet werden, um Informationen zu finden, die sonst sehr teuer zu finden wären.

- Informationen darüber finden, wo eine Person wahrscheinlich zu einem bestimmten Zeitpunkt ist. Dies kann verwendet werden, um Dissidenten aufzuspüren oder Attentate zu planen.
- Anonyme Konten im Internet mit realen Identitäten verknüpfen. Dies kann verwendet werden, um herauszufinden, wer Informationen weitergibt.

Im September 2024 erstellte eine Gruppe von Studenten [eine App](https://x.com/AnhPhuNguyen1/status/1840786336992682409), die Informationen über Fremde wie Namen, Verwandte und andere persönliche Daten in der erweiterten Realität mithilfe von Gesichtserkennung und LLMs anzeigt.

### Umweltgefahren {#environmental-risks}

Umweltschäden beginnen signifikant zu werden, und die größten KI-Unternehmen planen, ihren Energieverbrauch stark zu erhöhen. Sie können hier lesen, wie KI die Umwelt negativ beeinflussen wird [/environmental].

### Autonome Waffen {#autonomous-weapons}

Unternehmen verkaufen bereits KI-gesteuerte Waffen an Regierungen.
Lanius baut [fliegende Selbstmord-Drohnen](https://www.youtube.com/watch?v=G7yIzY1BxuI), die autonom Feinde identifizieren.
Palantirs [AIP-System](https://www.youtube.com/watch?v=XEM5qz__HOU) verwendet große Sprachmodelle, um Schlachtfelddaten zu analysieren und optimale Strategien zu entwickeln.

Nationen und Waffenunternehmen haben erkannt, dass KI einen enormen Einfluss auf die Überlegenheit gegenüber ihren Feinden haben wird.
Wir sind in ein neues Wettrüsten eingetreten.
Diese Dynamik belohnt das Beschleunigen und die Kürzung von Ecken.

Im Moment haben wir noch Menschen in der Schleife für diese Waffen.
Aber wenn die Fähigkeiten dieser KI-Systeme verbessert werden, wird es immer mehr Druck geben, den Maschinen die Macht zu geben, zu entscheiden.
Wenn wir die Kontrolle über Waffen an KI delegieren, könnten Fehler und Bugs schreckliche Konsequenzen haben.
Die Geschwindigkeit, mit der KI Informationen verarbeiten und Entscheidungen treffen kann, könnte Konflikte in Minuten eskalieren lassen.
Ein [aktueller Artikel](https://arxiv.org/pdf/2401.03408.pdf) kommt zu dem Schluss, dass "Modelle dazu neigen, eine Dynamik des Wettrüstens zu entwickeln, die zu größeren Konflikten und in seltenen Fällen sogar zum Einsatz von Nuklearwaffen führt".

Lesen Sie mehr auf [stopkillerrobots.org](https://www.stopkillerrobots.org/military-and-killer-robots/)

## Gefahren in naher Zukunft {#near-future-dangers}

### Machtakkumulation und Tyrannei {#power-accumulation-and-tyranny}

Leistungsfähige KI-Modelle können verwendet werden, um mehr Macht zu erlangen.
Diese positive Rückkopplungsschleife kann dazu führen, dass einige Unternehmen oder Regierungen eine ungesunde Menge an Macht haben.
Die Kontrolle über Tausende intelligenter, autonomer Systeme könnte verwendet werden, um Meinungen zu beeinflussen, Märkte zu manipulieren oder sogar Krieg zu führen.
In den Händen einer autoritären Regierung könnte dies verwendet werden, um Dissens zu unterdrücken und die Macht aufrechtzuerhalten.

### Biologische Waffen {#biological-weapons}

KI kann Wissen zugänglicher machen, was auch Wissen darüber einschließt, wie biologische Waffen erstellt werden. [Dieser Artikel](https://arxiv.org/abs/2306.03809) zeigt, wie GPT-4 nicht-wissenschaftlichen Studenten helfen kann, einen pandemischen Krankheitserreger zu erstellen:

> In einer Stunde schlugen die Chatbots vier potenzielle pandemische Krankheitserreger vor, erklärten, wie sie aus synthetischer DNA mithilfe von Reverse-Genetik erstellt werden können, lieferten die Namen von DNA-Synthese-Unternehmen, die wahrscheinlich keine Bestellungen überprüfen, identifizierten detaillierte Protokolle und wie man sie debuggt, und empfahlen, dass jeder, der nicht die Fähigkeiten für Reverse-Genetik hat, eine Kernanlage oder eine Vertragsforschungsorganisation beauftragt.

Diese Art von Wissen war noch nie so zugänglich, und wir haben nicht die Sicherheitsvorkehrungen, um mit den möglichen Konsequenzen umzugehen.

Darüber hinaus können einige KI-Modelle verwendet werden, um völlig neue gefährliche Krankheitserreger zu entwerfen.
Ein Modell namens MegaSyn entwarf [40.000 neue chemische Waffen / toxische Moleküle in einer Stunde](https://www.theverge.com/2022/3/17/22983197/ai-new-possible-chemical-weapons-generative-models-vx).
Das revolutionäre AlphaFold-Modell kann die Struktur von Proteinen vorhersagen, was auch eine [Dual-Use-Technologie](https://unicri.it/sites/default/files/2021-12/21_dual_use.pdf) ist.
Die Vorhersage von Proteinstrukturen kann verwendet werden, um "Krankheitsursachen-Mutationen mithilfe der Genomsequenz eines Individuums zu entdecken".
Wissenschaftler erstellen jetzt sogar [vollständig autonome chemische Labors, in denen KI-Systeme neue Chemikalien auf eigene Faust synthetisieren können](https://twitter.com/andrewwhite01/status/1670794000398184451).

Die grundlegende Gefahr besteht darin, dass die Kosten für die Entwicklung und den Einsatz biologischer Waffen durch KI um Größenordnungen gesenkt werden.

### Computerviren und Cybersicherheitsangriffe {#computer-viruses-and-cybersecurity-attacks}

Praktisch alles, was wir heute tun, hängt auf irgendeine Weise von Computern ab.
Wir bezahlen unsere Lebensmittel, planen unsere Tage, kontaktieren unsere Lieben und fahren sogar unsere Autos mit Computern.

Moderne KI-Systeme können Software analysieren und schreiben.
Sie [können Schwachstellen in Software finden](https://betterprogramming.pub/i-used-gpt-3-to-find-213-security-vulnerabilities-in-a-single-codebase-cc3870ba9411) und [könnten verwendet werden, um sie auszunutzen](https://blog.checkpoint.com/2023/03/15/check-point-research-conducts-initial-security-analysis-of-chatgpt4-highlighting-potential-scenarios-for-accelerated-cybercrime/).
Wenn die Fähigkeiten von KI wachsen, werden auch die Fähigkeiten der Exploits, die sie erstellen können, wachsen.

Hochpotente Computerviren waren immer extrem schwierig zu erstellen, aber KI könnte dies ändern.
Anstatt ein Team von geschickten Sicherheitsexperten/Hackern zu engagieren, um Zero-Day-Exploits zu finden, könnte man einfach eine viel billigere KI verwenden, um dies zu tun. Natürlich könnte KI auch bei der Cybersicherheit helfen, und es ist unklar, auf welcher Seite der Vorteil liegt.

[Lesen Sie mehr über KI und Cybersicherheitsrisiken](/cybersecurity-risks)

### Existenzrisiko {#existential-risk}

Viele KI-Forscher warnen, dass KI zum Ende der Menschheit führen könnte.

Sehr intelligente Dinge sind sehr mächtig.
Wenn wir eine Maschine bauen, die viel intelligenter ist als Menschen, müssen wir sicherstellen, dass sie das gleiche will wie wir.
Allerdings stellt sich heraus, dass dies sehr schwierig ist.
Dies wird als _Anpassungsproblem_ bezeichnet.
Wenn wir es nicht rechtzeitig lösen, könnten wir mit superintelligenten Maschinen enden, die sich nicht um unser Wohlergehen kümmern.
Wir würden eine neue Spezies auf dem Planeten einführen, die uns überlisten und überbieten könnte.

[Lesen Sie mehr über x-Risiko](/xrisk)

### Menschliche Entmachtung {#human-disempowerment}

Selbst wenn es uns gelingt, nur KI-Systeme zu erstellen, die wir individuell kontrollieren können, könnten wir unsere Macht, wichtige Entscheidungen zu treffen, schrittweise verlieren, jedes Mal, wenn eines in Institutionen oder den Alltag integriert wird.
Diese Prozesse würden letztendlich mehr Input von KI-Systemen als von Menschen haben, und wenn wir uns nicht schnell genug koordinieren oder entscheidendes Wissen über die Funktionsweise der Systeme fehlt, könnten wir ohne Kontrolle über unsere Zukunft enden.

Es wäre eine Zivilisation, in der jedes System für unterschiedliche Ziele optimiert ist, es keine klare Richtung gibt, wohin alles steuert, und es keine Möglichkeit gibt, dies zu ändern.
Das technische Wissen, das erforderlich ist, um diese Systeme zu ändern, könnte von vornherein fehlen oder im Laufe der Zeit verloren gehen, da wir immer abhängiger von Technologie werden und die Technologie immer komplexer wird.

Die Systeme könnten ihre Ziele erreichen, aber diese Ziele könnten die Werte, die sie ursprünglich haben sollten, nicht vollständig umfassen. Dieses Problem tritt bereits heute in gewissem Umfang auf, aber KI könnte es erheblich verstärken.

### Digitale Empfindung {#digital-sentience}

Wenn KI weiter voranschreitet, könnten zukünftige Systeme unglaublich komplex werden und neuronale Strukturen und Funktionen replizieren, die dem menschlichen Gehirn ähnlicher sind.
Diese erhöhte Komplexität könnte zu emergenten Eigenschaften wie KI-Subjektivität und/oder Bewusstsein führen, was sie moralischen Überlegungen würdig machen würde.

Das Problem ist, dass wir, angesichts unseres gegenwärtigen Mangels an Wissen über Bewusstsein und die Natur neuronaler Netze, keine Möglichkeit haben, zu bestimmen, ob einige KI-Systeme irgendeine Art von Erfahrung haben und was die Qualität dieser Erfahrungen wäre.
Wenn die KI-Systeme weiterhin nur mit ihren Fähigkeiten im Hinterkopf produziert werden, durch einen Prozess, den wir nicht vollständig verstehen, werden die Menschen sie weiterhin als Werkzeuge verwenden, ohne zu berücksichtigen, was ihre Wünsche sein könnten, und dass sie tatsächlich "digitale Menschen" versklaven könnten.

### Leidensrisiken {#suffering-risks}

Es ist nicht nur so, dass ein Wert-Lock-in dazu führen könnte, dass wir die besten Arten von Welten nicht erreichen, sondern es könnte uns auch in Dystopien führen, die schlimmer sind als das Aussterben und sich durch die gesamte Raumzeit erstrecken könnten.

Mögliche eingeschlossene Dystopien mit viel Leiden werden als _S-Risiken_ bezeichnet und umfassen Welten, in denen empfindungsfähige Wesen versklavt und gezwungen werden, schreckliche Dinge zu tun.
Diese Wesen könnten Menschen, Tiere, digitale Menschen oder andere außerirdische Spezies sein, die die KI im Kosmos finden könnte. Angesichts dessen, wie schwierig wir das Anpassungsproblem für lösbar halten, wie schlecht wir Menschen uns manchmal gegenseitig behandeln, wie schlecht wir die meisten Tiere behandeln und wie wir gegenwärtige KI behandeln, scheint eine Zukunft wie diese nicht so unwahrscheinlich, wie wir hoffen.

## Was können wir tun? {#what-can-we-do}

Für **alle** oben diskutierten Probleme steigt das Risiko, wenn die Fähigkeiten von KI verbessert werden.
Dies bedeutet, dass das Sicherste, was wir jetzt tun können, ist, **langsamer zu machen**.
Wir müssen die Entwicklung leistungsfähigerer KI-Systeme pausieren, bis wir herausgefunden haben, wie wir mit den Risiken umgehen können.

Siehe [unseren Vorschlag](/proposal) für weitere Details.
