---
title: Umfragen & Meinungsumfragen
description: Wie sehr sorgen sich normale Menschen und Experten um die Risiken und die Regulierung von künstlicher Intelligenz?
---

## Expertenmeinung zu katastrophalen Risiken {#expert-opinion-on-catastrophic-risks}

- **[KI-Forscher, AIImpacts 2022](https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/)**: schätzen die Wahrscheinlichkeit "wirklich schlechter Ergebnisse (wie dem Aussterben der Menschheit)" auf 14% und die Medianwahrscheinlichkeit auf 5%. 82% halten das Kontrollproblem für wichtig.
- **[KI-Forscher, AIImpacts 2023](https://wiki.aiimpacts.org/ai_timelines/predictions_of_human-level_ai_timelines/ai_timeline_surveys/2023_expert_survey_on_progress_in_ai)**: Die durchschnittliche Wahrscheinlichkeit für ein katastrophales Ereignis (p(doom)) liegt zwischen 14 und 19,4%, je nach Formulierung der Frage. 86% halten das Kontrollproblem für wichtig.
- **[KI-Ingenieure / Startup-Gründer, State of AI Engineering](https://elemental-croissant-32a.notion.site/State-of-AI-Engineering-2023-20c09dc1767f45988ee1f479b4a84135#694f89e86f9148cb855220ec05e9c631)**: Über 60% haben eine [p(doom)](/pdoom) > 25%. Nur 12% haben eine p(doom) = 0.
- **[KI-Sicherheitsforscher, AlignmentForum](https://web.archive.org/web/20221013014859/https://www.alignmentforum.org/posts/QvwSr5LsxyDeaPK5s/existential-risk-from-ai-survey-results)**: Die Befragten schätzten die Wahrscheinlichkeit für ein existenzielles Risiko aufgrund mangelnder technischer Forschung auf 20% und aufgrund eines Versagens von KI-Systemen, das zu tun, was die Menschen, die sie einsetzen, beabsichtigen, auf 30%, mit großer Variation (z.B. gibt es Datenpunkte bei etwa 1% und etwa 99%).

## Öffentliche Meinung zu katastrophalen Risiken {#public-opinion-on-catastrophic-risks}

- **[Bürger im Vereinigten Königreich, PublicFirst](https://publicfirst.co.uk/ai/)**: glauben, dass es eine 9%ige Wahrscheinlichkeit gibt, dass die Menschheit aufgrund von KI aussterben wird. Etwa 50% sagen, sie seien sehr oder einigermaßen besorgt darüber.
- **[Deutsche Bürger, Kira](https://www.zeit.de/digital/2023-04/ki-risiken-angst-umfrage-forschung-kira)**: Nur 14% glauben, dass KI einen positiven Einfluss auf die Welt haben wird, 40% sind unentschieden, 40% negativ.
- **[US-Bürger, RethinkPriorities](https://rethinkpriorities.org/publications/us-public-perception-of-cais-statement-and-the-risk-of-extinction)**: stimmen dem existenziellen Risiko-Statement zu (59%) und unterstützen es (58%). Ablehnung (26%) und Opposition (22%) waren relativ niedrig, und ein erheblicher Anteil der Menschen blieb neutral (12% bzw. 18% für Zustimmung und Unterstützung).
- **[Australische Bürger, Ready Research](https://theconversation.com/80-of-australians-think-ai-risk-is-a-global-priority-the-government-needs-to-step-up-225175)**: 80% glauben, dass das KI-Risiko eine globale Priorität ist, 64% wollen, dass die Regierung sich auf katastrophale Ergebnisse konzentriert (im Vergleich zu nur 25% auf Arbeitsplatzverlust oder 5% auf Voreingenommenheit).

## Öffentliche Meinung zu Regulierung und Governance {#public-opinion-on-regulations--governance}

- **[Bürger im Vereinigten Königreich, YouGov](https://time.com/7213096/uk-public-ai-law-poll/)**: 87% der Briten würden ein Gesetz unterstützen, das KI-Entwickler dazu verpflichtet, die Sicherheit ihrer Systeme vor der Veröffentlichung nachzuweisen, wobei 60% für ein Verbot der Entwicklung von "intelligenter-als-menschlichen" KI-Modellen sind.
- **[US-Bürger, RethinkPriorities](https://forum.effectivealtruism.org/posts/ConFiY9cRmg37fs2p/us-public-opinion-of-ai-policy-and-risk)**: 50% unterstützen eine Pause, 25% lehnen eine Pause ab.
- **[US-Bürger, YouGov](https://www.vox.com/future-perfect/2023/8/18/23836362/ai-slow-down-poll-regulation)**: 72% wollen, dass KI langsamer wird, 8% wollen, dass sie schneller wird. 83% der Wähler glauben, dass KI versehentlich ein katastrophales Ereignis verursachen könnte.
- **[US-Bürger, YouGov](https://theaipi.org/poll-shows-voters-oppose-open-sourcing-ai-models-support-regulatory-representation-on-boards-and-say-ai-risks-outweigh-benefits-2/)**: 73% glauben, dass KI-Unternehmen für Schäden durch die von ihnen entwickelte Technologie haftbar gemacht werden sollten, 67% denken, dass die Leistungsfähigkeit von KI-Modellen eingeschränkt werden sollte, und 65% glauben, dass es wichtiger ist, KI aus den Händen von böswilligen Akteuren fernzuhalten, als die Vorteile von KI allen zugänglich zu machen.
- **[US-Bürger, AIPI](https://www.politico.com/newsletters/digital-future-daily/2023/11/29/exclusive-what-people-actually-think-about-ai-00129147)**: 49:20 unterstützen "einen internationalen Vertrag, um jede 'intelligenter-als-menschliche' künstliche Intelligenz (KI) zu verbieten", 70:14 unterstützen "die Verhinderung, dass KI schnell übermenschliche Fähigkeiten erreicht".
- **[US-Professoren für Informatik, Axios Generation Lab](https://www.axios.com/2023/09/05/ai-regulations-expert-survey)**: Etwa 1 von 5 sagte voraus, dass KI "definitiv" unter menschlicher Kontrolle bleiben wird. Der Rest war zwischen denen, die sagten, KI werde "wahrscheinlich" oder "definitiv" außer Kontrolle geraten, und denen, die sagten, "wahrscheinlich nicht", aufgeteilt.
  Nur 1 von 6 sagte, KI sollte nicht oder kann nicht reguliert werden. Nur eine Handvoll vertraut dem privaten Sektor, sich selbst zu regulieren.
- **[US-Bürger, Sentience Institute](https://www.sentienceinstitute.org/aims-survey-supplement-2023)**: Es gab breite Unterstützung für Schritte, die unternommen werden könnten, um die Entwicklung zu verlangsamen. Die Menschen unterstützten öffentliche Kampagnen, um die KI-Entwicklung zu verlangsamen (71,3%), staatliche Regulierung, die die Entwicklung verlangsamt (71,0%), und eine sechsmonatige Pause bei bestimmten Arten von KI-Entwicklungen (69,1%). Die Unterstützung für ein Verbot von künstlicher allgemeiner Intelligenz (AGI), die intelligenter ist als Menschen, lag bei 62,9%.
- **[Bürger im Vereinigten Königreich, YouGov](https://inews.co.uk/news/politics/voters-deepfakes-ban-ai-intelligent-humans-2708693)**: 74% glauben, dass die Regierung die schnelle Schaffung von übermenschlicher KI verhindern sollte. Über 60% unterstützen einen Vertrag mit einem globalen Verbot von Superintelligenz.
- **[Bürger im Vereinigten Königreich, AISCC](https://aiscc.org/2023/11/01/yougov-poll-83-of-brits-demand-companies-prove-ai-systems-are-safe-before-release/)**: 83% der Menschen sagten, dass die Regierung von KI-Unternehmen verlangen sollte, die Sicherheit ihrer KI-Modelle vor der Veröffentlichung nachzuweisen.
- **[Bürger in den Niederlanden, den USA und dem Vereinigten Königreich, Existential Risk Observatory](https://www.existentialriskobservatory.org/papers_and_reports/Trends%20in%20Public%20Attitude%20Towards%20Existential%20Risk%20And%20Artificial%20Intelligence.pdf)**: Das öffentliche Bewusstsein für existenzielle Risiken wuchs in den USA von 7% auf 15% und in den Niederlanden und dem Vereinigten Königreich auf 19%. Die Unterstützung für eine staatlich angeordnete KI-Pause ist in den USA von 56% auf 66% gestiegen.

## [Zeitpläne](/timelines) {#timelines}

- **[Metaculus Weak AGI](https://www.metaculus.com/questions/3479/date-weakly-general-ai-is-publicly-known/)** vor 2026: 25% Chance, AGI bis 2027: 50% Chance (aktualisiert am 2024-11-05).
- **[Metaculus vollständige AGI](https://www.metaculus.com/questions/5121/date-of-artificial-general-intelligence/)** vor 2028: 25% Chance, vollständige AGI bis 2032: 50% Chance (aktualisiert am 2024-11-05).
