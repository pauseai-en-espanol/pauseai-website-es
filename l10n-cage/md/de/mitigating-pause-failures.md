---
title: Ein Stillstand in der KI-Entwicklung könnte schiefgehen. Wie können wir die Risiken minimieren?
description: Dieser Artikel behandelt einige der Risiken eines Stillstands in der KI-Entwicklung und wie man sie minimieren kann.
---

Wir setzen uns für einen Stillstand in der Entwicklung großer, allgemeiner KI-Modelle ein.
Siehe unseren [Vorschlag](/proposal) für weitere Details.

Diese Maßnahme ist nicht ohne Risiken.
In diesem Artikel werden wir einige dieser Risiken ansprechen und diskutieren, wie man sie minimieren kann.

## Zu frühes Anhalten {#pausing-too-early}

Wenn ein KI-Stillstand eintritt, bevor die Risiken groß genug sind, könnten wir die Vorteile von KI verpassen.
Letztendlich müssen wir die Risiken mit den Kosten eines Stillstands abwägen.

Unserer Meinung nach ist die Wahrscheinlichkeit, dass KI katastrophale Risiken [bald](/urgency) verursacht, bereits groß genug, um einen Stillstand zu rechtfertigen.
Wie Stuart Russell sagte, sollte man bei einem ungewissen Zeitlimit die Aktion wählen, die optimal wäre, wenn man das kürzeste Zeitlimit hätte.

Je länger wir warten, desto mehr Menschen werden denken, dass ein Stillstand nicht möglich ist, und desto mehr Menschen werden fantasieren und in theoretisch mögliche KI-Anwendungen investieren.
Dadurch wird mehr Geld in Lobbyarbeit gegen Menschen wie uns investiert.

Außerdem kann es viele Jahre dauern, von Protesten und Lobbyarbeit bis hin zu Überzeugungsarbeit an Menschen mit Macht, um einen Vertrag in Kraft zu setzen. Abgesehen davon, dass selbst wenn es nicht der Fall wäre, ein früher Stillstand uns Luft verschafft, damit böse Akteure und algorithmische Durchbrüche uns nicht in den Abgrund stürzen.

## Zu kurzer Stillstand / nur 6 Monate {#pausing-for-too-short--only-for-6-months}

Der Stillstand, den wir vorschlagen, hat eine unbestimmte Länge. Wir sollten nicht wieder anfangen, bis es einen breiten Konsens gibt, dass wir wissen, wie man Frontier-KIs sicher entwickeln kann. Technisch und demokratisch gesehen.
Unser Vorschlag ist NICHT "mindestens 6 Monate" wie der [offene Brief des Future of Life Institute](https://futureoflife.org/open-letter/pause-giant-ai-experiments/).

## Zu langer Stillstand {#pausing-for-too-long}

Nick Bostrom, einer der frühen Stimmen, die Bedenken über KI äußerten, befürchtet, dass [irgendwann zu viel über KI-Risiken nachgedacht wird](https://twitter.com/jachaseyoung/status/1723325057056010680), obwohl dieser Moment noch nicht gekommen ist.
Wenn die Besorgnis über KI weiter steigt und wir einen Stillstand erreichen, könnte es zu einer Situation kommen, in der die gesamte KI-Entwicklung tabuisiert oder illegal wird.
Wenn das passiert, werden wir nie die Vorteile von KI ernten, und in der Zwischenzeit könnten wir andere existenzielle Risiken begegnen, die wir mit Hilfe von KI hätten vermeiden können.

Wir können dieses Risiko minimieren, indem wir klar darlegen, unter welchen Bedingungen die KI-Entwicklung wieder aufgenommen werden sollte.
Wie wir sagten, schlagen wir vor, dass die KI-Entwicklung wieder aufgenommen werden sollte, wenn es möglich wird, sichere KI zu bauen.
Außerdem schlagen wir nur vor, die Entwicklung von sehr spezifischen Arten von Modellen zu verbieten: die größten, allgemeinen Modelle.

## Zentralisierung von KI könnte die Übernahmerisiken verschlimmern {#centralization-of-ai-might-make-takeover-risks-worse}

Wir schlagen keine Zentralisierung der KI-Entwicklung in einer einzigen Organisation vor. Das würde die KI-Entwicklung kontrollierbarer machen, aber es würde auch einen einzelnen Punkt des Versagens schaffen, den menschliche Gier und Dummheit ausnutzen könnten.
Die Entscheidung, ob ein CERN-/Apollo-/Manhattan-Projekt gut oder schlecht wäre, sollte multilateral diskutiert werden, sobald wir bereits bei einem Stillstand zusammengearbeitet haben und außerhalb eines Wettrennens sind.

## Dezentralisierung wird dazu führen, dass weniger sicherheitsbewusste Akteure die Entwicklung vorantreiben {#descentralization-will-cause-less-safety-minded-actors-to-lead-the-race}

Wenn Sie in die Geschichte von OpenAI, DeepMind und Anthropic eintauchen, werden Sie feststellen, dass alle von Menschen gegründet wurden, die sich sehr um KI-Risiken sorgen.
In gewisser Weise sind wir froh, dass die größten KI-Unternehmen derzeit KI-Sicherheit als Teil ihrer Kultur haben.
Vielleicht gibt ein Stillstand einer großen Anzahl von Unternehmen die Zeit, aufzuholen, was zu einer großen Gruppe von Unternehmen führen könnte, die weniger sicherheitsbewusst sind.

Wenn wir um einen zeitbasierten Stillstand bitten würden, wäre dies ein berechtigtes Anliegen.
Aber worum wir bitten, ist ein Stillstand, bis wir beweisen können, dass KI sicher gebaut werden kann, also sollten wir nicht mit Organisationen enden, die unsichere KI nach dem Stillstand bauen.

## Nationale/lokale Stillstände könnten scheitern {#national-local-pauses-might-fail}

Wenn ein Land die KI-Entwicklung anhält, werden andere Länder die KI-Entwicklung fortsetzen.
Wir könnten in einer Welt enden, in der die erste AGI von einem nicht kooperativen Akteur entwickelt wird, was wahrscheinlich ein schlechtes Ergebnis wäre.
Die Anreize für einen individuellen Stillstand sind schwach, weil die Vorteile der KI-Entwicklung groß sind und die Risiken der KI-Entwicklung global sind.
Dies ist eine klassische [Gefangenendilemma](https://de.wikipedia.org/wiki/Gefangenendilemma)-Situation.

Die Lösung dafür ist, den Stillstand international durch einen Vertrag zu machen, was wir vorschlagen.
Dies erfordert auch einen starken Durchsetzungsmechanismus.
Länder, die den Vertrag nicht einhalten, sollten bestraft werden.
Wirtschaftliche Sanktionen könnten ausreichen, aber militärische Interventionen könnten in extremen Fällen notwendig sein.

Ein Akteur, von dem einige Menschen glauben, dass er nicht anhält, ist China.
Wir stimmen dieser Einschätzung nicht zu und Sie können hier [mehr darüber lesen](/faq#if-we-pause-what-about-china).

## KI-Entwicklung könnte in den Untergrund gehen {#ai-development-might-go-underground}

Wenn die KI-Entwicklung (über einen bestimmten Schwellenwert hinaus) verboten wird, könnte sie in den Untergrund gehen.
Die potenziellen Vorteile sind so groß, dass ein Rogue-Akteur (Staat) beschließen könnte, KI in Geheimheit zu entwickeln.
Das bedeutet, dass der erste, der Superintelligenz erreicht, ein nicht kooperativer Akteur wäre, was wahrscheinlich ein schlechtes Ergebnis wäre.

Indem wir GPU-Verkäufe verfolgen, können wir große KI-Entwicklungen erkennen.
Da Frontier-Modell-GPU-Cluster immense Mengen an Energie und spezielle Gebäude erfordern, ist die physische Infrastruktur, die zum Trainieren eines großen Modells erforderlich ist, schwer zu verbergen.

Westliche Mächte (USA, Niederlande und Taiwan) kontrollieren die GPU-Lieferkette stark genug, um zu verhindern, dass nicht kooperative Staaten GPUs erhalten.
Nicht-staatliche Akteure sind unwahrscheinlich, dass sie ausreichende Ressourcen in Geheimheit sammeln können, um ein AGI für mindestens ein Jahrzehnt nachdem AGI von Big-Tech-Unternehmen möglich wird, zu trainieren.
Außerdem würde die Tatsache, dass es kein Geschäftsanreiz mehr gibt, dazu beitragen, die Menge an Untergrund-KI-Entwicklung zu reduzieren.

## Hardware-Überhang könnte zu einem schnellen Takeoff führen {#hardware-overhang-could-cause-a-fast-takeoff}

> Wenn wir die Hardware-Forschung und -Entwicklung nicht in den Stillstand einbeziehen, wird die Preis-Leistung von GPUs weiterhin alle 2,5 Jahre verdoppelt, wie es zwischen 2006 und 2021 der Fall war.
> Das bedeutet, dass KI-Systeme nach zehn Jahren mindestens 16-mal schneller und nach zwanzig Jahren 256-mal schneller werden, einfach aufgrund besserer Hardware.
> Wenn der Stillstand aufgehoben wird, würden diese Hardware-Verbesserungen sofort für das Training leistungsfähigerer Modelle zu geringeren Kosten verfügbar werden - ein Hardware-Überhang.
> Dies würde zu einem schnellen und ziemlich diskontinuierlichen Anstieg der KI-Fähigkeiten führen, was möglicherweise zu einem schnellen Takeoff-Szenario und all den damit verbundenen Risiken führen könnte.

[_Von Nora Belrose_](https://bounded-regret.ghost.io/ai-pause-will-likely-backfire-by-nora/)

Dies ist ein ernstes Anliegen, obwohl es starke Argumente dafür gibt, dass [ein Überhang unwahrscheinlich ist](https://blog.aiimpacts.org/p/are-there-examples-of-overhang-for).

PauseAI unterstützt einen Stillstand bei relevanten Rechenleistungsverbesserungen.
Außerdem, wie wir sagten, sollte die "Play"-Taste nicht gedrückt werden, wenn wir immer noch nicht wissen, wie man sichere KI baut.
Und das schließt das Training und die Bereitstellung von Modellen mit fortschrittlicherer Hardware ein.

## KI-Entwicklung ist notwendig, um zu lernen, wie man KIs sicher macht {#ai-development-is-necessary-for-learning-how-to-make-ais-safe}

Die meisten Menschen glauben, dass ein gewisses Maß an prosaischer/inkrementeller Ausrichtung notwendig ist, also wenn ein vollständiger Stillstand ohne Ausnahmen implementiert wird, würde nicht genug Fortschritt bei der Ausrichtung gemacht werden und schließlich würden Akteure, die sich nicht um Sicherheit und den Stillstand kümmern, eine nicht ausgerichtete leistungsfähige KI entwickeln.

Das ist ein Grund, warum wir vorschlagen, bestimmte Trainingsläufe zu genehmigen. Das würde es uns ermöglichen, von größeren Systemen zu lernen, wenn wir ihre Sicherheit gewährleisten können.
Allerdings haben wir in dem schlimmsten Fall, in dem wir ihre Sicherheit nicht gewährleisten können und nicht genug bei der Ausrichtung vorankommen, immer noch die Option, unsere Intelligenz durch andere Technologien zu steigern.

## Algorithmische oder Laufzeitverbesserungen können kleinere Modelle gefährlich machen {#algorithmic-or-runtime-improvements-may-make-smaller-models-dangerous-too}

Wir haben gesehen, dass Änderungen in den Trainingsdaten, Trainingsalgorithmen oder Laufzeitanwendungen zu großen Verbesserungen der Modellleistung führen können.
Deswegen konzentrieren wir uns nicht nur auf die Modellgröße.
Wir [schlagen vor](/proposal), die Entwicklung großer, allgemeiner KI-Modelle anzuhalten, die entweder 1) größer als 10^12 Parameter sind, 2) mehr als 10^25 FLOPs für das Training verwenden oder 3) Fähigkeiten haben, die GPT-4 überschreiten.
Diese dritte Bedingung wird hinzugefügt, um auch kleinere Modelle einzubeziehen, die gefährlich sein können.
Die Durchsetzung einer Obergrenze für Fähigkeiten ist schwierig, da es schwer ist, die Fähigkeiten eines Modells vorherzusagen, bevor es trainiert wird.

Da die Einsätze so hoch sind, sollten wir vorsichtig sein, also unterstützen wir auch einen Stillstand bei relevanten algorithmischen und Laufzeitverbesserungen.
Allerdings wird die Durchsetzung schwieriger sein als die Durchsetzung von Rechenleistungsregulierungen, weil Hardware leichter zu verfolgen ist als Software.

## Wenn wir nur allgemeine KI-Modelle verbieten, könnten wir immer noch AGI durch schmale Modelle erhalten {#if-we-only-ban-general-ai-models-we-might-still-get-agi-through-narrow-models}

Wir möchten gefährliche Modelle einschränken, die [gefährliche Fähigkeiten](/dangerous-capabilities) wie die Manipulation von Menschen, strategisches Planen und das Schreiben von Code haben.
Wir möchten keine sehr schmalen KI-Modelle einschränken, wie Bildklassifizierer in selbstfahrenden Autos oder medizinischer Diagnose.
Glücklicherweise fallen fast alle diese schmalen Modelle außerhalb unserer [vorgeschlagenen](/proposal) Einschränkungen, weil diese Modelle tendenziell relativ klein sind.

Ein ausreichend leistungsfähiges schmales Modell (trainiert auf realen Daten) könnte wahrscheinlich zu gefährlichen Fähigkeiten verallgemeinern.
Ein sehr leistungsfähiges Bildgenerator-Modell könnte beispielsweise Bilder von funktionsfähigem Code erstellen oder ein sehr leistungsfähiges Video-Modell könnte einen Film über eine KI erstellen, die eine erfolgreiche Übernahme plant.
Schmale Modelle werden oft besser in ihrer schmalen Aufgabe, indem sie verallgemeinern.
In gewissem Maße ist dies, was LLMs wie ChatGPT so erfolgreich macht: Sie werden nur trainiert, um "das nächste Wort vorherzusagen", aber um wirklich gut darin zu sein, müssen sie viel über die Welt lernen.

Daher haben wir in unserem Vorschlag "schmal" oder "allgemein" KI nicht definiert, sondern stattdessen drei Bedingungen verwendet, die sich auf die Modellgröße, die verwendete Rechenleistung und die Fähigkeiten beziehen.

## Wenn ein Stillstand implementiert wird, sollten wir einen politischen Kompromiss erwarten {#if-a-pause-is-implemented-we-should-expect-a-political-compromise}

Wir haben einen [spezifischen Vorschlag](/proposal), den wir für optimal halten.
Allerdings sollten wir nicht erwarten, dass unser Vorschlag genau so implementiert wird, wie wir es wollen.
Politik ist chaotisch und unvorhersehbar, also sollten wir erwarten, dass unsere Lobbyarbeit vage richtungsweisende Auswirkungen hat, anstatt präzise Auswirkungen.
Wenn wir eine Form eines Stillstands erhalten, aber es ist nicht genau das, was wir wollen, könnte dies schlimmer sein als gar keinen Stillstand zu haben.
Beispielsweise:

- Ein nationaler Stillstand, der potenziell schlechtere Akteure dazu bringen könnte, als erste AGI zu erreichen
- Ein internationaler Stillstand, der nicht ordnungsgemäß durchgesetzt wird, was zu einem ähnlichen Ergebnis führen könnte

Wir können dies minimieren, indem wir in unseren Kommunikationen konsistent und klar sind, was wir wollen.

## Zu spätes Anhalten {#pausing-too-late}

Dies ist das offensichtlichste und wahrscheinlichste Risiko eines Scheiterns: Wenn wir zu spät anhalten, werden wir wahrscheinlich katastrophale Risiken begegnen.
Und das könnte bald passieren, wie wir auf unserer [Dringlichkeitsseite](/urgency) erklären.

Deswegen brauchen wir Ihre Hilfe, um jetzt für einen [Stillstand](/action) zu sorgen.
