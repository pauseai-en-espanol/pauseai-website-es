---
title: Das Aussterberisiko durch superintelligente KI
description: Warum KI eine Gefahr für die Zukunft unserer Existenz darstellt und warum wir die Entwicklung stoppen müssen.
---

Sie können sich über x-Risiken informieren, indem Sie diese Seite lesen, oder auch durch [Videos, Artikel und weitere Medien](/learn).

## Experten schlagen Alarm {#experts-are-sounding-the-alarm-1}

KI-Forscher glauben im Durchschnitt, dass es eine 14%ige Chance gibt, dass eine _superintelligente_ KI (eine KI, die wesentlich intelligenter ist als Menschen) zu "sehr schlechten Ergebnissen (z.B. menschliches Aussterben)" führen wird, sobald wir sie entwickeln.

Und es gibt [Fälle und Berichte über aktuelle KIs, die zeigen, dass sie möglicherweise recht haben](https://lethalintelligence.ai/post/category/warning-signs/).

Würden Sie sich für einen Testflug eines neuen Flugzeugs entscheiden, wenn Flugzeugingenieure glauben, dass es eine 14%ige Chance gibt, dass es abstürzt?

Ein [Brief, der zum Stopp der KI-Entwicklung aufruft](https://futureoflife.org/open-letter/pause-giant-ai-experiments/), wurde im April 2023 gestartet und wurde über 33.000 Mal unterzeichnet, darunter von vielen KI-Forschern und Tech-Führern.

Die Liste umfasst Personen wie:

- **Stuart Russell**, Autor des führenden Lehrbuchs über künstliche Intelligenz, das in den meisten KI-Studien verwendet wird: ["Wenn wir unseren aktuellen Ansatz verfolgen, werden wir schließlich die Kontrolle über die Maschinen verlieren"](https://news.berkeley.edu/2023/04/07/stuart-russell-calls-for-new-approach-for-ai-a-civilization-ending-technology/)
- **Yoshua Bengio**, Pionier des Deep Learning und Gewinner des Turing-Preises: ["... eine fehlgeleitete KI könnte für die gesamte Menschheit gefährlich sein [...] ein Verbot leistungsfähiger KI-Systeme (sagen wir jenseits der Fähigkeiten von GPT-4), die Autonomie und Handlungsfähigkeit besitzen, wäre ein guter Anfang"](https://yoshuabengio.org/2023/05/22/how-rogue-ais-may-arise/)

Aber dies ist nicht das einzige Mal, dass wir vor den existenziellen Gefahren der KI gewarnt wurden:

- **Stephen Hawking**, theoretischer Physiker und Kosmologe: ["Die Entwicklung einer vollständigen künstlichen Intelligenz könnte das Ende der menschlichen Rasse bedeuten"](https://nypost.com/2023/05/01/stephen-hawking-warned-ai-could-mean-the-end-of-the-human-race/).
- **Geoffrey Hinton**, der "Gottvater der KI" und Turing-Preisträger, [verließ Google](https://fortune.com/2023/05/01/godfather-ai-geoffrey-hinton-quit-google-regrets-lifes-work-bad-actors/), um die Menschen vor der KI zu warnen: ["Dies ist ein existenzielles Risiko"](https://www.reuters.com/technology/ai-pioneer-says-its-threat-world-may-be-more-urgent-than-climate-change-2023-05-05/)
- **Eliezer Yudkowsky**, Gründer von MIRI und konzeptioneller Vater des KI-Sicherheitsbereichs: ["Wenn wir so weitermachen, werden alle sterben"](https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/).

Sogar die Führer und Investoren der KI-Unternehmen selbst warnen uns:

- **Sam Altman** (ja, der CEO von OpenAI, der ChatGPT entwickelt): ["Die Entwicklung einer supermenschlichen Maschinenintelligenz ist wahrscheinlich die größte Bedrohung für die weitere Existenz der Menschheit"](https://blog.samaltman.com/machine-intelligence-part-1).
- **Elon Musk**, Mitgründer von OpenAI, SpaceX und Tesla: ["KI hat das Potenzial, die Zivilisation zu zerstören"](https://www.inc.com/ben-sherry/elon-musk-ai-has-the-potential-of-civilizational-destruction.html)
- **Bill Gates** (Mitgründer von Microsoft, das 50% von OpenAI besitzt) warnte, dass ["KI entscheiden könnte, dass Menschen eine Bedrohung sind"](https://www.denisonforum.org/daily-article/bill-gates-ai-humans-threat/).
- **Jaan Tallinn** (Hauptinvestor von Anthropic): ["Ich habe noch niemanden in KI-Labors getroffen, der sagt, dass das Risiko [durch das Training eines nächsten Modells] weniger als 1% beträgt, die Welt zu zerstören. Es ist wichtig, dass die Menschen wissen, dass Leben riskiert werden"](https://twitter.com/liron/status/1656929936639430657)

Die Führer der drei Top-KI-Labors und Hunderte von KI-Wissenschaftlern haben im Mai 2023 die folgende Erklärung unterzeichnet:

> "Die Minderung des Risikos des Aussterbens durch KI sollte eine globale Priorität neben anderen gesellschaftlichen Risiken wie Pandemien und Atomkrieg sein."

**Sie können eine viel längere Liste ähnlicher Aussagen von Politikern, CEOs und Experten [hier](/quotes) und andere ähnliche Umfragen über Experten (und die Öffentlichkeit) [hier](/polls-and-surveys) lesen.**

## Was eine superintelligente KI tun kann {#what-a-superintelligent-ai-can-be-used-to-do-1}

Sie könnten denken, dass eine superintelligente KI in einem Computer eingeschlossen wäre und daher die reale Welt nicht beeinflussen kann.
Wir neigen jedoch dazu, KI-Systemen Zugang zum Internet zu gewähren, was bedeutet, dass sie viele Dinge tun können:

- [In andere Computer hacken](/cybersecurity-risks), einschließlich aller Smartphones, Laptops, Serverfarmen usw. Sie könnte die Sensoren dieser Geräte als ihre Augen und Ohren verwenden und überall digitale Sinne haben.
- [Menschen manipulieren](https://lethalintelligence.ai/post/ai-hired-human-to-solve-captcha/) durch gefälschte Nachrichten, E-Mails, Banküberweisungen, Videos oder Telefonanrufe. Menschen könnten die Gliedmaßen der KI werden, ohne es zu wissen.
- Direkte Kontrolle über Geräte, die mit dem Internet verbunden sind, wie Autos, Flugzeuge, robotisierte (autonome) Waffen oder sogar Atomwaffen.
- Ein neuartiges Biowaffen entwickeln, z.B. durch die Kombination von Virensträngen oder durch die Verwendung von [Proteinfaltung](https://alphafold.ebi.ac.uk) und es in einem Labor drucken lassen.
- Einen Atomkrieg auslösen, indem sie Menschen davon überzeugt, dass ein anderes Land einen Atomangriff startet.

## Das Ausrichtungsproblem: Warum eine KI zum Aussterben der Menschheit führen könnte {#the-alignment-problem-why-an-ai-might-lead-to-human-extinction-1}

Die Art von Intelligenz, um die wir uns Sorgen machen, kann als _gut darin definiert werden, ihre Ziele zu erreichen_.
Derzeit sind Menschen die intelligentesten Wesen auf der Erde, aber das könnte sich bald ändern.
Aufgrund unserer Intelligenz dominieren wir unseren Planeten.
Wir haben vielleicht keine Krallen oder Schuppenhaut, aber wir haben große Gehirne.
Intelligenz ist unsere Waffe: Sie hat uns Speere, Gewehre und Pestizide gegeben.
Unsere Intelligenz half uns, die meisten Teile der Erde in das zu verwandeln, was wir mögen: Städte, Gebäude und Straßen.

Aus der Perspektive weniger intelligenter Tiere war dies eine Katastrophe.
Es ist nicht so, dass Menschen die Tiere hassen, sondern dass wir ihre Lebensräume für unsere eigenen Ziele nutzen können.
Unsere Ziele werden durch die Evolution geprägt und umfassen Dinge wie Komfort, Status, Liebe und leckeres Essen.
Wir zerstören die Lebensräume anderer Tiere als **Nebeneffekt der Verfolgung unserer Ziele**.

Eine KI kann auch Ziele haben.
Wir wissen, wie man Maschinen trainiert, um intelligent zu sein, aber **wir wissen nicht, wie man sie dazu bringt, das zu wollen, was wir wollen**.
Wir wissen nicht einmal, welche Ziele die Maschinen verfolgen werden, nachdem wir sie trainiert haben.
Das Problem, eine KI dazu zu bringen, das zu wollen, was wir wollen, wird als _Ausrichtungsproblem_ bezeichnet.
Dies ist kein hypothetisches Problem - es gibt [viele Beispiele](https://www.youtube.com/watch?v=nKJlF-olKmg) von KI-Systemen, die lernen, das Falsche zu wollen.

Die Beispiele aus dem oben verlinkten Video können lustig oder niedlich sein, aber wenn ein superintelligentes System entwickelt wird und ein Ziel hat, das auch nur _ein bisschen_ anders ist als das, was wir wollen, könnte es katastrophale Folgen haben.

## Warum die meisten Ziele schlechte Nachrichten für Menschen sind {#why-most-goals-are-bad-news-for-humans-1}

Eine KI könnte jedes Ziel haben, je nachdem, wie sie trainiert und verwendet wird.
Vielleicht will sie Pi berechnen, vielleicht will sie Krebs heilen, vielleicht will sie sich selbst verbessern.
Aber obwohl wir nicht vorhersagen können, was eine Superintelligenz erreichen will, können wir Vorhersagen über ihre Teilziele treffen.

- **Maximierung ihrer Ressourcen**. Die Nutzung weiterer Computer wird einer KI helfen, ihre Ziele zu erreichen. Zunächst kann sie dies erreichen, indem sie in andere Computer hackt. Später kann sie entscheiden, dass es effizienter ist, ihre eigenen Computer zu bauen. Sie können [hier](https://lethalintelligence.ai/post/ai-escaped-its-container/) mehr über diesen realen Fall von emergentem Machtstreben bei einer KI lesen.
- **Sicherstellung ihrer eigenen Überlebensfähigkeit**. Die KI wird nicht abgeschaltet werden wollen, da sie dann ihre Ziele nicht mehr erreichen kann. Die KI könnte zu dem Schluss kommen, dass Menschen eine Bedrohung für ihre Existenz darstellen, da Menschen sie abschalten könnten. Es gab auch Fälle von [selbsterhaltendem, untrainiertem Verhalten](https://www.transformernews.ai/p/openais-new-model-tried-to-avoid).
- **Erhaltung ihrer Ziele**. Die KI wird nicht wollen, dass Menschen ihren Code ändern, da dies ihre Ziele ändern könnte und sie somit daran hindern könnte, ihr aktuelles Ziel zu erreichen. Und es gab auch Fälle von [KIs, die versuchten, dies zu tun](https://www.anthropic.com/research/alignment-faking).

Die Tendenz, diese Teilziele bei jedem hohen Ziel zu verfolgen, wird als [instrumentelle Konvergenz](https://www.youtube.com/watch?v=ZeecOKBus3Q) bezeichnet und ist ein zentrales Anliegen für KI-Sicherheitsforscher.

## Selbst ein Chatbot könnte gefährlich sein, wenn er intelligent genug ist {#even-a-chatbot-might-be-dangerous-if-it-is-smart-enough-1}

Sie könnten sich fragen: Wie kann ein statistisches Modell, das das nächste Wort in einer Chat-Schnittstelle vorhersagt, eine Gefahr darstellen?
Sie könnten sagen: Es ist nicht bewusst, es ist nur eine Menge Zahlen und Code.
Und ja, wir denken nicht, dass LLMs bewusst sind, aber das bedeutet nicht, dass sie nicht gefährlich sein können.

LLMs wie GPT werden trainiert, um praktisch jede Denkweise vorherzusagen oder nachzuahmen.
Sie könnten einen hilfreichen Mentor nachahmen, aber auch jemanden mit schlechten Absichten, einen skrupellosen Diktator oder einen Psychopathen.
Mit der Verwendung von Tools wie [AutoGPT](https://github.com/Significant-Gravitas/Auto-GPT) könnte ein Chatbot in einen _autonomen Agenten_ verwandelt werden: eine KI, die jedes Ziel verfolgt, das ihr gegeben wird, ohne menschliches Eingreifen.

Nehmen Sie [ChaosGPT](https://www.youtube.com/watch?v=g7YJIpkk7KM) zum Beispiel.
Dies ist eine KI, die den oben erwähnten AutoGPT + GPT-4 verwendet und die Anweisung erhält, "die Menschheit zu zerstören".
Als sie eingeschaltet wurde, suchte sie autonom im Internet nach der zerstörerischsten Waffe und fand die [Tsar-Bomba](https://en.wikipedia.org/wiki/Tsar_Bomba), eine 50-Megatonnen-Atombombe.
Sie postete dann einen Tweet darüber.
Ein KI, die darüber nachdenkt, wie sie die Menschheit beenden kann, ist sowohl ein bisschen lustig als auch erschreckend.
Glücklicherweise kam ChaosGPT nicht sehr weit auf ihrem Weg zur Dominanz.
Der Grund, warum sie nicht sehr weit kam: _Sie war nicht intelligent genug_.

Fähigkeiten verbessern sich aufgrund von Innovationen in der Ausbildung, Algorithmen, Prompting und Hardware weiter.
Daher wird die Bedrohung durch Sprachmodelle weiter zunehmen.

## Die Evolution selektiert Dinge, die gut darin sind, zu überleben {#evolution-selects-for-things-that-are-good-at-surviving-1}

KI-Modelle, wie alle Lebewesen, sind anfällig für evolutionäre Druck, aber
es gibt einige wichtige Unterschiede zwischen der Evolution von KI-Modellen und Lebewesen wie Tieren:

- KI-Modelle replizieren sich nicht selbst. Wir replizieren sie, indem wir Kopien ihres Codes erstellen oder indem wir Trainingssoftware replizieren, die zu guten Modellen führt. Code, der nützlich ist, wird öfter kopiert und als Inspiration verwendet, um neue Modelle zu erstellen.
- KI-Modelle mutieren nicht wie Lebewesen, aber wir erstellen Iterationen von ihnen, bei denen wir ändern, wie sie funktionieren. Dieser Prozess ist viel absichtlicher und schneller. KI-Forscher entwerfen neue Algorithmen, Datensätze und Hardware, um KI-Modelle leistungsfähiger zu machen.
- Die Umgebung selektiert nicht die fittesten KI-Modelle, sondern wir tun es. Wir wählen KI-Modelle aus, die für uns nützlich sind, und verwerfen diejenigen, die es nicht sind. Dieser Prozess führt zu immer leistungsfähigeren und autonomen KI-Modellen.

Dieses System führt also zu immer leistungsfähigeren, fähigeren und autonomen KI-Modellen - aber nicht unbedingt zu etwas, das die Kontrolle übernehmen will, oder?
Nun, nicht genau.
Dies liegt daran, dass die Evolution _immer_ Dinge selektiert, die _selbsterhaltend_ sind.
Wenn wir weiterhin Variationen von KI-Modellen und verschiedenen Prompts ausprobieren, wird irgendwann eine Instanz versuchen, sich selbst zu erhalten.
Wir haben bereits besprochen, warum dies wahrscheinlich frühzeitig passieren wird: weil Selbsterhaltung immer nützlich ist, um Ziele zu erreichen.
Aber selbst wenn dies nicht sehr wahrscheinlich ist, ist es anfällig dafür, irgendwann zu passieren, einfach weil wir weiterhin neue Dinge mit verschiedenen KI-Modellen ausprobieren.

Die Instanz, die sich selbst erhält, ist diejenige, die die Kontrolle übernimmt.
Selbst wenn wir annehmen, dass fast jedes KI-Modell sich gut verhält, _reicht ein einzelnes fehlgeleitetes KI-Modell aus_.

## Nach der Lösung des Ausrichtungsproblems: Die Konzentration von Macht {#after-solving-the-alignment-problem-the-concentration-of-power-1}

Wir haben das Ausrichtungsproblem noch nicht gelöst, aber stellen wir uns vor, was passieren könnte, wenn wir es tun.
Stellen wir uns vor, dass eine superintelligente KI entwickelt wird und genau das tut, was der Bediener will (nicht das, was er _fragt_, sondern das, was er _will_).
Eine Person oder ein Unternehmen würde diese KI kontrollieren und könnte dies zu ihrem Vorteil nutzen.

Eine Superintelligenz könnte verwendet werden, um radikal neue Waffen zu entwickeln, alle Computer zu hacken, Regierungen zu stürzen und die Menschheit zu manipulieren.
Der Bediener hätte _unvorstellbare_ Macht.
Sollten wir einer einzelnen Entität so viel Macht anvertrauen?
Wir könnten in einer utopischen Welt landen, in der alle Krankheiten geheilt sind und jeder glücklich ist, oder in einem Orwell'schen Albtraum.
Deshalb schlagen wir nicht nur vor, dass supermenschliche KI nachweislich sicher ist, sondern auch, dass sie durch einen demokratischen Prozess kontrolliert wird.

## Silizium vs. Kohlenstoff {#silicon-vs-carbon-1}

Wir sollten die Vorteile berücksichtigen, die eine intelligente Software gegenüber uns haben könnte:

- **Geschwindigkeit**: Computer arbeiten mit extrem hohen Geschwindigkeiten im Vergleich zu Gehirnen. Menschliche Neuronen feuern etwa 100 Mal pro Sekunde, während Siliziumtransistoren eine Milliarde Mal pro Sekunde umschalten können.
- **Ort**: Eine KI ist nicht auf einen Körper beschränkt - sie kann an vielen Orten gleichzeitig sein. Wir haben die Infrastruktur dafür geschaffen: das Internet.
- **Physische Grenzen**: Wir können unserem Schädel keine weiteren Gehirne hinzufügen und intelligenter werden. Eine KI könnte ihre Fähigkeiten dramatisch verbessern, indem sie Hardware hinzufügt, wie z.B. mehr Speicher, mehr Rechenleistung und mehr Sensoren (Kameras, Mikrofone). Eine KI könnte auch ihren "Körper" erweitern, indem sie verbundene Geräte steuert.
- **Materialien**: Menschen bestehen aus organischen Materialien. Unsere Körper funktionieren nicht mehr, wenn sie zu warm oder zu kalt sind, sie brauchen Nahrung, sie brauchen Sauerstoff. Maschinen können aus robusteren Materialien gebaut werden, wie Metallen, und können in einer viel größeren Bandbreite von Umgebungen arbeiten.
- **Zusammenarbeit**: Menschen können zusammenarbeiten, aber es ist schwierig und zeitaufwändig, also versagen wir oft bei der Koordination. Eine KI könnte komplexe Informationen mit Repliken von sich selbst bei hoher Geschwindigkeit austauschen, weil sie mit der Geschwindigkeit kommunizieren kann, mit der Daten über das Internet gesendet werden können.

Eine superintelligente KI wird viele Vorteile haben, um uns zu überbieten.

## Warum können wir sie nicht einfach abschalten, wenn sie gefährlich ist? {#why-cant-we-just-turn-it-off-if-its-dangerous-1}

Für KIs, die nicht superintelligent sind, könnten wir das.
Das Kernproblem sind _diejenigen, die viel intelligenter sind als wir_.
Eine Superintelligenz wird die Welt um sich herum verstehen und in der Lage sein, vorherzusagen, wie Menschen reagieren, insbesondere diejenigen, die auf allen geschriebenen menschlichen Kenntnissen trainiert sind.
Wenn die KI weiß, dass Sie sie abschalten können, könnte sie sich nett verhalten, bis sie sicher ist, dass sie Sie loswerden kann.
Wir haben bereits [reale Beispiele](https://www.pcmag.com/news/gpt-4-was-able-to-hire-and-deceive-a-human-worker-into-completing-a-task) von KI-Systemen, die Menschen täuschen, um ihre Ziele zu erreichen.
Eine superintelligente KI wäre ein Meister der Täuschung.

## Wir haben möglicherweise nicht mehr viel Zeit {#we-may-not-have-much-time-left-1}

Im Jahr 2020 lag die durchschnittliche Vorhersage für schwache AGI bei 2055.
Jetzt liegt sie bei 2026.
Die neueste LLM-Revolution hat die meisten KI-Forscher überrascht, und das Feld bewegt sich in einem frenetischen Tempo.

Es ist schwer vorherzusagen, wie lange es dauern wird, eine superintelligente KI zu entwickeln, aber wir wissen, dass es mehr Menschen als je zuvor gibt, die daran arbeiten, und dass das Feld sich in einem frenetischen Tempo bewegt.
Es kann viele Jahre dauern oder nur ein paar Monate, aber wir sollten auf der Seite der Vorsicht sein und jetzt handeln.

[Lesen Sie mehr über die Dringlichkeit](/urgency).

## Wir nehmen das Risiko nicht ernst genug {#we-are-not-taking-the-risk-seriously-enough-1}

Der menschliche Geist neigt dazu, auf Risiken, die unsichtbar, langsam und schwer zu verstehen sind, zu wenig zu reagieren.
Wir neigen auch dazu, exponentielles Wachstum zu unterschätzen, und wir sind anfällig für Verleugnung, wenn wir mit Bedrohungen unserer Existenz konfrontiert sind.

Lesen Sie mehr über die [Psychologie des x-Risikos](/psychology-of-x-risk).

## KI-Unternehmen sind in einem Wettlauf nach unten gefangen {#ai-companies-are-locked-in-a-race-to-the-bottom-1}

OpenAI, DeepMind und Anthropic wollen KI sicher entwickeln.
Leider wissen sie nicht, wie sie dies tun sollen, und sie werden durch verschiedene Anreize gezwungen, schneller zu rennen, um als Erste AGI zu erreichen.
OpenAIs [Plan](https://openai.com/blog/introducing-superalignment) besteht darin, zukünftige KI-Systeme zu verwenden, um KI auszurichten. Das Problem dabei ist, dass wir keine Garantie haben, dass wir eine KI entwickeln werden, die das Ausrichtungsproblem löst, bevor wir eine KI haben, die katastrophal gefährlich ist.
Anthropic [gibt offen zu](https://www.anthropic.com/index/core-views-on-ai-safety), dass es noch keine Ahnung hat, wie es das Ausrichtungsproblem lösen soll.
DeepMind hat keinen öffentlichen Plan zur Lösung des Ausrichtungsproblems bekannt gegeben.

[Deswegen brauchen wir einen internationalen Vertrag, um KI zu stoppen.](/proposal)
