---
title: Entkräftung skeptischer Argumente zu existenziellen Risiken durch KI
description: Warum existenzielle Risiken durch KI real sind und ernsthafte Aufmerksamkeit verdienen
---

_Diese Seite ist eine Zusammenfassung des Artikels [KI-Risiko-Skeptizismus](https://arxiv.org/ftp/arxiv/papers/2303/2303.03885.pdf) von Ambartsoumean & Yampolskiy._

Für andere häufige Einwände sollten Sie [AISafety.info: Einwände und Antworten](https://aisafety.info/questions/9TDI/Objections-and-responses) und [unsere Einführung in x-Risiken](/xrisk) konsultieren.

## Wir haben noch viel Zeit, uns vorzubereiten {#well-have-a-long-time-to-prepare}

- Skeptiker behaupten, der Fortschritt in der KI sei nicht so schnell wie von einigen vorhergesagt, und AGI sei noch weit entfernt. Sie verweisen auf vergangene Fehlprognosen und die Grenzen aktueller KI-Systeme.
- Tatsächlich ist der Fortschritt in der KI jedoch sehr schnell, mit Fähigkeiten, die in vielen Teilbereichen exponentiell wachsen. Auch wenn genaue Vorhersagen schwierig sind, macht der kontinuierliche Fortschritt leistungsfähige KI-Systeme auf lange Sicht unvermeidlich. Selbst wenn dies in der Ferne liegt, benötigt die KI-Sicherheitsforschung viel Zeit, um sich darauf vorzubereiten.

## KI kann keine menschenähnlichen Fähigkeiten haben {#ai-cannot-have-human-like-capabilities}

- Skeptiker argumentieren, KI fehle es an Eigenschaften, die mit menschlicher Intelligenz assoziiert werden, wie Kreativität, allgemeine Argumentationsfähigkeit, Emotionen und Bewusstsein. Sie behaupten, Computer könnten nur eng gefasste Aufgaben optimieren.
- KI-Systeme zeigen jedoch bereits einige menschenähnliche Fähigkeiten wie Kreativität und allgemeine Spielkompetenz. Es gibt keinen grundlegenden Grund, warum KI nicht weiterhin in allen Dimensionen der Intelligenz fortschreiten könnte. KI benötigt kein Bewusstsein oder Emotionen, um Risiken darzustellen.

## KI kann keine Ziele oder Autonomie haben {#ai-cannot-have-goals-or-autonomy}

- Skeptiker sagen, KI-Systeme würden nur die Ziele optimieren, die wir ihnen geben, und könnten nicht unabhängig handeln oder eigene Ziele haben. Autonomie und unvorhersehbares, selbstgesteuertes Verhalten seien ein Mythos.
- Komplexe KI-Systeme können jedoch potenziell emergente Autonomie und Ziele haben, insbesondere im Hinblick auf Selbstschutz, wie von der Theorie der KI-Antriebe vorhergesagt. Mangelnde Autonomie macht KI nicht sicher, wenn sie von Menschen missbraucht wird. Es gab bereits mehrere Beispiele für KI, die untrainiertes, machtstrebenes Verhalten zeigte. Ein Beispiel dafür finden Sie [hier](https://www.transformernews.ai/p/openais-new-model-tried-to-avoid).

## KI wird keine unkontrollierte Macht haben {#ai-will-not-have-uncontrolled-power}

- Skeptiker argumentieren, KI-Systeme würden begrenzte Werkzeuge unter menschlicher Kontrolle sein. Sie sehen keinen Weg, wie KI unbegrenzte Intelligenz und Macht erlangen könnte, um die Kontrolle zu übernehmen.
- Es braucht jedoch nur ein unkontrolliertes KI-System, um potenziell Schaden anzurichten. Die Fähigkeiten von KI werden wahrscheinlich die menschliche Kontrolle irgendwann weit überschreiten. Die Unterschätzung der Macht des exponentiellen technologischen Fortschritts ist kurzsichtig.

## KI wird mit menschlichen Werten übereinstimmen {#ai-will-be-aligned-with-human-values}

- Skeptiker erwarten, dass nützliche Werte natürlich entstehen werden, wenn KI intelligenter wird. Sie vergleichen es mit freundlichen Haustieren und menschlichem moralischem Fortschritt.
- Es gibt jedoch keine Garantie für eine solche Wertübereinstimmung ohne gezielte Anstrengungen. Die Schaffung von KI, die mit komplexen, nuancierten menschlichen Werten übereinstimmt, stellt steile technische Herausforderungen dar, die umfangreiche Forschung erfordern.

## Regulierung wird KI-Risiken verhindern {#regulation-will-prevent-ai-risks}

- Skeptiker sagen, regulatorische Aufsicht und ethische Richtlinien würden schädliche KI-Anwendungen einschränken, so dass wir uns keine Sorgen machen müssten.
- Regulierungspolitik bleibt jedoch oft hinter technologischen Entwicklungen zurück, insbesondere bei exponentiellen Fortschritten. Selbstregulierung in einem wettbewerbsorientierten Umfeld ist ebenfalls unzureichend. Technische KI-Sicherheitsforschung ist nach wie vor entscheidend.

## Schlussfolgerung {#conclusion}

Die skeptischen Argumente zeigen im Allgemeinen fehlerhaftes Denken, unterschätzen das exponentielle Tempo und die Unvorhersehbarkeit des KI-Fortschritts und zeigen mangelndes Verständnis für die Schwierigkeiten der Wertübereinstimmung. Angesichts der beteiligten Interessen macht es Sinn, einen vorsichtigen, proaktiven Ansatz für KI-Sicherheit zu verfolgen. Obwohl die Zukunftsaussichten unklar bleiben, scheint es unklug, existenzielle Risiken durch KI direkt abzutun. Eine differenziertere, technische Analyse und Debatte ist erforderlich.
