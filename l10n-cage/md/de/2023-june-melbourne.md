---
title: PauseAI-Protest in Melbourne - 16. Juni
description: Schließt euch PauseAI für eine bevorstehende friedliche Demonstration am Melbourne Convention and Exhibition Centre (MCEC) an, wo Sam Altman einen Vortrag halten wird.
---

<script>
    import WidgetConsent from '$lib/components/widget-consent/WidgetConsent.svelte'
</script>

<WidgetConsent>
<div>
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Today we protested in Melbourne, where OpenAI&#39;s Sam Altman was speaking. OpenAI aims to build a superintelligence, which has a serious chance to kill everyone on earth. We&#39;re demanding our governments to step in and <a href="https://twitter.com/hashtag/PauseAI?src=hash&amp;ref_src=twsrc%5Etfw">#PauseAI</a>.<br><br>Press release: <a href="https://t.co/xu7XXTUUyT">https://t.co/xu7XXTUUyT</a> <a href="https://t.co/HtYymXpqjf">https://t.co/HtYymXpqjf</a></p>&mdash; PauseAI (@pause_ai_info) <a href="https://twitter.com/pause_ai_info/status/1669809871867240451?ref_src=twsrc%5Etfw">June 16, 2023</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</div>
</WidgetConsent>

Schließt euch #PauseAI für eine bevorstehende friedliche Demonstration am Melbourne Convention and Exhibition Centre (MCEC) an, wo Sam Altman einen Vortrag halten wird.

- Datum und Uhrzeit: Freitag, 16. Juni, 14 Uhr AEST
- Ort: Haupteingang des MCEC, 1 Convention Centre Place, South Wharf, VIC 3006, Australien
- Demonstrationszeiten: 13:30 Uhr bis 15 Uhr (Ankunftszeit) und 16:30 Uhr (Abreisezeit)
- Logistik: Bitte bringt Schilder und Flyer mit. Es ist keine Teilnahmegebühr erforderlich. Das Startup Victoria-Mitgliedschaftsticket ist derzeit kostenlos.

Schließt euch uns an, um eure Stimme für die Sicherheit von künstlicher Intelligenz zu erheben und einen Unterschied zu machen. Bitte schließt euch #PauseAIs [Discord-Server](https://discord.gg/2XXWXvErfA) an, dem #australia-Kanal und AGI Moratoriums Slack, [#λ-australia](https://www.campaignforaisafety.org/r/2b0991d9?m=4045bfdd-2b52-4fa2-b4c5-0d8adb4aac63), für weitere Diskussionen.

## Pressemitteilung {#press-release-3}

Am Freitag, dem 16. Juni, werden Freiwillige der neuen [PauseAI](http://pauseai.info)-Bewegung am Melbourne Convention and Exhibition Centre zusammenkommen, um die australische Regierung aufzufordern, die Führung bei der Pause der Entwicklung von leistungsfähigeren und gefährlicheren KI-Systemen zu übernehmen.

Eine wachsende Zahl von KI-Experten hat eine [Erklärung](https://www.safe.ai/statement-on-ai-risk) unterzeichnet, die lautet:

> "Die Minderung des Risikos des Aussterbens durch KI sollte eine globale Priorität neben anderen gesellschaftlichen Risiken wie Pandemien und Atomkrieg sein."

Diese Erklärung wurde von praktisch allen KI-Labors (OpenAI, Google DeepMind, Anthropic) und Hunderten von KI-Wissenschaftlern, darunter Geoffrey Hinton, dem "Gottvater der KI", unterzeichnet.

KI-Sicherheitsforscher haben sich noch nicht auf die Größe des Risikos des menschlichen Aussterbens geeinigt.
Die Ergebnisse der ["Umfrage zum existenziellen Risiko durch KI"](https://forum.effectivealtruism.org/posts/8CM9vZ2nnQsWJNsHx/existential-risk-from-ai-survey-results) zeigen, dass die Schätzungen zwischen 2% und 98% liegen, mit einem Durchschnitt von 30%.

Die Demonstranten fordern die australische Regierung auf, die Führung bei der globalen KI-Sicherheit zu übernehmen und die Entwicklung von gefährlicheren KI-Systemen zu pausieren.
Sie bitten sie auch, die Pause auf dem [KI-Sicherheitsgipfel](https://pauseai.info/summit) zu priorisieren, der von Großbritannien organisiert wird und später im Jahr 2023 stattfinden wird.

Die Pause der KI-Entwicklung ist ein radikal anderer Ansatz zur Sicherheit als der, den die KI-Lab-Chefs wie Sam Altman vorschlagen.
OpenAI glaubt, dass ["es unintuitiv riskant und schwierig wäre, die Schaffung von Superintelligenz zu stoppen"](https://openai.com/blog/governance-of-superintelligence), also verfolgen sie die weitere Entwicklung in Richtung Superintelligenz.

> "Wir haben eine Wahl: Riskieren wir alles, um eine Superintelligenz zu bauen, über die die Öffentlichkeit nie konsultiert wurde, oder stoppen wir, solange wir noch können?" - PauseAI-Demonstranten

> "KI-Unternehmen riskieren alles; wir sehen bereits den Schaden, und es wird noch schlimmer werden. Die Technologieentwicklung ist nicht unvermeidlich, und eine Pause sollte als machbare Option in Betracht gezogen werden. Wir können die Zukunft nicht einigen wenigen CEOs überlassen, die bereit sind, die Menschheit für ihre Träume zu riskieren. Wir alle verdienen ein Mitspracherecht an unserer Zukunft, und eine globale Pause gibt uns diese Chance."

> "Trotz der Anerkennung der Gefahren der fortgesetzten KI-Entwicklung nutzen diese Unternehmen dies nur als Vorwand, um weiterzumachen, und scheinen freiwillig auf diese gefährliche Macht verzichten zu wollen. In solchen Situationen ist die globale Zusammenarbeit bei der Eindämmung dieser gefährlichen Entwicklung von entscheidender Bedeutung, damit wir sicherstellen, dass die Technologieentwicklung für alle funktioniert."

> "Wir haben möglicherweise nicht den Luxus der Zeit. KI-Entwicklungen erfolgen in einem atemberaubenden Tempo, und wir müssen jetzt handeln, um die schlimmsten Szenarien zu verhindern. Der Gipfel im Herbst könnte sogar zu spät sein, um das Schlimmste zu verhindern. Wir brauchen Regierungen, die die KI-Entwicklung jetzt pausieren"

Die PauseAI-Demonstranten haben konkrete [Agenda-Vorschläge](/summit) und [Politikvorschläge](/proposal) für den Gipfel.

Für weitere Informationen besucht bitte [PauseAI.info](http://pauseai.info).

## Kontakt {#contact-5}

- Michael Huang ([Twitter](https://twitter.com/michhuan))
