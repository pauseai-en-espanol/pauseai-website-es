---
title: ¿Por qué pausar la IA avanzada?
description: Por qué pausar la IA avanzada, riesgos para seguridad, empleo y medioambiente; opacidad de modelos, amenaza existencial y propuesta de tratado global.
---

![Folleto para reclamar un tratado internacional sobre IA.](/es/hojatratado.jpg)
_Folleto para reclamar un tratado internacional sobre IA. Fuente: PauseAI_

En 2023, un destacado grupo de científicos, expertos y CEOs [firmaron la siguiente declaración](https://aistatement.com):

> _"Mitigar el riesgo de extinción a causa de la IA debería ser una prioridad global, equiparable a otros riesgos masivos como las pandemias y la guerra nuclear"._
>
> > **Statement On AI Risk, Center For AI Safety**

¿Fue esa una maniobra extraña para promocionar la inteligencia artificial exagerando su peligrosidad? Ni mucho menos. Veamos por qué.

## Esto no es ciencia ficción {#sf}

Al enfrentarnos a este tema, resulta fácil descartar cualquier preocupación legítima con un simple argumento: "es solo ciencia ficción". Cabe señalar que hay predicciones sobre la IA que ya se han cumplido: hoy día vivimos en el mundo de [la película de 2013 _Her_](https://www.youtube.com/watch?v=M7oBRAG8NBg), con humanos enamorados de compañeros virtuales que hablan a través del teléfono móvil o el ordenador.

En otros aspectos, la realidad es peor de lo pronosticado en novelas y películas: no contamos con un equivalente fiable de [las tres reglas de la robótica](https://es.wikipedia.org/wiki/Tres_leyes_de_la_rob%C3%B3tica) de Asimov, y casi nadie vio venir que la IA iba a afectar a la creación artística de manera dramática.

## Dos tipos de IA {#tipos-ia}

Dado que el término "inteligencia artificial" abarca un gran número de tecnologías, cabe establecer una distinción básica entre dos tipos:

- **La "IA-herramienta"** (a veces llamada en inglés _narrow AI_ o "IA estrecha"). Ejemplos: herramientas de reconocimiento facial, diagnóstico médico, etc. Gracias a los algoritmos y la acumulación masiva de datos, estas aplicaciones pueden alcanzar una gran precisión y capacidad, para bien o para mal. Conviene reclamar normas para su correcto uso y transparencia en los algoritmos, como ya hacen algunas organizaciones y partidos políticos.
- **La "IA avanzada"** (en inglés se usan términos como _frontier AI models_, "IA de vanguardia"). Modelos como Chat-GPT o Grok son desarrollados en el marco de la carrera hacia una "inteligencia artificial general" (AGI en sus siglas inglesas). Como dice la periodista Karen Hao, la intención es crear una _"everything machine"_: una máquina capaz de hacer todo. Incluido dejar a los humanos sin trabajo.

## Una caja negra que habla {#caja-negra}

Parte del gravísimo problema de seguridad de la IA avanzada procede de su carácter opaco; en muchos aspectos difiere de cualquier invención humana.

- **Esta IA no se programa**, no hay una persona que escriba una a una líneas de código. En lugar de eso, sus creadores establecen unos algoritmos que permiten a la IA aprender por sí misma durante un largo proceso que requiere de dos cosas en abundancia: ordenadores potentes (de ahí la proliferación de centros de datos) e información sacada de internet, libros y otras fuentes, a menudo sin que las empresas pidieran permiso ni ofrecieran una compensación a los autores.
- El resultado de dicho proceso es **una caja negra informática** con trillones de números que se relacionan entre sí de manera misteriosa. Una caja negra que sabe reconocer patrones, hablar, etc. Nadie sabe cómo funciona realmente por dentro una IA de este tipo, ni siquiera sus creadores.
- Un entrenamiento posterior supervisado por humanos intenta evitar **comportamientos o respuestas indeseados** (por ejemplo, que la IA fomente el suicidio o ayude a fabricar armas biológicas). Esa medida de seguridad puede fallar a lo largo de interacciones prolongadas con los usuarios, o cuando alguien con un mínimo de experiencia intenta saltársela.
- Recordemos las acciones que permite el dominio del lenguaje: sugerir, planear, reforzar creencias erróneas, mentir, manipular, chantajear, etc. De todo esto ya han dado señales los modelos avanzados. Además, son capaces de escribir código. En el futuro próximo estas IAs **podrían mejorarse a sí mismas** o automatizar el desarrollo de nuevas versiones, una línea roja que las empresas están dispuestas a cruzar si eso les concede ventaja sobre sus competidores.

> ![logo](/es/bullet.png) El [Discord de PauseAI](https://discord.gg/2XXWXvErfA) es un buen sitio para hablar de estos temas

## Resumen de los riesgos {#riesgos}

**Ya se notan los impactos** medioambientales, sociales y económicos de la _revolución de la IA_: desde los 'contenidos basura' (_AI slop_) que inundan internet, hasta el voraz consumo de energía y agua de los centros de datos, los problemas mentales sufridos por usuarios y la pérdida de puestos de trabajo.

La mayoría de esfuerzos van dirigidos a incrementar la capacidad de la inteligencia artificial, y **se descuida la seguridad**. Debemos temer que terroristas y criminales aprovechen esta tecnología, y además cabe la posibilidad de que cualquier modelo de IA suficientemente avanzado se salga de control, incluso en su fase de desarrollo.

> ![logo](/es/bullet.png) Lee aquí [un listado completo](/risks) de riesgos de la IA

Pero eso no es todo. Más allá de la inteligencia artificial general, los gurús de Silicon Valley vislumbran **la aparición de una superinteligencia**, un "dios digital" en palabras de Elon Musk, con mayores capacidades intelectuales que la humanidad en su conjunto. Ese momento podría llegar, según previsiones bien fundadas, antes de que acabe la década. ¿Y qué garantías tenemos de que una superinteligencia artificial estaría al servicio de la humanidad? Ninguna en absoluto.

Hablamos, por supuesto, de previsiones. Pero en esta **carrera sin freno a la vista** se están invirtiendo enormes sumas de dinero, con el riesgo adicional de que una burbuja cause una crisis económica mayor que la de las hipotecas _subprime_ de 2007.

![Protesta de PauseAI en Alemania.](/es/protestaalemania.jpg)
_Protesta en Alemania. Fuente: PauseAI_

## Conclusión: pausemos la IA {#conclusion}

Cuesta creer que haya empresas y gobiernos dispuestos a fiar por completo nuestro futuro a una tecnología tan impredecible y peligrosa, por muy productiva que sea.

Por estos y otros muchos motivos, coincidimos con los firmantes de la declaración de 2023 antes citada: la IA avanzada implica un riesgo de extinción, y solicitamos una pausa en el desarrollo de estos modelos hasta que se demuestre que son seguros. Consideramos necesaria [la adopción de un tratado internacional](/proposal) similar a los de no proliferación nuclear. Puede parecer una medida drástica, pero hay demasiado en juego.

> ![logo](/es/bullet.png) [Infórmate aquí](/inscripcion) sobre cómo unirte a PauseAI

---

✍️*Esta página fue redactada al 100% por un humano*
