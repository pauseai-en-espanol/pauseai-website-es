---
title: Risico's van kunstmatige intelligentie
description: KI bedreigt onze democratie, onze technologie en onze soort.
---

KI is een krachtige technologie die onze wereld in toenemende mate verandert.
Het biedt een enorm potentieel, maar ook een groot aantal ernstige risico's.
Dit is een poging om alle risico's te inventariseren die kunnen worden verminderd door een pauze.

## Huidige gevaren {#present-dangers}

### Nepnieuws, polarisatie en bedreiging van de democratie {#fake-news-polarization-and-threatening-democracy}

Een groot deel van onze samenleving is gebaseerd op vertrouwen. We vertrouwen erop dat het geld op onze bankrekening echt is, dat de nieuwsberichten die we lezen waar zijn en dat de mensen die online recensies plaatsen, echt bestaan.

KI-systemen zijn uitzonderlijk goed in het creëren van nepmedia, ook wel deepfakes genoemd.
Ze kunnen nepvideo's, nepaudio, neptekst en nepafbeeldingen creëren.
Het creëren van nepmedia is niet nieuw, maar KI maakt het veel goedkoper en veel realistischer.
Deze mogelijkheden verbeteren zich snel.

Nog maar twee jaar geleden lachten we om de afschuwelijk onrealistische Dall-E-afbeeldingen, maar nu hebben we [deepfake-afbeeldingen die fotowedstrijden winnen](https://www.theguardian.com/technology/2023/apr/17/photographer-admits-prize-winning-image-was-ai-generated).
Een door KI gegenereerde afbeelding van een explosie veroorzaakte [paniekverkopen op Wall Street](https://www.euronews.com/next/2023/05/23/fake-news-about-an-explosion-at-the-pentagon-spreads-on-verified-accounts-on-twitter).
Een audioclip van 10 seconden of een enkele afbeelding kan voldoende zijn om een overtuigende deepfake te creëren.
Misschien nog gevaarlijker dan de deepfakes zelf, is hoe het bestaan van overtuigende deepfakes het vertrouwen vernietigt.
[Een echte afbeelding kan worden genoemd als KI-gegenereerd](https://www.axios.com/2024/08/13/trump-crowd-photo-ai-deepfake-truth), en mensen zullen het geloven.

GPT-4 kan schrijven op een manier die niet te onderscheiden is van mensen, maar met een veel sneller tempo en een fractie van de kosten.
We zullen mogelijk binnenkort zien dat sociale media worden overspoeld met nepdiscussies en meningen, en nepnieuwsartikelen die niet te onderscheiden zijn van echte.
Ook wel een "dode internet" genoemd.

Dit leidt tot polarisatie tussen verschillende groepen mensen die in verschillende informatiebronnen en verhalen geloven en, door het consumeren van vervormde voorstellingen van wat er gebeurt, hun verschillen escaleren tot gewelddadige en antidemocratische reacties.

Een pauze in de ontwikkeling van de krachtigste KI-modellen (onze [voorstellen](/proposal)) zou de modellen die tegenwoordig worden gebruikt om nepmedia te creëren niet stoppen, maar het zou kunnen helpen om toekomstige, nog krachtigere modellen te voorkomen.
Bovendien zou het de basis leggen voor toekomstige regulering gericht op het verminderen van nepmedia en andere specifieke problemen veroorzaakt door KI. Om nog maar te zwijgen van het vergroten van de publieke aandacht en bewustwording van deze gevaren en het bewijs dat ze kunnen worden aangepakt.

### Deepfakes-gebaseerde intimidatie en oplichting {#deepfakes-powered-harrasment-and-scams}

Deepfakes kunnen niet alleen de identiteit van beroemde mensen stelen en [desinformatie creëren](https://time.com/6565446/biden-deepfake-audio/), maar ze kunnen ook jouw identiteit stelen.
Iedereen met foto's, video's of audio van iemand (en voldoende kennis) kan deepfakes van hen creëren en gebruiken om fraude te plegen, hen te intimideren of seksueel niet-consensueel materiaal te creëren.
Ongeveer [96% van alle deepfake-content is seksueel materiaal](https://www.technologyreview.com/2019/10/07/132735/deepfake-porn-deeptrace-legislation-california-election-disinformation/).

Je kunt een overzicht vinden van KI-incidenten, die voornamelijk deepfake-nieuws, oplichting en intimidatie zijn, [hier](https://incidentdatabase.ai/summaries/incidents/).

Zoals het gedeelte over nepnieuws zegt, zou nepmedia niet helemaal worden voorkomen door onze voorstellen, maar ze zouden tot op zekere hoogte kunnen worden verminderd.
Een niet zo kleine mate als je in aanmerking neemt dat KI-meervoudige systemen zoals chatbots heel populair zijn geworden, en we zouden ze ervan weerhouden om nog capabeler en populairder te worden, wat systemen zou kunnen omvatten die zijn ontworpen met minder filters en trainbaar met nieuwe gezichten.

### Vooroordelen en discriminatie {#biases-and-discrimination}

KI-systemen worden getraind op gegevens, en veel van de gegevens die we hebben, zijn op de een of andere manier bevooroordeeld.
Dit betekent dat KI-systemen de vooroordelen van onze samenleving zullen erven.
Een geautomatiseerd recruitment-systeem bij Amazon [erfde een vooroordeel tegen vrouwen](https://www.reuters.com/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G).
Zwarte patiënten werden [minder waarschijnlijk doorverwezen naar een medisch specialist](https://www.science.org/doi/full/10.1126/science.aax2342).
Bevooroordeelde systemen die worden gebruikt in wetshandhaving, zoals voorspellende politiealgoritmes, kunnen leiden tot oneerlijke doelwitten van specifieke groepen.
Generatieve KI-modellen kopiëren niet alleen de vooroordelen uit hun trainingsgegevens, [ze versterken ze](https://www.bloomberg.com/graphics/2023-generative-ai-bias/).
Deze vooroordelen verschijnen vaak zonder dat de makers van het KI-systeem zich hiervan bewust zijn.

### Baanverlies, economische ongelijkheid en instabiliteit {#job-loss-economic-inequality-and-instability}

Tijdens de industriële revolutie verloren veel mensen hun baan aan machines.
Echter, er werden nieuwe (vaak betere) banen gecreëerd, en de economie groeide.
Deze keer kan het anders zijn.

KI vervangt niet alleen onze spieren, zoals de stoommachine deed, maar ook onze hersenen.
Gewone mensen hebben mogelijk niets meer te bieden aan de economie.
Afbeeldingsgeneratiemodellen (die zwaar getraind zijn op auteursrechtelijk beschermd materiaal van professionele kunstenaars) hebben al een [impact op de creatieve industrie](https://cointelegraph.com/news/artists-face-a-choice-with-ai-adapt-or-become-obsolete).
Schrijvers zijn [in staking](https://www.newscientist.com/article/2373382-why-use-of-ai-is-a-major-sticking-point-in-the-ongoing-writers-strike/).
GPT-4 is [geslaagd voor het advocaatsexamen](https://law.stanford.edu/2023/04/19/gpt-4-passes-the-bar-exam-what-that-means-for-artificial-intelligence-tools-in-the-legal-industry/), kan uitstekende geschreven content creëren en kan code schrijven (opnieuw, gedeeltelijk getraind op [auteursrechtelijk beschermd materiaal](https://www.ischool.berkeley.edu/news/2023/new-research-prof-david-bamman-reveals-chatgpt-seems-be-trained-copyrighted-books)).

De mensen die deze KI-systemen bezitten, zullen in staat zijn om erop te kapitaliseren, maar de mensen die hun baan aan hen verliezen, zullen dat niet zijn.
Het is moeilijk te voorspellen welke banen als eerste zullen worden vervangen.
Ze kunnen je werkloos en zonder inkomen achterlaten, ongeacht hoeveel tijd, geld en energie je hebt gestoken in het verkrijgen van de ervaring en kennis die je hebt, en hoe waardevol ze een moment geleden waren.
De manier waarop we rijkdom in onze samenleving verdelen, is niet voorbereid op dit scenario.

Beleidsmaatregelen zoals een Universeel Basisinkomen kunnen de ergste economische gevolgen voorkomen, maar het is niet duidelijk of ze op tijd zullen worden geïmplementeerd.
Zodra onze banen zijn vervangen, kunnen we zonder onderhandelingsmacht worden achtergelaten om sociale vangnetten te eisen.

En zelfs als we erin slagen om de problemen rond ongelijkheid en instabiliteit goed te navigeren, kunnen we uiteindelijk in een wereld terechtkomen waarin ons gevoel van doel verloren is gegaan.
Veel kunstenaars voelen dit al, omdat ze zien dat hun werk wordt vervangen door KI.
Binnenkort kunnen we allemaal zo voelen.

### Geestelijke gezondheid, verslaving en verbreking van de verbinding tussen mensen {#mental-health-addiction-and-disconnection-between-people}

Socialemediabedrijven hebben al enige tijd KI-systemen gebruikt om hun winst te maximaliseren, terwijl ze gebruik maken van onze primaire geesten, waardoor onze geestelijke gezondheid wordt beschadigd.
KI-chatbots die gebruikers een romantische relatie bieden, hebben de afgelopen jaar een enorme groei doorgemaakt, met meer dan 3 miljard zoekresultaten voor 'KI-vriendin' op Google.
Deze KI-relatie-apps zijn [verslavend](https://onlinelibrary.wiley.com/doi/10.1002/mar.21899), vooral voor "eenzame, kwetsbare mensen".

De bedrijven die deze apps controleren, worden gestimuleerd om ze zo verslavend mogelijk te maken en hebben een enorme hoeveelheid macht door het vormen van het gedrag en de meningen van deze modellen.

Een pauze in de grootste modellen zou kunnen voorkomen dat ze meervoudige chatbots worden die perfect aansluiten bij onze behoeften zonder dat mensen de langetermijngevolgen begrijpen.

### Geautomatiseerd onderzoek (verlies van privacy) {#automated-investigation-loss-of-privacy}

We laten veel sporen achter op het web.
Het verbinden van de punten is moeilijk en tijdrovend, maar KI kan dit nu veel goedkoper maken.
Grote taalmodellen kunnen nu autonoom het web doorzoeken en zijn nu goed genoeg om grote hoeveelheden gegevens te analyseren en interessante details te vinden.
Dit kan worden gebruikt om informatie te vinden die anders heel duur zou zijn om te vinden.

- Vind informatie over waar een individu waarschijnlijk op een bepaald moment zal zijn. Dit kan worden gebruikt om dissidenten op te sporen of aanslagen te plannen.
- Koppel anonieme accounts op het web aan echte identiteiten. Dit kan worden gebruikt om uit te zoeken wie informatie lekt.

In september 2024 bouwde een groep studenten [een app](https://x.com/AnhPhuNguyen1/status/1840786336992682409) die informatie over vreemden zoals namen, familieleden en andere persoonlijke gegevens in augmented reality toont met behulp van gezichtsherkenning en LLM's.

### Milieurisico's {#environmental-risks}

Milieuschade begint significant te worden, en de grootste KI-bedrijven zijn van plan om hun energieverbruik aanzienlijk te verhogen. Je kunt lezen over hoe KI het milieu negatief zal beïnvloeden [hier](/environmental).

### Autonome wapens {#autonomous-weapons}

Bedrijven verkopen al KI-aangedreven wapens aan regeringen.
Lanius bouwt [vliegende zelfmoorddrones](https://www.youtube.com/watch?v=G7yIzY1BxuI) die autonoom vijanden identificeren.
Palantir's [AIP-systeem](https://www.youtube.com/watch?v=XEM5qz__HOU) gebruikt grote taalmodellen om gegevens van het slagveld te analyseren en optimale strategieën te bedenken.

Natieën en wapenbedrijven hebben beseft dat KI een enorme impact zal hebben op het verslaan van hun vijanden.
We zijn een nieuwe wapenwedloop ingegaan.
Deze dynamiek beloont het versnellen en afsnijden van hoeken.

Op dit moment hebben we nog mensen in de loop voor deze wapens.
Maar naarmate de mogelijkheden van deze KI-systemen verbeteren, zal er meer en meer druk zijn om de machines de macht te geven om te beslissen.
Wanneer we de controle over wapens aan KI delegeren, kunnen fouten en bugs verschrikkelijke gevolgen hebben.
De snelheid waarmee KI informatie kan verwerken en beslissingen kan nemen, kan conflicten in enkele minuten doen escaleren.
Een [recent artikel](https://arxiv.org/pdf/2401.03408.pdf) concludeert dat "modellen de neiging hebben om wapenwedloopdynamiek te ontwikkelen, wat leidt tot grotere conflicten, en in zeldzame gevallen zelfs tot de inzet van kernwapens".

Lees meer op [stopkillerrobots.org](https://www.stopkillerrobots.org/military-and-killer-robots/)

## Toekomstige gevaren {#near-future-dangers}

### Machtaccumulatie en tirannie {#power-accumulation-and-tyranny}

Krachtige KI-modellen kunnen worden gebruikt om meer macht te krijgen.
Deze positieve feedbackloop kan leiden tot een paar bedrijven of regeringen die een ongezonde hoeveelheid macht hebben.
De controle hebben over duizenden intelligente, autonome systemen kan worden gebruikt om meningen te beïnvloeden, markten te manipuleren of zelfs oorlog te voeren.
In handen van een autoritaire regering kan dit worden gebruikt om dissidenten te onderdrukken en de macht te behouden.

### Biologische wapens {#biological-weapons}

KI kan kennis toegankelijker maken, wat ook kennis over het creëren van biologische wapens omvat. [Dit artikel](https://arxiv.org/abs/2306.03809) laat zien hoe GPT-4 niet-wetenschappelijke studenten kan helpen om een pandemisch pathogeen te creëren:

> In een uur stelden de chatbots vier potentiële pandemische pathogenen voor, legden uit hoe ze kunnen worden gegenereerd uit synthetisch DNA met behulp van reverse genetica, leverden de namen van DNA-synthesemaatschappijen die onwaarschijnlijk orders screenen, identificeerden gedetailleerde protocollen en hoe ze kunnen worden opgelost, en adviseerden iedereen die de vaardigheden mist om reverse genetica uit te voeren, een kernfaciliteit of contractonderzoeksorganisatie in te schakelen.

Dit type kennis is nog nooit zo toegankelijk geweest, en we hebben geen waarborgen om de mogelijke gevolgen aan te pakken.

Bovendien kunnen sommige KI-modellen worden gebruikt om volledig nieuwe gevaarlijke pathogenen te ontwerpen.
Een model genaamd MegaSyn ontwierp [40.000 nieuwe chemische wapens / giftige moleculen in een uur](https://www.theverge.com/2022/3/17/22983197/ai-new-possible-chemical-weapons-generative-models-vx).
Het revolutionaire AlphaFold-model kan de structuur van eiwitten voorspellen, wat ook een [dual-use technologie](https://unicri.it/sites/default/files/2021-12/21_dual_use.pdf) is.
Het voorspellen van eiwitstructuren kan worden gebruikt om "ziekteverwekkende mutaties te ontdekken met behulp van het genoom van één individu".
Wetenschappers creëren nu zelfs [volledig autonome chemische laboratoria, waar KI-systemen nieuwe chemicaliën op eigen kracht kunnen synthetiseren](https://twitter.com/andrewwhite01/status/1670794000398184451).

Het fundamentele gevaar is dat de kosten van het ontwerpen en toepassen van biologische wapens door KI met een factor worden verlaagd.

### Computervirussen en cybersecurity-aanvallen {#computer-viruses-and-cybersecurity-attacks}

Vrijwel alles wat we tegenwoordig doen, is op de een of andere manier afhankelijk van computers.
We betalen onze boodschappen, plannen onze dagen, contacteren onze dierbaren en zelfs rijden onze auto's met computers.

Moderne KI-systemen kunnen software analyseren en schrijven.
Ze [kunnen kwetsbaarheden vinden](https://betterprogramming.pub/i-used-gpt-3-to-find-213-security-vulnerabilities-in-a-single-codebase-cc3870ba9411) in software, en [ze kunnen worden gebruikt om ze uit te buiten](https://blog.checkpoint.com/2023/03/15/check-point-research-conducts-initial-security-analysis-of-chatgpt4-highlighting-potential-scenarios-for-accelerated-cybercrime/).
Naarmate de mogelijkheden van KI toenemen, zullen ook de mogelijkheden van de exploits die ze kunnen creëren toenemen.

Hoogpotente computervirussen zijn altijd extreem moeilijk te creëren geweest, maar KI kan dat veranderen.
In plaats van een team van geschoolde beveiligingsexperts/hackers in te huren om zero-day exploits te vinden, kun je gewoon een veel goedkopere KI gebruiken om het voor je te doen. Natuurlijk kan KI ook helpen bij cyberdefensie, en het is onduidelijk aan welke kant het voordeel ligt.

[Lees meer over KI en cybersecurity-risico's](/cybersecurity-risks)

### Bestaansrisico {#existential-risk}

Veel KI-onderzoekers waarschuwen dat KI tot het einde van de mensheid kan leiden.

Heel intelligente dingen zijn heel krachtig.
Als we een machine bouwen die veel intelligenter is dan mensen, moeten we er zeker van zijn dat het hetzelfde wil als wij.
Echter, dit blijkt heel moeilijk te zijn.
Dit wordt het _afstemmingsprobleem_ genoemd.
Als we er niet in slagen om het op tijd op te lossen, kunnen we uiteindelijk met superintelligente machines eindigen die zich niets aantrekken van ons welzijn.
We zouden een nieuwe soort op de planeet introduceren die ons kan overtroeven en ons kan overtreffen.

[Lees meer over x-risico](/xrisk)

### Menselijke machteloosheid {#human-disempowerment}

Zelfs als we erin slagen om alleen KI-systemen te creëren die we individueel kunnen controleren, kunnen we onze macht om belangrijke beslissingen te nemen geleidelijk verliezen elke keer dat er één wordt opgenomen in instellingen of het dagelijks leven.
Die processen zouden uiteindelijk meer input van KI-systemen hebben dan van mensen, en als we niet snel genoeg kunnen coördineren of we ontbreken cruciale kennis over de werking van de systemen, kunnen we uiteindelijk zonder controle over onze toekomst komen te zitten.

Het zou een beschaving zijn waarin elk systeem wordt geoptimaliseerd voor verschillende doelstellingen, er is geen duidelijke richting voor waar alles naartoe gaat, en er is geen manier om het te veranderen.
De technische kennis die nodig is om deze systemen te wijzigen, kan in de eerste plaats ontbreken of in de loop van de tijd verloren gaan, naarmate we steeds afhankelijker worden van technologie en de technologie steeds complexer wordt.

De systemen kunnen hun doelen bereiken, maar die doelen kunnen onze waarden niet volledig omvatten. Dit probleem gebeurt nu al tot op zekere hoogte, maar KI kan het aanzienlijk versterken.

### Digitale bewustzijn {#digital-sentience}

Naarmate KI blijft vooruitgaan, kunnen toekomstige systemen ongelooflijk geavanceerd worden, neurale structuren en functies repliceren die meer lijken op de menselijke hersenen.
Deze toegenomen complexiteit kan leiden tot emergente eigenschappen zoals KI-subjectiviteit en/of bewustzijn, wat hen morele overwegingen waard zou maken.

Het probleem is dat, gezien onze huidige gebrek aan kennis over bewustzijn en de aard van neurale netwerken, we geen manier zullen hebben om te bepalen of sommige KI's enige vorm van ervaring hebben en wat de kwaliteit van die ervaringen zou zijn.
Als de KI's blijven worden geproduceerd met alleen hun capaciteiten in gedachten, door een proces dat we niet volledig begrijpen, zullen mensen de KI's blijven gebruiken als gereedschap, zonder rekening te houden met wat hun verlangens zouden kunnen zijn, en dat ze eigenlijk "digitale mensen" zouden kunnen knechten.

### Lijden-risico's {#suffering-risks}

Het is niet alleen dat waardeverankering ons kan doen falen om de beste soort werelden te bereiken, maar het kan ons ook doen eindigen in dystopieën die erger zijn dan uitsterven en die zich door de hele ruimte-tijd kunnen uitstrekken.

Mogelijke verankerde dystopieën met veel lijden worden _S-risico's_ genoemd en omvatten werelden waarin bewuste wezens worden geknecht en gedwongen worden om vreselijke dingen te doen.
Die wezens kunnen mensen, dieren, digitale mensen of andere buitenaardse soorten zijn die de KI in het heelal kan vinden. Gezien hoe moeilijk we denken dat het volledig oplossen van afstemming is, hoe slecht mensen elkaar soms behandelen, hoe slecht we de meeste dieren behandelen, en hoe we huidige KI's behandelen, lijkt een toekomst als deze niet zo onwaarschijnlijk als we zouden hopen.

## Wat kunnen we doen? {#what-can-we-do}

Voor **alle** bovengenoemde problemen neemt het risico toe naarmate de mogelijkheden van KI toenemen.
Dit betekent dat het veiligste wat we nu kunnen doen, is **vertragen**.
We moeten de ontwikkeling van krachtigere KI-systemen pauzeren totdat we hebben uitgevonden hoe we met de risico's om moeten gaan.

Zie [onze voorstellen](/proposal) voor meer details.
