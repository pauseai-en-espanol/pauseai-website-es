---
title: Veelgestelde vragen
description: Veelgestelde vragen over PauseAI en de risico's van superintelligente AI.
---

<script>
    import SimpleToc from '$lib/components/simple-toc/SimpleToc.svelte'
</script>
<style>
    h2 {
        font-size: 1.2rem;
    }
</style>

<SimpleToc />

## Wie zijn wij? {#who-are-you}

Wij zijn een gemeenschap van vrijwilligers en [lokale gemeenschappen](/communities) die worden gecoördineerd door een [non-profitorganisatie](/organization) die zich inzet om de [risico's van AI](/risks) (inclusief het [risico van menselijke uitsterving](/xrisk)) te mitigeren.
Ons doel is om onze overheden te overtuigen om in te grijpen en [de ontwikkeling van supermenselijke AI te pauzeren](/proposal).
We doen dit door het publiek te informeren, met besluitvormers te praten en [evenementen](/events) te organiseren.

## Hebben jullie sociale media? {#do-you-have-social-media}

Je kunt ons vinden op [Discord](https://discord.gg/NuqkHutXW3) (dit is waar de meeste coördinatie plaatsvindt!), [Twitter](https://twitter.com/PauseAI), [Substack](https://substack.com/@pauseai), [Facebook](https://www.facebook.com/PauseAI), [TikTok](https://www.tiktok.com/@pauseai), [LinkedIn](https://www.linkedin.com/uas/login?session_redirect=/company/97035448/), [YouTube](https://www.youtube.com/@PauseAI), [Instagram](https://www.instagram.com/pause_ai), [Telegram](https://t.me/+UeTsIsNkmt82ZmQ8), [Whatsapp](https://chat.whatsapp.com/JgcAbjqRr8X3tvrXdeQvfj) en [Reddit](https://www.reddit.com/r/PauseAI/).
Je kunt ons mailen of contact opnemen via [joep@pauseai.info](mailto:joep@pauseai.info).

## Zijn jullie niet gewoon bang voor veranderingen en nieuwe technologie? {#arent-you-just-scared-of-changes-and-new-technology}

Je zou verbaasd zijn dat de meeste mensen in PauseAI zichzelf als techno-optimisten beschouwen.
Velen van hen zijn betrokken bij AI-ontwikkeling, zijn gadget-liefhebbers en zijn meestal erg enthousiast over de toekomst.
Vooral velen van hen zijn enthousiast over het potentieel van AI om de mensheid te helpen.
Dat is waarom voor velen van hen de trieste realisatie dat AI een existentieel risico vormt, een zeer moeilijke waarheid was om te accepteren.

## Willen jullie alle AI verbieden? {#do-you-want-to-ban-all-ai}

Nee, alleen de ontwikkeling van de grootste algemene doel-AI-systemen, vaak "Frontier-modellen" genoemd.
Bijna alle bestaande AI zou legaal zijn onder [ons voorstel](/proposal), en de meeste toekomstige AI-modellen zullen ook legaal blijven.
We roepen op tot een verbod op krachtigere algemene AI-systemen, totdat we weten hoe we veilige AI kunnen bouwen en deze onder democratische controle hebben.

## Geloven jullie dat GPT-4 ons gaat doden? {#do-you-believe-gpt-4-is-going-to-kill-us}

Nee, we denken niet dat [huidige AI-modellen](/sota) een existentieel risico vormen.
Het lijkt waarschijnlijk dat de meeste volgende AI-modellen dat ook niet zullen zijn.
Maar als we blijven bouwen aan steeds krachtigere AI-systemen, zullen we uiteindelijk een punt bereiken waarop een ervan een existentieel risico vormt.

## Kan een pauze contraproductief zijn en de zaken erger maken? {#can-a-pause-backfire-and-make-things-worse}

We hebben deze zorgen behandeld in [dit artikel](/mitigating-pause-failures).

## Is een pauze zelfs mogelijk? {#is-a-pause-even-possible}

AGI is niet onvermijdelijk.
Het vereist hordes ingenieurs met miljoenen-dollar salarissen.
Het vereist een volledig functionele en onbeperkte toeleveringsketen van de meest complexe hardware.
Het vereist dat we allemaal toestaan dat deze bedrijven met onze toekomst gokken.

[Lees meer over de haalbaarheid van een pauze](/feasibility).

## Wie betaalt jullie? {#who-is-paying-you}

Zie onze [financieringspagina](/funding)

## Wat zijn jullie plannen? {#what-are-your-plans}

Onze focus ligt op [het laten groeien van de beweging](/growth-strategy), het organiseren van protesten, lobbyen bij politici en het informeren van het publiek.

Bekijk onze [roadmap](/roadmap) voor een gedetailleerd overzicht van onze plannen en wat we kunnen doen met meer financiering.

## Hoe denken jullie dat jullie overheden kunnen overtuigen om AI te pauzeren? {#how-do-you-think-you-can-convince-governments-to-pause-ai}

Bekijk onze [theorie van verandering](/theory-of-change) voor een gedetailleerd overzicht van onze strategie.

## Waarom protesteren jullie? {#why-do-you-protest}

- Protesteren laat de wereld zien dat we om deze kwestie geven. Door te protesteren, laten we zien dat we bereid zijn om onze tijd en energie te besteden om mensen te laten luisteren.
- Protesten kunnen en zullen vaak de publieke opinie, stemgedrag, bedrijfs gedrag en beleid positief beïnvloeden.
- Verreweg de meeste mensen zijn voorstander van vreedzame/niet-gewelddadige protesten.
- Er is geen bewijs voor een "backfire"-effect, tenzij het protest gewelddadig is. Onze protesten zijn vreedzaam en niet-gewelddadig.
- Het is een sociale bindingsexperience. Je ontmoet andere mensen die jouw zorgen en bereidheid om actie te ondernemen delen.
- Bekijk [dit geweldige artikel](https://forum.effectivealtruism.org/posts/4ez3nvEmozwPwARr9/a-case-for-the-effectiveness-of-protest) voor meer inzichten over waarom protesteren werkt.

Als je een [protest wilt organiseren](/organizing-a-protest), kunnen we je helpen met advies en middelen.

## Hoe groot is de kans dat superintelligente AI zeer slechte uitkomsten zal veroorzaken, zoals menselijke uitsterving? {#how-likely-is-it-that-superintelligent-ai-will-cause-very-bad-outcomes-like-human-extinction}

We hebben [een lijst van 'p(doom)'-waarden](/pdoom) (kans op slechte uitkomsten) samengesteld van verschillende bekende experts op het gebied.

AI-veiligheidsonderzoekers (die de experts zijn op dit gebied) zijn verdeeld over deze vraag, en schattingen lopen uiteen van 2% tot 97% met een gemiddelde van 30%.
Houd er rekening mee dat geen enkele (gepeilde) AI-veiligheidsonderzoeker gelooft dat er een kans van 0% is.
Er kan echter selectiebias zijn: mensen die in het veld van AI-veiligheid werken, doen dit waarschijnlijk omdat ze geloven dat het voorkomen van slechte AI-uitkomsten belangrijk is.

Als je AI-onderzoekers in het algemeen vraagt (geen veiligheidsspecialisten), daalt dit aantal tot een gemiddelde waarde van ongeveer 14%, met een mediaan van 5%.
De overgrote meerderheid, 86% van hen, gelooft dat het afstemmingsprobleem zowel een reëel als een belangrijk probleem is.
Houd er rekening mee dat er mogelijk een selectiebias is in de tegenovergestelde richting: mensen die in AI werken, doen dit waarschijnlijk omdat ze geloven dat AI gunstig zal zijn.

_Stel je voor dat je wordt uitgenodigd om een testvlucht te maken op een nieuw vliegtuig_.
De vliegtuigingenieurs denken dat er een kans van 14% is op een crash.
Zou je in dat vliegtuig stappen? Omdat we nu allemaal aan boord gaan van het AI-vliegtuig.

## Hoe lang hebben we nog tot superintelligente AI? {#how-long-do-we-have-until-superintelligent-ai}

Het kan maanden duren, het kan decennia duren, niemand weet het zeker.
We weten echter wel dat de voortgang van AI vaak grof wordt onderschat.
Nog maar drie jaar geleden dachten we dat we in 2055 AI-systemen zouden hebben die de SAT-test konden doorstaan.
We bereikten dat in april 2023.
We moeten handelen alsof we nog maar heel weinig tijd hebben, omdat we niet willen worden verrast.

[Lees meer over urgentie](/urgency).

## Als we pauzeren, wat dan met China? {#if-we-pause-what-about-china}

Om te beginnen heeft China strengere AI-reguleringen dan vrijwel elk ander land.
Ze lieten zelfs geen chatbots toe en verboden training op internetgegevens tot september 2023.
China heeft een meer controlerende regering en heeft dus nog meer reden om de oncontroleerbare en onvoorspelbare gevolgen van AI te vrezen.
Tijdens de VN-veiligheidsraadvergadering over AI-veiligheid was China het enige land dat de mogelijkheid noemde om een pauze in te voeren.

Houd er ook rekening mee dat we voornamelijk vragen om een _internationale_ pauze, afgedwongen door een verdrag.
Zo'n verdrag moet ook door China worden ondertekend.
Als het verdrag garandeert dat andere landen ook zullen stoppen, en er voldoende handhavingsmechanismen zijn, zou dit iets moeten zijn dat China ook wil zien.

## OpenAI en Google zeggen dat ze willen worden gereguleerd. Waarom protesteren jullie tegen hen? {#openai-and-google-are-saying-they-want-to-be-regulated-why-are-you-protesting-them}

We applaudisseren OpenAI en Google voor hun oproep tot internationale regulering van AI.
We geloven echter dat de huidige voorstellen niet voldoende zijn om een AI-catastrofe te voorkomen.
Google en Microsoft hebben nog niet publiekelijk iets gezegd over het existentiële risico van AI.
Alleen OpenAI noemt expliciet het risico van uitsterving, en opnieuw applaudisseren we hen voor het serieus nemen van dit risico.
Hun strategie is echter vrij expliciet: een pauze is onmogelijk, we moeten eerst superintelligentie bereiken.
Het probleem hiermee is echter dat ze niet geloven dat ze het afstemmingsprobleem hebben opgelost.
De AI-bedrijven zijn verwikkeld in een race naar de bodem, waarbij AI-veiligheid wordt opgeofferd voor concurrentievoordeel.
Dit is gewoon het resultaat van marktdynamiek.
We hebben overheden nodig om in te grijpen en beleid te implementeren (op internationaal niveau) dat de slechtste uitkomsten voorkomt.

## Drijven AI-bedrijven het existentiële risiconarratief om ons te manipuleren? {#are-ai-companies-pushing-the-existential-risk-narrative-to-manipulate-us}

We kunnen niet met zekerheid weten wat de motieven van deze bedrijven zijn, maar we weten wel dat x-risico niet aanvankelijk door AI-bedrijven werd gepusht - het waren wetenschappers, activisten en NGO's.
Laten we naar de tijdlijn kijken.

Er zijn veel mensen geweest die sinds de vroege jaren 2000 hebben gewaarschuwd voor x-risico.
Eliezer Yudkowsky, Nick Bostrom, Stuart Russell, Max Tegmark en vele anderen.
Ze hadden geen AI-technologie om te pushen - ze waren gewoon bezorgd over de toekomst van de mensheid.

De AI-bedrijven noemden x-risico nooit tot voor kort.

Sam Altman is een interessante uitzondering.
Hij schreef over existentieel AI-risico in 2015, op zijn privéblog, voordat hij OpenAI oprichtte.
In de jaren daarna maakte hij vrijwel geen expliciete melding meer van x-risico.
Tijdens de Senaatsvergadering op 16 mei 2023, toen hem werd gevraagd naar zijn x-risicoblogpost, antwoordde hij alleen door te praten over banen en de economie.
Hij pushte het x-risiconarratief hier niet, hij vermeed het actief.

In mei 2023 veranderde alles:

- Op 1 mei stapt 'Godfather of AI' Geoffrey Hinton uit zijn baan bij Google om te waarschuwen voor x-risico.
- Op 5 mei wordt de eerste PauseAI-protest aangekondigd, recht voor de deur van OpenAI.
- Op 22 mei publiceerde OpenAI een blogpost over de governance van superintelligentie, en noemde x-risico voor het eerst.
- Op 24 mei erkent ex-Google CEO Eric Schmidt x-risico.
- Op 30 mei werd de Safe.ai-verklaring (erkenning van x-risico) gepubliceerd. Deze keer met mensen van OpenAI, Google en Microsoft.

Deze bedrijven zijn erg traag geweest in het erkennen van x-risico, gezien het feit dat veel van hun werknemers er al jaren van op de hoogte zijn.
Dus in onze ogen pushen de AI-bedrijven het x-risiconarratief niet, ze reageren op anderen die het pushen, en hebben gewacht met hun reactie totdat het absoluut noodzakelijk was.

De zakelijke prikkels wijzen in de andere richting: bedrijven zouden liever niet hebben dat mensen zich zorgen maken over de risico's van hun producten.
Vrijwel alle bedrijven bagatelliseren risico's om klanten en investeringen aan te trekken, in plaats van ze te overdrijven.
Hoeveel strikte regulering en negatieve aandacht nodigen de bedrijven uit door deze gevaren toe te geven?
En zou een bedrijf als OpenAI 20% van zijn rekenkracht aan AI-veiligheid wijden als het niet in de risico's geloofde?

Hier is onze interpretatie: de AI-bedrijven ondertekenden de verklaring omdat ze weten dat x-risico een probleem is dat zeer serieus moet worden genomen.

Een grote reden waarom veel andere mensen nog steeds niet willen geloven dat x-risico een reële zorg is?
Omdat het erkennen dat we in feite in gevaar zijn een heel, heel angstaanjagende zaak is.

[Lees meer over de psychologie van x-risico](/psychology-of-x-risk).

## Oké, ik wil helpen! Wat kan ik doen? {#ok-i-want-to-help-what-can-i-do}

Er zijn veel dingen die je kunt doen.
Op je eigen, kun je een brief schrijven, flyers posten, leren en anderen informeren, deelnemen aan een protest, of doneren van geld!
Maar nog belangrijker: je kunt bij PauseAI aansluiten en coördineren met anderen die actie ondernemen.
Kijk of er lokale gemeenschappen in jouw omgeving zijn.
Als je meer wilt bijdragen, kun je vrijwilliger worden en lid worden van een van onze teams, of een lokale gemeenschap oprichten!

Zelfs als we worden geconfronteerd met het einde van de wereld, kan er nog steeds hoop en zeer lonend werk zijn.
