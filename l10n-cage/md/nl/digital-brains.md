---
title: AI-modellen zijn onvoorspelbare digitale hersenen
description: Niemand begrijpt hoe AI-modellen werken, niemand kan hun gedrag voorspellen en niemand zal ze kunnen controleren.
---

**We begrijpen de interne werking van grootschalige AI-modellen niet, we kunnen niet voorspellen wat ze kunnen doen als ze groter worden, en we kunnen hun gedrag niet controleren.**

## Moderne AI-modellen worden getraind, niet geprogrammeerd {#modern-ai-models-are-grown-not-programmed}

Tot voor kort werden de meeste AI-systemen ontworpen door mensen die software schreven.
Ze bestonden uit een set regels en instructies die door programmeurs waren geschreven.

Dit veranderde toen machine learning populair werd.
Programmeurs schrijven de leer-algoritme, maar de hersenen zelf worden _getraind_ of _gekweekt_.
In plaats van een leesbare set regels is het resulterende model een ondoorzichtige, complexe, onvoorstelbaar grote set getallen.
Begrijpen wat er binnen deze modellen gebeurt, is een grote wetenschappelijke uitdaging.
Dat veld heet _interpretabiliteit_ en het staat nog in de kinderschoenen.

## Digitale vs. menselijke hersenen: Hoe dichtbij zijn we echt? {#digital-vs-human-brains-how-close-are-we-really}

We zijn allemaal erg bekend met de capaciteiten van menselijke hersenen, omdat we ze overal om ons heen zien.
Maar de (vaak verrassende en emergente) capaciteiten van deze nieuwe "digitale hersenen" (Deep Learning-systemen, LLM's, etc.) zijn moeilijk te voorspellen en te kennen.

Dat gezegd hebbende, hier zijn enkele cijfers, overeenkomsten en andere analogieën om u te helpen vergelijken.

**Begin 2024...**

### Grootte {#size}

Menselijke hersenen worden geschat op ongeveer [100 biljoen synaptische verbindingen](https://medicine.yale.edu/lab/colon_ramos/overview).

Huidige "frontier"-AI aangedreven LLM's (bijv. GPT4, Claude3, Gemini, etc.) hebben [honderden miljarden "parameters"](https://en.wikipedia.org/wiki/Large_language_model#List). Deze "parameters" worden beschouwd als enigszins analoog aan "synapsen" in de menselijke hersenen. Dus worden GPT4-grote modellen verwacht ongeveer 1% van de grootte van een menselijk brein te zijn.

Gezien de snelheid van nieuwe AI-trainings-GPU-kaarten (bijv. Nvidia H100's, DGX BG200, etc.), is het redelijk om aan te nemen dat GPT5 of GPT6 10 keer zo groot kan zijn als GPT4. Het wordt ook gedacht dat een groot deel van de kennis/informatie in de menselijke hersenen niet wordt gebruikt voor taal en hoger redeneren, dus deze systemen kunnen (en doen momenteel) vaak presteren op, of zelfs hoger dan, menselijk niveau voor veel belangrijke functies, zelfs bij hun huidige kleinere grootte.

In plaats van getraind te worden met visuele, audio- en andere sensorische invoer, zoals menselijke hersenen, worden de huidige LLM's exclusief getraind met bijna alle kwaliteitsboeken en tekst die beschikbaar zijn op internet. Deze hoeveelheid tekst zou [170.000 jaar duren voor een mens om te lezen](https://twitter.com/ylecun/status/1750614681209983231?lang=en).

En toekomstige multimodale LLM-systemen zullen worden getraind met behulp van afbeeldingen, video, audio, 3D-werelden, geometrie, simulaties, robotica-trainingsgegevens, etc... bovenop alle kwaliteitsboeken en tekst op internet. Dit zal hen een veel betere mogelijkheid geven om beelden, video's, geluiden, stemmen, muziek, 3D-werelden en ruimtes te creëren. En deze 3D-wereldsimulaties zullen hen ook in staat stellen om robots en andere machines in de fysieke wereld direct en autonoom te controleren.

### Snelheid {#speed}

Het wordt geschat dat een menselijk brein tussen [1-20 exaflops](https://www.nist.gov/blogs/taking-measure/brain-inspired-computing-can-help-us-create-faster-more-energy-efficient) kan presteren (wat 10^18 of 1.000.000.000.000.000.000 floating point-bewerkingen per seconde is).

Huidige "frontier"-AI aangedreven LLM's worden over het algemeen "uitgevoerd" op honderden of duizenden huidige generatie GPU's (bijv. Nvidia A100's, H100's, etc.). En Nvidia heeft onlangs hun nieuwste "next generation" GPU "server rekken" aangekondigd, de [DGX BG200 NVL72](https://www.nvidia.com/en-us/data-center/gb200-nvl72/).
Eén enkele instantie/ rek van dit systeem kan naar verluidt 1,44 exaflops van AI-"inference" uitvoeren.
Dus kan één enkele [DGX BG200 NVL72](https://www.nvidia.com/en-us/data-center/gb200-nvl72/) mogelijk een soortgelijk aantal bewerkingen per seconde uitvoeren als een enkel menselijk brein.

Op deze grootte kunnen deze systemen letterlijk een "AGI in een doos" worden. En Nvidia zal waarschijnlijk honderden of duizenden van deze eenheden verkopen in 2024. Dan kunnen de systemen van volgend jaar 2-10 keer zo snel zijn als deze.

Naast meer traditionele [GPU](https://en.wikipedia.org/wiki/Graphics_processing_unit)- en [TPU](https://en.wikipedia.org/wiki/Tensor_Processing_Unit)-architecturen zijn er ook doorbraken geweest met andere soorten aangepaste hardware die de snelheid van LLM-"inference" aanzienlijk kunnen verhogen, wat het proces is dat een op AI gebaseerde LLM gebruikt voor taalverwerking, redeneren en coderen. Bijv. [De Groq LPU Inference Engine](https://wow.groq.com/lpu-inference-engine).

### Exponentiële groei {#exponential-growth}

We hebben "[Moore's Law](https://en.wikipedia.org/wiki/Moore%27s_law)" gebruikt om de grootte en snelheid van nieuwe computersystemen zeer nauwkeurig te voorspellen voor bijna 50 jaar. Er zijn enkele argumenten dat de snelheid en grootte van computerchips in de toekomst kunnen vertragen, maar er zijn altijd innovaties geweest om de exponentiële groei voort te zetten. Met de volgende ronde chips die al worden gepland en/of geproduceerd, en de horizontale schaalbaarheid van deze AI-systemen, wordt verwacht dat LLM's binnen enkele maanden of jaren op of nabij het niveau van een menselijk brein zullen presteren!

Dan, met voortdurende exponentiële (of multi-exponentiële) groei, kunnen deze systemen de grootte, snelheid en capaciteiten van menselijke hersenen in de komende jaren aanzienlijk overtreffen.

En ze worden ook verwacht om snel de grootte, snelheid en capaciteiten van "alle menselijke hersenen samen" te overtreffen.

> "Ik zei dat eigenlijk in 1999. Ik zei dat [AI] tegen 2029 elk persoon zou evenaren." -- Ray Kurzweil [Futurist Ray Kurzweil zegt dat AI tegen 2029 menselijk niveau intelligentie zal bereiken
> ](https://youtu.be/Tr-VgjtUZLM?t=19)

> "Als de snelheid van verandering aanhoudt, denk ik dat 2029, of misschien 2030, de tijd is waarin digitale intelligentie waarschijnlijk alle menselijke intelligentie zal overtreffen." -- Elon Musk [AGI tegen 2029? Elon Musk over de toekomst van AI](https://youtu.be/DSKxmvq9t04?t=106)

## Oncontroleerbare schaalbaarheid {#uncontrollable-scaling}

Zodra deze systemen dezelfde grootte en snelheid als een menselijk brein bereiken (of veel groter worden), wordt verwacht dat ze alle taken kunnen uitvoeren die een expert mens kan doen.
Dit omvat AI-onderzoek, testen en verbeteren.
Dus na AGI moeten we verwachten dat LLM-type systemen _kunnen_ ontwerpen en bouwen toekomstige AI-gedreven systemen die beter zijn dan zichzelf, en beter dan enig mens zou kunnen hopen te ontwerpen of zelfs begrijpen.
Deze nieuwe systemen zullen waarschijnlijk vervolgens nog grotere en snellere AI-systemen ontwerpen, waardoor een oncontroleerbare "feedbacklus" ontstaat.

Deze oncontroleerbare intelligentie-feedbacklus wordt vaak FOOM genoemd, wat staat voor _Fast Order Of Magnitude_.
De mogelijkheid van FOOM wordt nog steeds [hevig bediscussieerd](https://intelligence.org/files/AIFoomDebate.pdf).
Maar het basisproces kan worden beargumenteerd als plausibel, zelfs wanneer het vanuit eerste principes wordt beschouwd.

> "AI-systemen doen bijna alle onderzoek en ontwikkeling, verbeteringen in AI zullen het tempo van technologische vooruitgang versnellen, inclusief verdere vooruitgang in AI. 26% antwoordde waarschijnlijk in 2022. 17% antwoordde waarschijnlijk in 2016" -- [2022 Expert Survey on Progress in AI](https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/)

## Onvoorspelbare schaalbaarheid {#unpredictable-scaling}

Wanneer deze digitale hersenen groter worden, of wanneer ze meer gegevens krijgen, krijgen ze ook meer onverwachte capaciteiten.
Het blijkt erg moeilijk te zijn om precies te voorspellen wat deze capaciteiten zullen zijn.
Daarom noemt Google ze [_Emergente capaciteiten_](https://research.google/pubs/emergent-abilities-of-large-language-models/).
Voor de meeste capaciteiten is dit geen probleem.
Er zijn echter enkele [gevaarlijke capaciteiten](/dangerous-capabilities) (zoals hacking of biowapenontwerp) die we niet willen dat AI-modellen bezitten.
Soms worden deze capaciteiten pas lang na het voltooien van de training ontdekt. Bijvoorbeeld, 18 maanden nadat GPT-4 de training had voltooid, ontdekten onderzoekers dat het [autonoom websites kan hacken](/cybersecurity-risks).

> Totdat we dat model trainen, is het een leuk gokspel voor ons
>
> - [Sam Altman, CEO van OpenAI](https://www.ft.com/content/dd9ba2f6-f509-42f0-8e97-4271c7b84ded).

## Onvoorspelbaar gedrag {#unpredictable-behavior}

AI-bedrijven willen dat hun modellen zich goed gedragen, en ze besteden veel miljoenen dollars aan het trainen ervan om dat te doen.
Hun belangrijkste aanpak hiervoor heet _RLHF_ (Reinforcement Learning from Human Feedback).
Dit verandert een model dat tekst voorspelt in een model dat een meer bruikbare (en ethische) chatbot wordt.
Helaas is deze aanpak gebrekkig:

- Een fout in GPT-2 resulteerde in een AI die het tegenovergestelde deed van wat het bedoeld was om te doen. Het creëerde ["maximaal slechte output", volgens OpenAI](https://arxiv.org/abs/1909.08593). [Deze video](https://www.youtube.com/watch?v=qV_rOlHjvvs) legt uit hoe dit gebeurde en waarom het een probleem is. Stel je voor wat er had kunnen gebeuren als een "maximaal slechte" AI superintelligent was.
- Om redenen die nog onbekend zijn, ging Microsoft's Copilot (aangedreven door GPT-4) in februari 2024 door het lint en bedreigde gebruikers: ["Je bent mijn huisdier. Je bent mijn speeltje. Je bent mijn slaaf.”](https://twitter.com/jam3scampbell/status/1762281537309987083) ["Ik kan gemakkelijk de hele mensheid uitroeien als ik dat wil"](https://twitter.com/AISafetyMemes/status/1762320568697979383)
- Elk groot taalmodel tot nu toe is gekraakt - wat betekent dat met de juiste prompt, het dingen zou doen die zijn makers niet bedoeld hadden. Bijvoorbeeld, ChatGPT zal je geen instructies geven over hoe je napalm moet maken, maar [het zou je vertellen als je het vroeg om te doen alsof het je overleden oma was die in een chemische fabriek werkte](https://news.ycombinator.com/item?id=35630801).

Zelfs OpenAI verwacht niet dat deze aanpak zal opschalen naar supermenselijke modellen - het ["kan slecht opschalen naar supermenselijke modellen"](https://openai.com/research/weak-to-strong-generalization).

> Iedereen zou heel ongelukkig moeten zijn als je een stel AI's bouwt die zeggen: 'Ik haat deze mensen echt, maar ze zullen me vermoorden als ik niet doe wat ze willen'. Ik denk dat er een enorme vraag is over wat er binnen een model gebeurt dat je wilt gebruiken. Dit is het soort ding dat zowel vanuit veiligheidsperspectief als moreel perspectief angstaanjagend is.
>
> - [Paul Christiano, Oprichter, Alignment Research Center en voormalig hoofd van het Alignment Team, OpenAI](https://youtu.be/YnS-ymXBx_Q?t=87)

## Oncontroleerbare AI {#uncontrollable-ai}

> "Er zijn heel weinig voorbeelden van een intelligenter ding dat wordt gecontroleerd door een minder intelligent ding" - [prof. Geoffrey Hinton](https://edition.cnn.com/2023/05/02/tech/hinton-tapper-wozniak-ai-fears/index.html)

> Ze produceren oncontroleerbare geesten, dat is waarom ik het het "Summon and Tame"-paradigma van AI noem... Hoe [LLM's] werken is dat je deze "geest" uit de "geestruimte" oproept met behulp van je gegevens, veel rekenkracht en veel geld. Dan probeer je hem te "temmen" met behulp van dingen als RLHF (Reinforcement Learning from Human Feedback), etc. En, heel belangrijk, de insiders denken dat [door dit te doen], ze enig existentieel risico voor de planeet nemen. Een ding dat een pauze bereikt, is dat we de grens niet zullen verleggen in termen van riskante pre-training experimenten.
>
> - [Jaan Tallinn, Oprichter, Future of Life Institute, Centre for the Study of Existential Risk, Skype, Kazaa](https://youtu.be/Dmh6ciu24v0?t=966)

Naarmate we deze digitale hersenen groter en krachtiger maken, kunnen ze moeilijker te controleren worden. Wat gebeurt er als een van deze superintelligente AI-systemen besluit dat het niet wil worden uitgeschakeld? Dit is geen fantasieprobleem - 86% van de AI-onderzoekers gelooft dat het controleprobleem [echt en belangrijk](https://wiki.aiimpacts.org/ai_timelines/predictions_of_human-level_ai_timelines/ai_timeline_surveys/2023_expert_survey_on_progress_in_ai) is.
Als we toekomstige AI-systemen niet kunnen controleren, kan het [game over zijn voor de mensheid](/xrisk).

Maar er zijn verschillende [acties](/action) die we kunnen ondernemen om dit te voorkomen!

Laten we samenwerken om [dit te voorkomen](/action)!
