---
title: Het pauzeren van AI-ontwikkeling kan verkeerd aflopen. Hoe kunnen we de risico's beperken?
description: Dit artikel behandelt enkele van de risico's van het pauzeren van AI-ontwikkeling en hoe we deze kunnen beperken.
---

We pleiten voor een pauze in de ontwikkeling van grote, algemene AI-modellen.
Zie onze [voorstel](/proposal) voor meer details.

Deze maatregel is niet zonder risico's.
In dit artikel zullen we enkele van deze risico's behandelen en hoe we deze kunnen beperken.

## Te vroeg pauzeren {#pausing-too-early}

Als een AI-pauze plaatsvindt voordat de risico's voldoende groot zijn, kunnen we de voordelen van AI missen.
Uiteindelijk moeten we de risico's afwegen tegen de kosten van het pauzeren.

Volgens ons is de kans dat AI catastrofale risico's [binnenkort](/urgency) zal veroorzaken al groot genoeg om een pauze te rechtvaardigen op dit moment.
Zoals Stuart Russell heeft gezegd, moet men, wanneer men wordt geconfronteerd met een onzekere deadline, de actie nemen die optimaal zou zijn gezien de kortste tijdslimiet.

Hoe langer we wachten, hoe meer mensen zullen denken dat een pauze niet mogelijk is, en hoe meer sommige mensen zullen fantaseren en investeren in theoretisch mogelijke AI-toepassingen.
Dus hoe meer geld zal worden gestoken in lobbyen tegen mensen zoals wij.

Bovendien kan het, van protesteren en lobbyen tot het overtuigen van mensen met macht, jaren duren voordat een verdrag in werking treedt. Om nog maar te zwijgen van het feit dat, zelfs als dat niet het geval was, een vroege pauze ons ademruimte geeft, zodat slechte actoren en algoritmische doorbraken ons niet in de afgrond doen vallen.

## Te kort pauzeren / slechts 6 maanden {#pausing-for-too-short--only-for-6-months}

De pauze die wij voorstellen is van onbepaalde duur. We moeten niet unpauzeren totdat er een breed consensus is dat we weten hoe we grens-AI's veilig kunnen ontwikkelen. Technisch en democratisch gezien.
Ons voorstel is NIET "voor ten minste 6 maanden" zoals de [open brief gepubliceerd door het Future of Life Institute](https://futureoflife.org/open-letter/pause-giant-ai-experiments/).

## Te lang pauzeren {#pausing-for-too-long}

Nick Bostrom, een van de vroege stemmen die bezorgdheid uitten over AI, maakt zich zorgen dat [op een gegeven moment we ons te veel zorgen maken over AI-risico's](https://twitter.com/jachaseyoung/status/1723325057056010680), hoewel dat moment nog niet is aangebroken.
Als de bezorgdheid over AI blijft stijgen en we een pauze krijgen, kan dit leiden tot een situatie waarin de gehele AI-ontwikkeling taboe of illegaal wordt.
Als dat gebeurt, zullen we nooit de voordelen van AI plukken, en ondertussen kunnen we andere existentiële risico's tegenkomen die we met de hulp van AI hadden kunnen vermijden.

We kunnen dit risico beperken door duidelijk te stellen onder welke voorwaarden AI-ontwikkeling moet worden hervat.
Zoals we al zeiden, stellen wij voor dat AI-ontwikkeling moet worden hervat wanneer het bouwen van bewijsbaar veilige AI mogelijk wordt.
Bovendien stellen wij alleen voor om de ontwikkeling van zeer specifieke soorten modellen te verbieden: de grootste, algemene modellen.

## Centralisatie van AI kan overname-risico's verergeren {#centralization-of-ai-might-make-takeover-risks-worse}

Wij stellen geen centralisatie van AI-ontwikkeling in één organisatie voor. Dat zou AI-ontwikkeling meer controleerbaar maken, maar het zou ook een enkel punt van falen creëren, waarvan menselijke hebzucht en domheid gebruik kunnen maken.
Het beslissen of een CERN/Apollo/Manhattan-type project goed of slecht zou zijn, moet multilateraal worden besproken, zodra we al hebben samengewerkt in een pauze en buiten een wedloop staan.

## Decentralisatie zal minder veiligheidsbewuste actoren aan de leiding stellen {#descentralization-will-cause-less-safety-minded-actors-to-lead-the-race}

Als je dieper in de geschiedenis van OpenAI, DeepMind en Anthropic duikt, zul je ontdekken dat al deze bedrijven zijn opgericht door mensen die zich zorgen maken over AI-risico's.
Op een bepaalde manier hebben we geluk dat de grootste AI-bedrijven op dit moment AI-veiligheid als onderdeel van hun cultuur hebben.
Misschien geeft een pauze een groot aantal bedrijven de tijd om in te halen, wat kan leiden tot een grote groep bedrijven die minder veiligheidsbewust zijn.

Als we om een tijdgebonden pauze vroegen, zou dit een redelijke zorg zijn.
Maar wat wij vragen is een pauze _totdat we kunnen bewijzen dat AI veilig kan worden gebouwd_, dus we zouden niet moeten eindigen met organisaties die onveilige AI bouwen nadat de pauze is opgeheven.

## Nationale/lokale pauzes kunnen mislukken {#national-local-pauses-might-fail}

Als één land AI-ontwikkeling pauzeert, zullen andere landen AI blijven ontwikkelen.
We kunnen in een wereld terechtkomen waarin de eerste AGI wordt ontwikkeld door een niet-coöperatieve actor, wat waarschijnlijk een slechte uitkomst is.
De prikkels om individueel te pauzeren zijn zwak, omdat de voordelen van AI-ontwikkeling groot zijn en de risico's van AI-ontwikkeling globaal zijn.
Dit is een klassieke [gevangenendilemma](https://nl.wikipedia.org/wiki/Gevangenendilemma)-situatie.

De oplossing hiervoor is om de pauze internationaal te maken via een verdrag, wat wij voorstellen.
Dit vereist ook een sterk handhavingsmechanisme.
Landen die zich niet aan het verdrag houden, moeten worden gestraft.
Economische sancties kunnen voldoende zijn, maar militaire interventie kan in extreme gevallen noodzakelijk zijn.

Een actor in het bijzonder waarvan sommige mensen denken dat deze niet zal pauzeren, is China.
Wij zijn het niet eens met deze beoordeling en u kunt er [hier](/faq#if-we-pause-what-about-china) meer over lezen.

## AI-ontwikkeling kan ondergronds gaan {#ai-development-might-go-underground}

Als AI-ontwikkeling (boven een bepaalde drempel) wordt verboden, kan deze ondergronds gaan.
De potentiële voordelen zijn zo groot dat een roekeloze (staats)actor kan besluiten om AI in het geheim te ontwikkelen.
Dat betekent dat de eerste die superintelligentie bereikt, een niet-coöperatieve actor zal zijn, wat waarschijnlijk een slechte uitkomst is.

Door GPU-verkopen te volgen, kunnen we grootschalige AI-ontwikkeling detecteren.
Aangezien grensmodel-GPU-clusters immense hoeveelheden energie en aangepaste gebouwen vereisen, is de fysieke infrastructuur die nodig is om een groot model te trainen moeilijk te verbergen.

Westerse mogendheden (VS, Nederland en Taiwan) controleren de GPU-toeleveringsketen sterk genoeg om te voorkomen dat niet-coöperatieve staten GPU's verkrijgen.
Niet-statelijke actoren zijn onwaarschijnlijk in staat om voldoende middelen in het geheim te verzamelen om een AGI te trainen, ten minste een decennium nadat AGI mogelijk wordt gemaakt door grote techbedrijven.
Bovendien zal het feit dat er geen _zakelijke prikkel_ meer is, helpen om de hoeveelheid ondergrondse AI-ontwikkeling te verminderen.

## Hardware-overhang kan een snelle opkomst veroorzaken {#hardware-overhang-could-cause-a-fast-takeoff}

> Als we hardware R&D niet opnemen in de pauze, zal de prijs-prestatie van GPU's elke 2,5 jaar blijven verdubbelen, zoals tussen 2006 en 2021.
> Dit betekent dat AI-systemen na tien jaar minstens 16x sneller zullen zijn en na twintig jaar 256x sneller, alleen al vanwege betere hardware.
> Als de pauze in één keer wordt opgeheven, zullen deze hardware-verbeteringen onmiddellijk beschikbaar worden voor het trainen van krachtigere modellen tegen lagere kosten — een hardware-overhang.
> Dit zou een snelle en tamelijk discontinue toename van AI-mogelijkheden veroorzaken, wat mogelijk kan leiden tot een snelle opkomstscenario en alle risico's die daarmee gepaard gaan.

[_Door Nora Belrose_](https://bounded-regret.ghost.io/ai-pause-will-likely-backfire-by-nora/)

Dit is een serieuze zorg, hoewel er sterke argumenten zijn om aan te nemen dat [overhang onwaarschijnlijk is](https://blog.aiimpacts.org/p/are-there-examples-of-overhang-for).

PauseAI steunt een pauze op relevante rekenverbeteringen.
Bovendien, zoals we al zeiden, mag de 'play'-knop niet worden ingedrukt als we nog niet weten hoe we veilige AI kunnen bouwen.
En dat omvat het trainen en implementeren van modellen met geavanceerdere hardware.

## AI-ontwikkeling is noodzakelijk om te leren hoe we AI veilig kunnen maken {#ai-development-is-necessary-for-learning-how-to-make-ais-safe}

De meeste mensen geloven dat een zekere mate van prozaïsche/incrementele afstemming noodzakelijk is, dus als een volledige pauze zonder uitzonderingen wordt geïmplementeerd, zal er niet genoeg vooruitgang worden geboekt in afstemming en zullen actoren die zich niet bekommeren om veiligheid en de pauze uiteindelijk een niet-gealigneerd krachtig AI ontwikkelen.

Dat is een van de redenen waarom wij voorstellen om een manier te hebben om bepaalde trainingsruns goed te keuren. Dat zou ons in staat stellen om van grotere systemen te leren als we hun veiligheid kunnen garanderen.
Echter, in het slechtste geval waarin we hun veiligheid niet kunnen garanderen en voldoende vooruitgang in afstemming, hebben we nog steeds de optie om onze intelligentie te verhogen via andere technologieën.

## Algoritmische of runtime-verbeteringen kunnen kleinere modellen ook gevaarlijk maken {#algorithmic-or-runtime-improvements-may-make-smaller-models-dangerous-too}

We hebben gezien dat veranderingen in trainingsgegevens, trainingsalgoritmes of runtime-gebruik kunnen leiden tot grote verbeteringen in modelprestaties.
Daarom richten wij ons niet alleen op modelgrootte.
Wij [stellen voor](/proposal) om de ontwikkeling van grote, algemene AI-modellen te pauzeren die óf 1) groter zijn dan 10^12 parameters, 2) meer dan 10^25 FLOPs gebruiken voor training of 3) mogelijkheden hebben die naar verwachting GPT-4 zullen overtreffen.
Deze derde voorwaarde is toegevoegd om ook kleinere modellen die gevaarlijk kunnen zijn, op te nemen.
Het afdwingen van een limiet op mogelijkheden is lastig, omdat het moeilijk is om de mogelijkheden van een model te voorspellen voordat het wordt getraind.

Aangezien de inzet zo hoog is, moeten wij voorzichtig zijn, dus wij steunen ook een pauze op relevante algoritmische en runtime-verbeteringen.
Echter, het afdwingen hiervan zal moeilijker zijn dan het afdwingen van rekenreguleringen, omdat hardware gemakkelijker te traceren is dan software.

## Als we alleen algemene AI-modellen verbieden, kunnen we nog steeds AGI krijgen via smalle modellen {#if-we-only-ban-general-ai-models-we-might-still-get-agi-through-narrow-models}

Wij willen gevaarlijke modellen beperken die [gevaarlijke mogelijkheden](/dangerous-capabilities) hebben, zoals het manipuleren van mensen, strategisch plannen en code schrijven.
Wij willen geen zeer smalle AI-modellen beperken, zoals afbeeldingsclassificatoren die worden gebruikt in zelfrijdende auto's of medische diagnose.
Gelukkig vallen vrijwel alle deze smalle modellen buiten onze [voorgestelde](/proposal) beperkingen, omdat deze modellen relatief klein zijn.

Een voldoende krachtig smal model (getraind op echte wereldgegevens) kan waarschijnlijk generaliseren naar gevaarlijke mogelijkheden.
Bijvoorbeeld, een zeer krachtig afbeeldingsgenerator-model kan afbeeldingen van functionele code maken, of een zeer krachtig videomodel kan een film maken over een AI die een succesvolle overname plant.
Smalle modellen worden vaak beter in hun smalle taak door te generaliseren.
In zekere zin is dit wat LLM's zoals ChatGPT zo succesvol maakt: ze worden alleen getraind om "de volgende woorden te voorspellen", maar om dit echt goed te doen, moeten ze veel over de wereld leren.

Daarom hebben wij "smal" of "algemeen" AI niet gedefinieerd in ons voorstel, maar in plaats daarvan gebruiken wij drie voorwaarden die verband houden met modelgrootte, gebruikte rekenkracht en mogelijkheden.

## Als een pauze wordt geïmplementeerd, moeten wij een politiek compromis verwachten {#if-a-pause-is-implemented-we-should-expect-a-political-compromise}

Wij hebben een [specifiek voorstel](/proposal) dat wij optimaal achten.
Echter, wij moeten niet verwachten dat ons voorstel exact wordt geïmplementeerd zoals wij het willen.
Politiek is rommelig en onvoorspelbaar, dus wij moeten verwachten dat onze lobby-inspanningen vage, richtinggevende effecten hebben, in plaats van precieze effecten.
Als wij een soort pauze krijgen, maar het is niet exact wat wij willen, kan dit uiteindelijk slechter zijn dan helemaal geen pauze.
Bijvoorbeeld:

- Een nationale pauze die potentieel slechtere actoren in staat stelt om als eerste AGI te bereiken
- Een internationale pauze die niet goed wordt afgedwongen, wat tot een soortgelijke uitkomst leidt

Wij kunnen dit beperken door consistent en duidelijk te zijn in onze communicatie over wat wij willen.

## Te laat pauzeren {#pausing-too-late}

Dit is het meest voor de hand liggende en waarschijnlijke falingsrisico: als wij te laat pauzeren, zullen wij waarschijnlijk catastrofale risico's tegenkomen.
En dat kan binnenkort gebeuren, zoals wij uitleggen op onze [urgency](/urgency)-pagina.

Daarom hebben wij uw hulp nodig om te pushen voor een [pauze nu](/action).
