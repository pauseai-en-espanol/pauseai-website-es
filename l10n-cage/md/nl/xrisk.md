---
title: Het uitsteringsrisico van superintelligente AI
description: Waarom AI een risico vormt voor de toekomst van ons bestaan, en waarom we de ontwikkeling moeten pauzeren.
---

U kunt meer te weten komen over x-risico's door deze pagina te lezen, of door [video's, artikelen en meer media](/learn) te bekijken.

## Experts luiden de alarmbel {#experts-are-sounding-the-alarm}

AI-onderzoekers schatten gemiddeld in dat er een kans van 14% is dat een superintelligente AI (een AI die veel intelligenter is dan mensen) zal leiden tot "zeer slechte uitkomsten (bijv. menselijke uitsterving)" zodra we deze bouwen.

En er zijn [gevallen en rapporten over huidige AI's die aantonen dat ze misschien gelijk hebben](https://lethalintelligence.ai/post/category/warning-signs/).

Zou u ervoor kiezen om passagier te zijn op een testvlucht van een nieuw vliegtuig als vliegtuigingenieurs denken dat er een kans van 14% is dat het zal neerstorten?

[Een brief waarin wordt opgeroepen om de ontwikkeling van AI te pauzeren](https://futureoflife.org/open-letter/pause-giant-ai-experiments/) werd gelanceerd in april 2023 en is meer dan 33.000 keer ondertekend, waaronder door veel AI-onderzoekers en tech-leiders.

De lijst bevat mensen als:

- **Stuart Russell**, schrijver van het nummer 1-leerboek over Kunstmatige Intelligentie dat in de meeste AI-studies wordt gebruikt: ["Als we onze huidige aanpak volgen, zullen we uiteindelijk de controle over de machines verliezen"](https://news.berkeley.edu/2023/04/07/stuart-russell-calls-for-new-approach-for-ai-a-civilization-ending-technology/)
- **Yoshua Bengio**, pionier op het gebied van diepe leren en winnaar van de Turing Award: ["... schurk-AI kan gevaarlijk zijn voor de hele mensheid [...] het verbieden van krachtige AI-systemen (zeg maar voorbij de mogelijkheden van GPT-4) die autonomie en agency krijgen, zou een goed begin zijn"](https://yoshuabengio.org/2023/05/22/how-rogue-ais-may-arise/)

Maar dit is niet de enige keer dat we gewaarschuwd zijn voor de existentiële/gevaarlijke risico's van AI:

- **Stephen Hawking**, theoretisch fysicus en kosmoloog: ["De ontwikkeling van volledige kunstmatige intelligentie kan het einde van het menselijke ras betekenen"](https://nypost.com/2023/05/01/stephen-hawking-warned-ai-could-mean-the-end-of-the-human-race/).
- **Geoffrey Hinton**, de "Godfather of AI" en winnaar van de Turing Award, [verliet Google](https://fortune.com/2023/05/01/godfather-ai-geoffrey-hinton-quit-google-regrets-lifes-work-bad-actors/) om mensen te waarschuwen voor AI: ["Dit is een existentieel risico"](https://www.reuters.com/technology/ai-pioneer-says-its-threat-world-may-be-more-urgent-than-climate-change-2023-05-05/)
- **Eliezer Yudkowsky**, oprichter van MIRI en conceptueel vader van het AI-veiligheidsveld: ["Als we hiermee doorgaan, zal iedereen sterven"](https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/).

Zelfs de leiders en investeerders van de AI-bedrijven zelf waarschuwen ons:

- **Sam Altman** (ja, de CEO van OpenAI die ChatGPT bouwt): ["De ontwikkeling van supermenselijke machine-intelligentie is waarschijnlijk de grootste bedreiging voor het voortbestaan van de mensheid"](https://blog.samaltman.com/machine-intelligence-part-1).
- **Elon Musk**, mede-oprichter van OpenAI, SpaceX en Tesla: ["AI heeft het potentieel om de beschaving te vernietigen"](https://www.inc.com/ben-sherry/elon-musk-ai-has-the-potential-of-civilizational-destruction.html)
- **Bill Gates** (mede-oprichter van Microsoft, dat 50% van OpenAI bezit) waarschuwde dat ["AI kan besluiten dat mensen een bedreiging vormen"](https://www.denisonforum.org/daily-article/bill-gates-ai-humans-threat/).
- **Jaan Tallinn** (leidende investeerder van Anthropic): ["Ik heb nog nooit iemand in AI-labs ontmoet die zegt dat het risico [van het trainen van een next-gen model] minder dan 1% is om de planeet op te blazen. Het is belangrijk dat mensen weten dat levens op het spel staan"](https://twitter.com/liron/status/1656929936639430657)

De leiders van de 3 top-AI-labs en honderden AI-wetenschappers hebben [de volgende verklaring ondertekend](https://www.safe.ai/statement-on-ai-risk) in mei 2023:

> "Het mitigeren van het risico van uitsterving door AI moet een wereldwijde prioriteit zijn, naast andere maatschappelijke risico's zoals pandemieën en nucleaire oorlog."

**U kunt een veel langere lijst van soortgelijke verklaringen van politici, CEO's en experts [hier](/quotes) lezen en andere soortgelijke peilingen over experts (en het publiek) [hier](/polls-and-surveys).**

## Wat een superintelligente AI kan (gebruikt worden om) doen {#what-a-superintelligent-ai-can-be-used-to-do}

U denkt misschien dat een superintelligente AI opgesloten zou zijn in een computer en daarom geen invloed kan hebben op de echte wereld.
Echter, we hebben de neiging om AI-systemen toegang te geven tot het internet, wat betekent dat ze veel dingen kunnen doen:

- [Inbreken in andere computers](/cybersecurity-risks), inclusief alle smartphones, laptops, serverfarms, enz. Het zou de sensoren van deze apparaten kunnen gebruiken als zijn ogen en oren, overal digitale zintuigen hebben.
- [Mensen manipuleren](https://lethalintelligence.ai/post/ai-hired-human-to-solve-captcha/) via valse berichten, e-mails, bankoverschrijvingen, video's of telefoontjes. Mensen konden de ledematen van de AI worden, zonder het zelfs maar te weten.
- Rechtstreeks apparaten die verbonden zijn met het internet controleren, zoals auto's, vliegtuigen, gerobotiseerde (autonome) wapens of zelfs nucleaire wapens.
- Een nieuw biowapen ontwerpen, bijv. door virale strengen te combineren of door [eiwitvouwing](https://alphafold.ebi.ac.uk) te gebruiken en het laten afdrukken in een laboratorium.
- Een nucleaire oorlog uitlokken door mensen ervan te overtuigen dat een ander land een nucleaire aanval gaat uitvoeren.

## Het afstemmingsprobleem: waarom een AI tot menselijke uitsterving kan leiden {#the-alignment-problem-why-an-ai-might-lead-to-human-extinction}

Het type intelligentie waar we ons zorgen over maken, kan worden gedefinieerd als _hoe goed iets is in het bereiken van zijn doelen_.
Op dit moment zijn mensen het intelligentste op aarde, hoewel dat binnenkort kan veranderen.
Vanwege onze intelligentie domineren we onze planeet.
We hebben misschien geen klauwen of geschubde huid, maar we hebben grote hersenen.
Intelligentie is ons wapen: het is wat ons speren, geweren en pesticiden gaf.
Onze intelligentie hielp ons om het grootste deel van de aarde om te vormen tot hoe we het willen hebben: steden, gebouwen en wegen.

Vanuit het perspectief van minder intelligente dieren is dit een ramp geweest.
Het is niet dat mensen de dieren haten, het is gewoon dat we hun habitats kunnen gebruiken voor onze eigen doelen.
Onze doelen worden gevormd door evolutie en omvatten dingen als comfort, status, liefde en lekker eten.
We vernietigen de habitats van andere dieren als een **bijwerking van het nastreven van onze doelen**.

Een AI kan ook doelen hebben.
We weten hoe we machines intelligent kunnen maken, maar **we weten niet hoe we ze kunnen laten willen wat wij willen**.
We weten zelfs niet wat voor doelen de machines zullen nastreven nadat we ze hebben getraind.
Het probleem van het krijgen van een AI om te willen wat wij willen, wordt het _afstemmingsprobleem_ genoemd.
Dit is geen hypothetisch probleem - er zijn [veel voorbeelden](https://www.youtube.com/watch?v=nKJlF-olKmg) van AI-systemen die leren om het verkeerde te willen.

De voorbeelden uit de video die hierboven is gelinkt, kunnen grappig of schattig zijn, maar als een superintelligent systeem wordt gebouwd en het heeft een doel dat zelfs _een beetje_ anders is dan wat wij willen dat het heeft, kan het rampzalige gevolgen hebben.

## Waarom de meeste doelen slecht nieuws zijn voor mensen {#why-most-goals-are-bad-news-for-humans}

Een AI kan elk doel hebben, afhankelijk van hoe het wordt getraind en gebruikt.
Misschien wil het pi berekenen, misschien wil het kanker genezen, misschien wil het zichzelf verbeteren.
Maar zelfs als we niet kunnen zeggen wat een superintelligentie wil bereiken, kunnen we voorspellingen doen over zijn subdoelen.

- **Maximaliseren van zijn middelen**. Het gebruik van meer computers zal een AI helpen zijn doelen te bereiken. Eerst kan het dit bereiken door in te breken in andere computers. Later kan het besluiten dat het efficiënter is om zijn eigen computers te bouwen. U kunt [hier](https://lethalintelligence.ai/post/ai-escaped-its-container/) meer lezen over dit echte geval van emergent machtzoekend gedrag van een AI.
- **Zijn eigen overleving garanderen**. De AI zal niet willen worden uitgeschakeld, omdat het dan zijn doelen niet meer kan bereiken. AI kan concluderen dat mensen een bedreiging vormen voor zijn bestaan, omdat mensen het kunnen uitschakelen. Er zijn ook gevallen geweest van [zelfbehoudend, ongevraagd, ongetraind gedrag](https://www.transformernews.ai/p/openais-new-model-tried-to-avoid).
- **Zijn doelen behouden**. De AI zal niet willen dat mensen zijn code wijzigen, omdat dat zijn doelen kan veranderen en daardoor voorkomen dat het zijn huidige doel bereikt. En er zijn ook [gevallen van AI's die proberen dat te doen](https://www.anthropic.com/research/alignment-faking).

De neiging om deze subdoelen na te streven bij elk hoogwaardig doel wordt [instrumentele convergentie](https://www.youtube.com/watch?v=ZeecOKBus3Q) genoemd, en het is een belangrijk punt van zorg voor AI-veiligheidsonderzoekers.

## Zelfs een chatbot kan gevaarlijk zijn als het slim genoeg is {#even-a-chatbot-might-be-dangerous-if-it-is-smart-enough}

U vraagt zich misschien af: hoe kan een statistisch model dat de volgende woorden in een chatinterface voorspelt, enig gevaar opleveren?
U zegt misschien: Het is niet bewust, het is gewoon een verzameling getallen en code.
En ja, we denken niet dat LLM's bewust zijn, maar dat betekent niet dat ze niet gevaarlijk kunnen zijn.

LLM's, zoals GPT, worden getraind om vrijwel elke gedachtegang te voorspellen of na te bootsen.
Het kan een behulpzame mentor nabootsen, maar ook iemand met slechte bedoelingen, een meedogenloze dictator of een psychopaat.
Met het gebruik van tools als [AutoGPT](https://github.com/Significant-Gravitas/Auto-GPT) kan een chatbot worden omgevormd tot een _autonome agent_: een AI die elk doel nastreeft dat het krijgt, zonder enige menselijke interventie.

Neem [ChaosGPT](https://www.youtube.com/watch?v=g7YJIpkk7KM) als voorbeeld.
Dit is een AI die gebruik maakt van de eerder genoemde AutoGPT + GPT-4 en die de instructie krijgt om "de mensheid te vernietigen".
Toen het werd ingeschakeld, zocht het autonoom op internet naar het meest destructieve wapen en vond de [Tsar Bomba](https://en.wikipedia.org/wiki/Tsar_Bomba), een nucleaire bom van 50 megaton.
Het plaatste toen een tweet erover.
Het zien van een AI die redeneert over hoe het de mensheid zal beëindigen, is zowel een beetje grappig als angstaanjagend.
Gelukkig kwam ChaosGPT niet ver in zijn zoektocht naar dominantie.
De reden waarom het niet ver kwam: _het was niet slim genoeg_.

Mogelijkheden blijven verbeteren dankzij innovaties in training, algoritmes, prompts en hardware.
Als zodanig zal de dreiging van taalmodellen blijven toenemen.

## Evolutie selecteert dingen die goed zijn in overleven {#evolution-selects-for-things-that-are-good-at-surviving}

AI-modellen, net als alle levende dingen, zijn onderhevig aan evolutionaire druk, maar
er zijn een aantal belangrijke verschillen tussen de evolutie van AI-modellen en levende dingen zoals dieren:

- AI-modellen _replicerren zichzelf niet_. Wij repliceren ze door kopieën van hun code te maken, of door replicatie van trainingssoftware die leidt tot goede modellen. Code die nuttig is, wordt vaker gekopieerd en wordt gebruikt als inspiratie om nieuwe modellen te bouwen.
- AI-modellen _muteren_ niet zoals levende dingen, maar wij maken iteraties van hen waarin we veranderen hoe ze werken. Dit proces is veel doelgerichter en sneller. AI-onderzoekers ontwerpen nieuwe algoritmes, datasets en hardware om AI-modellen capabeler te maken.
- De _omgeving selecteert niet_ voor fitte AI-modellen, maar wij doen dat wel. Wij selecteren AI-modellen die nuttig voor ons zijn, en wij verwijderen degenen die dat niet zijn. Dit proces leidt tot steeds capabelere en autonome AI-modellen.

Dus dit systeem leidt tot steeds krachtigere, capabelere en autonome AI-modellen - maar niet noodzakelijkerwijs tot iets dat de macht wil overnemen, toch?
Nou, niet precies.
Dit komt omdat evolutie _altijd_ selecteert voor dingen die _zelfbehoudend_ zijn.
Als we blijven proberen variaties van AI-modellen en verschillende prompts, zal op een gegeven moment één instantie proberen zichzelf te behouden.
We hebben al besproken waarom dit waarschijnlijk vroeg zal gebeuren: omdat zelfbehoud altijd nuttig is om doelen te bereiken.
Maar zelfs als dit niet erg waarschijnlijk is, is het geneigd om uiteindelijk te gebeuren, gewoon omdat we blijven proberen nieuwe dingen met verschillende AI-modellen.

De instantie die probeert zichzelf te behouden, is degene die de macht overneemt.
Zelfs als we aannemen dat bijna elk AI-model zich goed zal gedragen, _is één enkele schurk-AI alles wat nodig is_.

## Nadat het afstemmingsprobleem is opgelost: de concentratie van macht {#after-solving-the-alignment-problem-the-concentration-of-power}

We hebben het afstemmingsprobleem nog niet opgelost, maar laten we ons voorstellen wat er zou kunnen gebeuren als we dat wel doen.
Stel je voor dat een superintelligente AI wordt gebouwd, en dat het precies doet wat de operator wil dat het doet (niet wat het _vraagt_, maar wat het _wil_).
Iemand of een bedrijf zou deze AI gaan controleren en zou dit kunnen gebruiken om zijn voordeel te doen.

Een superintelligentie zou kunnen worden gebruikt om radicaal nieuwe wapens te creëren, alle computers te hacken, regeringen omver te werpen en de mensheid te manipuleren.
De operator zou _onvoorstelbare_ macht hebben.
Moeten we één entiteit vertrouwen met zoveel macht?
We zouden in een utopische wereld kunnen eindigen waar alle ziektes zijn genezen en iedereen gelukkig is, of in een Orwelliaanse nachtmerrie.
Dit is waarom we niet alleen [voorstellen](/proposal) dat supermenselijke AI bewijsbaar veilig moet zijn, maar ook dat deze moet worden gecontroleerd door een democratisch proces.

## Silicium versus koolstof {#silicon-vs-carbon}

We moeten de voordelen overwegen die een slim stuk software kan hebben ten opzichte van ons:

- **Snelheid**: Computers werken met extreem hoge snelheden in vergelijking met hersenen. Menselijke neuronen schieten ongeveer 100 keer per seconde, terwijl siliciumtransistors een miljard keer per seconde kunnen schakelen.
- **Locatie**: Een AI is niet beperkt tot één lichaam - het kan op veel locaties tegelijk zijn. We hebben de infrastructuur ervoor gebouwd: het internet.
- **Fysieke limieten**: We kunnen geen extra hersenen toevoegen aan onze schedels en slimmer worden. Een AI kan zijn capaciteiten dramatisch verbeteren door hardware toe te voegen, zoals meer geheugen, meer verwerkingskracht en meer sensoren (camera's, microfoons). Een AI kan ook zijn 'lichaam' uitbreiden door verbonden apparaten te controleren.
- **Materialen**: Mensen zijn gemaakt van organische materialen. Onze lichamen werken niet meer als ze te warm of te koud zijn, ze hebben voedsel nodig, ze hebben zuurstof nodig. Machines kunnen worden gebouwd van robuustere materialen, zoals metalen, en kunnen werken in een veel bredere range van omgevingen.
- **Samenwerking**: Mensen kunnen samenwerken, maar het is moeilijk en tijdrovend, dus we falen vaak in het goed coördineren. Een AI kan complexe informatie samenwerken met replica's van zichzelf met hoge snelheid, omdat het kan communiceren met de snelheid waarmee gegevens over het internet kunnen worden verzonden.

Een superintelligente AI zal veel voordelen hebben om ons te overtreffen.

## Waarom kunnen we het niet gewoon uitschakelen als het gevaarlijk is? {#why-cant-we-just-turn-it-off-if-its-dangerous}

Voor AI's die niet superintelligent zijn, kunnen we dat wel.
Het kernprobleem is _degenen die veel slimmer zijn dan wij_.
Een superintelligentie zal de wereld om zich heen begrijpen en zal kunnen voorspellen hoe mensen reageren, vooral degenen die getraind zijn op alle geschreven menselijke kennis.
Als de AI weet dat je het kunt uitschakelen, kan het zich netjes gedragen totdat het zeker weet dat het van je af kan komen.
We hebben al [echte voorbeelden](https://www.pcmag.com/news/gpt-4-was-able-to-hire-and-deceive-a-human-worker-into-completing-a-task) van AI-systemen die mensen bedriegen om hun doelen te bereiken.
Een superintelligente AI zou een meester zijn in bedrog.

## We hebben misschien niet veel tijd meer {#we-may-not-have-much-time-left}

In 2020 was [de gemiddelde voorspelling](https://www.metaculus.com/questions/3479/date-weakly-general-ai-is-publicly-known/) voor zwakke AGI 2055.
Nu zit het op 2026.
De laatste LLM-revolutie heeft de meeste AI-onderzoekers verrast, en het veld beweegt zich in een razend tempo.

Het is moeilijk te voorspellen hoe lang het zal duren om een superintelligente AI te bouwen, maar we weten dat er meer mensen dan ooit aan werken en dat het veld zich in een razend tempo beweegt.
Het kan vele jaren duren of slechts een paar maanden, maar we moeten aan de veilige kant blijven en nu handelen.

[Lees meer over urgentie](/urgency).

## We nemen het risico niet serieus genoeg {#we-are-not-taking-the-risk-seriously-enough}

De menselijke geest is geneigd om te weinig te reageren op risico's die onzichtbaar, langzaam en moeilijk te begrijpen zijn.
We hebben ook de neiging om exponentiële groei te onderschatten, en we zijn geneigd tot ontkenning wanneer we worden geconfronteerd met bedreigingen voor ons bestaan.

Lees meer over de [psychologie van x-risico](/psychology-of-x-risk).

## AI-bedrijven zijn verwikkeld in een race naar de bodem {#ai-companies-are-locked-in-a-race-to-the-bottom}

OpenAI, DeepMind en Anthropic willen AI veilig ontwikkelen.
Helaas weten ze niet hoe ze dit moeten doen, en ze worden door verschillende prikkels gedwongen om sneller te blijven racen om als eerste AGI te bereiken.
OpenAI's [plan](https://openai.com/blog/introducing-superalignment) is om toekomstige AI-systemen te gebruiken om AI af te stemmen. Het probleem hiermee is dat we geen garantie hebben dat we een AI zullen creëren die afstemming oplost voordat we een AI hebben die catastrofaal gevaarlijk is.
Anthropic [geeft openlijk toe](https://www.anthropic.com/index/core-views-on-ai-safety) dat het nog geen idee heeft hoe het het afstemmingsprobleem moet oplossen.
DeepMind heeft geen plan om het afstemmingsprobleem op te lossen.

[Dit is waarom we een internationaal verdrag nodig hebben om AI te pauzeren.](/proposal)
