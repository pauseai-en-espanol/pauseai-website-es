---
title: Leer waarom AI-veiligheid belangrijk is
description: Educatieve bronnen (video's, artikelen, boeken) over AI-risico's en AI-alignering
---

<script>
import NewsletterSignup from '$lib/components/NewsletterSignup.svelte';
</script>

<NewsletterSignup />

## Op deze website {#on-this-website}

- [Risico's](/risks). Een overzicht van de risico's van AI.
- [X-risico](/xrisk). Waarom AI een existentieel risico vormt.
- [Overname](/ai-takeover). Hoe AI de wereld kan overnemen.
- [Citaten](/quotes). Citaten over AI-risico's en bestuur.
- [Haalbaarheid van een pauze](/feasibility). De haalbaarheid van een pauze in AI-ontwikkeling.
- [De pauzeknop bouwen](/building-the-pause-button). Wat nodig is om AI te pauzeren.
- [FAQ](/faq). Veelgestelde vragen over AI-veiligheid en PauseAI.
- [Actie](/action). Wat je kunt doen om te helpen (met links naar veel actiegerelateerde gidsen)

## Andere websites {#other-websites}

- [The Compendium](https://www.thecompendium.ai/). Een uitgebreide bundel kennis over waarom de huidige AI-race zo gevaarlijk is en wat we eraan kunnen doen.
- [A Narrow Path](https://www.narrowpath.co/). Een gedetailleerd plan over de stappen die we moeten nemen om onze kansen op overleven in de komende decennia te vergroten.
- [Keep the Future Human](https://keepthefuturehuman.ai/). Een artikel van FLI's Anthony Aguirre over waarom en hoe we de toekomst menselijk kunnen houden. (bekijk [hoofdstuk 8: Hoe je geen AGI bouwt](https://keepthefuturehuman.ai/chapter-8-how-to-not-build-agi/))
- [AISafety.com](https://www.aisafety.com) & [AISafety.info](https://aisafety.info). De landingspagina's voor AI-veiligheid. Leer over de risico's, gemeenschappen, evenementen, banen, cursussen, ideeën over hoe je de risico's kunt verminderen en meer!
- [Existential Safety](https://existentialsafety.org/). Een uitgebreide lijst van acties die we kunnen nemen om onze existentiële veiligheid van AI te vergroten.
- [AISafety.dance](https://aisafety.dance). Een leukere, vriendelijke en interactieve introductie tot de AI-catastrofische risico's!
- [AISafety.world](https://aisafety.world/tiles/). Het hele AI-veiligheidslandschap met alle organisaties, media-uitingen, forums, blogs en andere actoren en bronnen.
- [IncidentDatabase.ai](https://incidentdatabase.ai/). Database van incidenten waarbij AI-systemen schade hebben veroorzaakt.

- [LethalIntelligence.ai](https://lethalintelligence.ai/). Een verzameling bronnen over AI-risico's en AI-alignering.

## Nieuwsbrieven {#newsletters}

- [PauseAI Substack](https://pauseai.substack.com/): Onze nieuwsbrief.
- [TransformerNews](https://www.transformernews.ai/) Uitgebreide wekelijkse nieuwsbrief over AI-veiligheid en bestuur.
- [Don't Worry About The Vase](https://thezvi.substack.com/): Een nieuwsbrief over AI-veiligheid, rationaliteit en andere onderwerpen.

## Video's {#videos}

- [PauseAI Playlist](https://www.youtube.com/playlist?list=PLI46NoubGtIJa0JVCBR-9CayxCOmU0EJt) is een YouTube-afspeellijst die we hebben samengesteld, met video's van 1 minuut tot 1 uur in verschillende formaten en van diverse bronnen, en die geen voorkennis vereist.
- [Robert Miles' YouTube](https://www.youtube.com/watch?v=tlS5Y2vm02c&list=PLfHsskCxi_g-c62a_dmsNuHynaXsRQm40) zijn een geweldige plek om de basisprincipes van AI-alignering te begrijpen.
- [LethalIntelligence's YouTube](https://www.youtube.com/channel/UCLwop3J1O7wL-PNWGjQw8fg)

## Podcasts {#podcasts}

- [DoomDebates](https://www.youtube.com/@DoomDebates) door Liron Shapira, volledig gericht op AI-doom.
- [For Humanity Podcast](https://www.youtube.com/@ForHumanityPodcast) door ex-nieuwsanker John Sherman.
- [Future of Life Institute | Connor Leahy over AI-veiligheid en waarom de wereld fragiel is](https://youtu.be/cSL3Zau1X8g?si=0X3EKoxZ80_HN9Rl&t=1803). Interview met Connor over de AI-veiligheidsstrategieën.
- [Lex Fridman | Max Tegmark: Het geval voor het stoppen van AI-ontwikkeling](https://youtu.be/VcVfceTsD0A?t=1547). Interview dat dieper ingaat op de details van onze huidige gevaarlijke situatie.
- [Sam Harris | Eliezer Yudkowsky: AI, racen naar de afgrond](https://samharris.org/episode/SE60B0CF4B8). Gesprek over de aard van intelligentie, verschillende soorten AI, het aligneringsprobleem, Is vs Ought, en meer. Een van de vele afleveringen van Making Sense over AI-veiligheid.
- [Connor Leahy, AI-brandalarm](https://youtu.be/pGjyiqJZPJo?t=2510). Praatje over de intelligentie-explosie en waarom het de belangrijkste gebeurtenis zou zijn die ooit zou kunnen gebeuren.
- [De 80.000 uur Podcast aanbevolen afleveringen over AI](https://80000hours.org/podcast/on-artificial-intelligence/). Niet 80.000 uur lang, maar een compilatie van afleveringen van de 80.000 uur Podcast over AI-veiligheid.
- [Future of Life Institute Podcast-afleveringen over AI](https://futureoflife.org/podcast/?_category_browser=ai). Alle afleveringen van de FLI Podcast over de toekomst van kunstmatige intelligentie.

Podcasts met PauseAI-leden kunnen worden gevonden in de [media-aandacht](/press) lijst.

## Artikelen {#articles}

- [Het 'kijk niet omhoog'-denken dat ons met AI kan doen](https://time.com/6273743/thinking-that-could-doom-us-with-ai/) (door Max Tegmark)
- [Het pauzeren van AI-ontwikkeling is niet genoeg. We moeten het allemaal stoppen](https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/) (door Eliezer Yudkowsky)
- [Het geval voor het vertragen van AI](https://www.vox.com/the-highlight/23621198/artificial-intelligence-chatgpt-openai-existential-risk-china-ai-safety-technology) (door Sigal Samuel)
- [De AI-revolutie: de weg naar superintelligentie](https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html) (door WaitButWhy)
- [Hoe rogue AI's kunnen ontstaan](https://yoshuabengio.org/2023/05/22/how-rogue-ais-may-arise/) (door Yoshua Bengio)

- [Redeneren door argumenten tegen het serieus nemen van AI-veiligheid](https://yoshuabengio.org/2024/07/09/reasoning-through-arguments-against-taking-ai-safety-seriously/) (door Yoshua Bengio)
- [The Compendium](https://www.thecompendium.ai/)
- [A Narrow Path](https://www.narrowpath.co/)
- [Keep the Future Human](https://keepthefuturehuman.ai/)
- Bekijk de Leessectie op [LethalIntelligence.ai](https://lethalintelligence.ai/reading-time/)

Als je wilt lezen wat journalisten over PauseAI hebben geschreven, bekijk dan de lijst van [media-aandacht](/press).

## Boeken {#books}

- [Als iemand het bouwt, sterft iedereen](https://ifanyonebuildsit.com/) (Eliezer Yudkowsky & Nate Soares, 2025)
- [Oncontroleerbaar: de dreiging van kunstmatige superintelligentie en de race om de wereld te redden](https://www.goodreads.com/book/show/202416160-uncontrollable) (Darren McKee, 2023). Krijg het gratis!

- [Het aligneringsprobleem](https://www.goodreads.com/book/show/50489349-the-alignment-probleem) (Brian Christian, 2020)
- [Menselijk compatibel: kunstmatige intelligentie en het probleem van controle](https://www.goodreads.com/en/book/show/44767248) (Stuart Russell, 2019)
- [Leven 3.0: mens zijn in het tijdperk van kunstmatige intelligentie](https://www.goodreads.com/en/book/show/34272565) (Max Tegmark, 2017)
- [Superintelligentie: paden, gevaren, strategieën](https://www.goodreads.com/en/book/show/20527133) (Nick Bostrom, 2014)

## Papers {#papers}

- [Een compilatie](https://arkose.org/aisafety) van AI-veiligheidspapers
- [Een andere compilatie](https://futureoflife.org/resource/introductory-resources-on-ai-risks/#toc-44245428-2) van AI-veiligheidspapers
- [Aligneringsfaking in grote taalmodellen](https://www.anthropic.com/news/alignment-faking) recent paper door Anthropic zelf
- [Het beheersen van extreme AI-risico's te midden van snelle vooruitgang](https://www.science.org/doi/abs/10.1126/science.adn0117) van de peetvaders van het veld

## Cursussen {#courses}

- [Introductie tot transformatieve AI](https://aisafetyfundamentals.com/intro-to-tai/) (15 uur)
- [AGI-veiligheidsfundamentals](https://www.agisafetyfundamentals.com/) (30 uur)
- [CHAI-bibliografie van aanbevolen materialen](https://humancompatible.ai/bibliography) (50 uur+)
- [AISafety.training](https://aisafety.training/): Overzicht van trainingsprogramma's, conferenties en andere evenementen

## Organisaties {#organizations}

- [Future of Life Institute](https://futureoflife.org/cause-area/artificial-intelligence/) startte de [open brief](https://futureoflife.org/open-letter/pause-giant-ai-experiments/), geleid door Max Tegmark.
- [Center for AI Safety](https://www.safe.ai/) (CAIS) is een onderzoekscentrum aan de Tsjechische Technische Universiteit in Praag, geleid door Dan Hendrycks.
- [Conjecture](https://www.conjecture.dev/). Start-up die werkt aan AI-alignering en AI-beleid, geleid door Connor Leahy.
- [Existential Risk Observatory](https://existentialriskobservatory.org/). Nederlandse organisatie die het publiek informeert over x-risico's en communicatiestrategieën bestudeert.
- [Centre for the Governance of AI](https://www.governance.ai/)
- [FutureSociety](https://thefuturesociety.org/about-us/)
- [Center for Human-Compatible Artificial Intelligence](https://humancompatible.ai/about/) (CHAI), geleid door Stuart Russell.
- [Machine Intelligence Research Institute](https://intelligence.org/) (MIRI), doet wiskundig onderzoek naar AI-veiligheid, geleid door Eliezer Yudkowsky.
- [Institute for AI Policy and Strategy](https://www.iaps.ai/) (IAPS)
- [The AI Policy Institute](https://theaipi.org/)
- [AI Safety Communications Centre](https://aiscc.org/2023/11/01/yougov-poll-83-of-brits-demand-companies-prove-ai-systems-are-safe-before-release/)
- [The Midas Project](https://www.themidasproject.com/) Bedrijfscampagnes voor AI-veiligheid.
- [The Human Survival Project](https://thehumansurvivalproject.org/)
- [AI Safety World](https://aisafety.world/) Hier is een overzicht van het AI-veiligheidslandschap.

## Als je overtuigd bent en actie wilt ondernemen {#if-you-are-convinced-and-want-to-take-action}

Er zijn veel [dingen die je kunt doen](/action).
Een brief schrijven, naar een protest gaan, wat geld doneren of lid worden van een gemeenschap is niet zo moeilijk!
En deze acties hebben een echte impact.
Zelfs als we tegen het einde van de wereld aan zitten, kan er nog steeds hoop zijn en zeer lonend werk worden gedaan.

## Of als je nog steeds niet helemaal zeker bent {#or-if-you-still-dont-feel-quite-sure-of-it}

Leren over de [psychologie van x-risico](/psychology-of-x-risk) kan je helpen.
