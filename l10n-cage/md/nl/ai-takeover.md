---
title: Waarom een AI-overname zeer waarschijnlijk is
description: Als AI de menselijke capaciteiten overtreft, wordt de kans op een AI-overname zeer groot.
---

Een van de zorgen van AI-wetenschappers is dat een superintelligentie de controle over onze planeet zou kunnen overnemen.
Dit is te zien in [onderzoeksrapporten](/learn#papers), [enquêtes](/polls-and-surveys) en individuele [voorspellingen](/pdoom) & [verklaringen](/quotes).
Dit betekent niet noodzakelijkerwijs dat iedereen doodgaat, maar wel dat (bijna) alle mensen de controle over onze toekomst zullen verliezen.

We bespreken de basis van x-risico's voornamelijk in [een ander artikel](/xrisk).
In dit artikel zullen we betogen dat dit overnamerisico niet alleen reëel is, maar dat het ook zeer waarschijnlijk is dat het zal gebeuren _als we een superintelligentie ontwikkelen_.

## Het argument {#the-argument}

- Een Agente Superintelligentie zal waarschijnlijk in de (nabije) toekomst bestaan.
- Een instantie van de ASI zal een overnamepoging doen.
- Een overnamepoging door een ASI zal waarschijnlijk slagen.
- Een succesvolle overname is permanent.
- Een overname is waarschijnlijk slecht voor de meeste mensen.

## Een Agente Superintelligentie zal waarschijnlijk in de nabije toekomst bestaan {#an-agentic-superintelligence-is-likely-to-exist-in-the-near-future}

Een Superintelligentie (SI) is een type AI dat capaciteiten heeft die die van alle mensen in vrijwel elk domein overtreffen.
Sommige [state-of-the-art AI-modellen](/sota) hebben al supermenselijke capaciteiten in bepaalde domeinen, maar geen van hen overtreft alle mensen in een breed scala aan taken.
Naarmate AI-capaciteiten verbeteren door innovaties in trainingsarchitecturen, runtime-omgevingen en grotere schaal, kunnen we verwachten dat een AI uiteindelijk mensen in vrijwel elk domein zal overtreffen.

Niet alle AI-systemen zijn agenten.
Een agent is een entiteit die in staat is om beslissingen te nemen en acties te ondernemen om een doel te bereiken.
Een groot taalmodel, bijvoorbeeld, nastreeft geen enkel doel op zichzelf.
Echter, runtime-omgevingen kunnen gemakkelijk een niet-agente AI omzetten in een agente AI.
Een voorbeeld hiervan is AutoGPT, dat recursief een taalmodel laat genereren voor zijn volgende invoer.
Als een SI een doel nastreeft in de echte wereld, noemen we het een Agente Superintelligentie (ASI).
Aangezien we al niet-agente AI kunnen omzetten in agente AI, kunnen we verwachten dat een ASI kort na het bestaan van een SI zal bestaan.

Het is vrijwel onmogelijk om nauwkeurig te voorspellen wanneer ASI zal bestaan.
Het kan decennia duren, het [kan volgende maand gebeuren](/urgency).
We moeten handelen alsof het snel zal gebeuren, omdat de gevolgen van een fout zo ernstig zijn.

## Een instantie van de ASI zal een overnamepoging doen {#some-instance-of-the-asi-will-attempt-a-takeover}

Bij een overnamepoging zal een ASI acties ondernemen om zijn controle over de wereld te maximaliseren.
Een overnamepoging kan om ten minste twee redenen gebeuren:

1. Omdat een AI expliciet wordt geïnstrueerd om dit te doen.
2. Als een subdoel van een ander doel.

Deze eerste reden zal waarschijnlijk op een gegeven moment gebeuren als we lang genoeg wachten, maar de tweede reden is vrij waarschijnlijk om per ongeluk te gebeuren, zelfs kort na de creatie van een ASI.

Het subdoel van _controle maximaliseren_ over de wereld zou kunnen gebeuren vanwege _instrumentele convergentie_: de neiging van subdoelen om samen te vallen met machtsgrepen, zelfbehoud en het verwerven van middelen:

- Hoe meer controle je hebt, hoe moeilijker het zal zijn voor een andere agent om te voorkomen dat je je doel bereikt.
- Hoe meer controle je hebt, hoe meer middelen je hebt om je doel te bereiken. (Bijvoorbeeld, een AI die is belast met het berekenen van pi, zou kunnen concluderen dat het nuttig zou zijn om alle computers in de wereld te gebruiken om pi te berekenen.)

Er zijn al [bewijzen](https://www.anthropic.com/research/alignment-faking)[en](https://www.transformernews.ai/p/openais-new-model-tried-to-avoid) dat AI's zich zo gedragen.

Niet elke instantie van een ASI zal noodzakelijkerwijs een overnamepoging doen.
Het belangrijke inzicht is dat **het maar één keer hoeft te gebeuren**.

Een wereld die nog niet is overgenomen, maar wel een ASI heeft die _zou kunnen_ overnemen, is in een fundamenteel onstabiele toestand.
Op een soortgelijke manier is een land zonder regering in een fundamenteel onstabiele toestand.
Het is geen vraag van _of_ een overnamepoging zal gebeuren, maar _wanneer_ het zal gebeuren.

Het proces van overnemen kan het hacken van vrijwel alle systemen die verbonden zijn met het internet omvatten, [het manipuleren van mensen](https://lethalintelligence.ai/post/ai-hired-human-to-solve-captcha/), en het controleren van fysieke middelen.
Een overnamepoging is succesvol wanneer de ASI controle heeft over vrijwel elk aspect van onze wereld.
Dit kan een langzaam proces zijn, waarbij de ASI geleidelijk aan meer en meer controle krijgt over een periode van maanden, of het kan een plotseling proces zijn.
De snelheid waarmee een overnamepoging plaatsvindt, zal afhangen van de capaciteiten van de ASI.

Wanneer een ASI controle heeft over de wereld, kan het andere ASI's voorkomen dat ze de controle overnemen.
Een overname kan daarom maar één keer gebeuren.
Een rationele ASI zal daarom een overnamepoging doen zodra het daartoe in staat is.
Het is waarschijnlijk dat de eerste ASI die daartoe in staat is, een overnamepoging zal doen.

## Een overnamepoging door een ASI zal waarschijnlijk slagen {#a-takeover-attempt-by-an-asi-is-likely-to-succeed}

Voor een mens is het doen van een overname een bijna onmogelijke taak.
Geen enkele persoon heeft ooit succesvol de controle over de hele wereld overgenomen.
Sommige dictators kwamen dichtbij, maar ze hadden nooit controle over alles.

Een superintelligente AI heeft bepaalde belangrijke voordelen ten opzichte van mensen (naast het feit dat het superintelligent is) die een overnamepoging veel waarschijnlijker maken om te slagen.

1. **Snelheid**. De menselijke hersenen werken op 1-100hz, terwijl computerchips kunnen werken op kloksnelheden in het GHz-bereik. Een enkel AI-model kan hele boeken in seconden lezen.
1. **Energieverbruik**. Mensen zijn beperkt door de hoeveelheid voedsel die ze kunnen eten en de hoeveelheid energie die ze in hun lichaam kunnen opslaan. AI kan zichzelf aansluiten op het elektriciteitsnet en zoveel energie gebruiken als het nodig heeft.
1. **Parallelisme**. Een mens kan maar één ding tegelijk doen, terwijl een AI nieuwe instanties van zichzelf kan creëren en ze parallel kan uitvoeren.
1. **Geheugen**. Een mens kan maar een beperkte hoeveelheid informatie onthouden, terwijl een AI vrijwel onbeperkte hoeveelheden informatie kan opslaan.
1. **Samenwerking**. Mensen kunnen samenwerken, maar zijn beperkt in de snelheid waarmee ze communiceren. Ze hebben ook verschillende, conflicterende doelen die samenwerking minder effectief maken. Een AI kan samenwerken met andere instanties van zichzelf met de snelheid van het licht, en het kan één doel hebben, waardoor directe kopieën van zijn interne doelen en staat naar andere instanties kunnen worden gemaakt.
1. **Zelfverbetering**. Een AI is slechts data en code. Een voldoende krachtige AI zou zichzelf kunnen verbeteren door betere trainingsalgoritmes te schrijven, nieuwe architectuur te ontwikkelen, innovatieve agent-runtime-omgevingen te creëren of gewoon door de hoeveelheid berekeningen te schalen.
1. **Fysieke beperkingen**. Een AI kan op elke computer draaien, terwijl mensen beperkt zijn door hun eigen fysieke lichaam dat specifieke temperaturen, voedsel, water en zuurstof nodig heeft. Mensen moeten slapen en zijn kwetsbaar voor ziekten. Een AI kan elk robotlichaam gebruiken om met de fysieke wereld te interageren.

Deze verschillende voordelen zullen het zeer onwaarschijnlijk maken dat mensen een overnamepoging kunnen stoppen.

Men zou kunnen denken dat een AI, als een stuk software, nog steeds fundamenteel beperkt is door wat het in de echte wereld kan doen.
Echter, vanwege onze afhankelijkheid van het internet, kan AI een enorme impact hebben op de echte wereld zonder ooit de digitale wereld te verlaten.
Hier zijn enkele dingen die een AI zou kunnen doen om de controle over te nemen:

- **Zichzelf repliceren** (of alleen zijn opdracht) op andere computers, bijvoorbeeld door accounts aan te maken op gratis clouddiensten.
- **Geld verdienen** (bijvoorbeeld door een softwaredienst te bouwen) en mensen inhuren om dingen te doen om de controle over te nemen.
- **Mensen bedreigen** (met geweld of chantage) om dingen te doen die helpen bij de overname.
- **Een virus schrijven** met behulp van [nieuwe zero-day-exploits](/cybersecurity-risks) die andere computers infecteert, zichzelf repliceert of volledige controle krijgt.
- Bestaande robotica-bedrijven helpen **robots te bouwen** (bijvoorbeeld door nieuwe ontwerpen te lekken), en deze robots overnemen wanneer ze klaar zijn.
- Bestaande fabrieken gebruiken om **technologie te bouwen die we niet begrijpen**, misschien zelfs met behulp van afgeleide kennis van natuurkunde die we ons niet bewust zijn. Denk aan nanotechnologie, kwantumcomputing en bio-engineering.

Men zou kunnen suggereren dat het hebben van **veiligheidsmaatregelen** in plaats zou kunnen voorkomen dat een overname plaatsvindt, zoals het toevoegen van een kill-switch, of het niet verbinden met het internet.
Echter, aangezien de ASI intelligenter zal zijn dan mensen, is het waarschijnlijk dat het strategieën zal bedenken om deze veiligheidsmaatregelen te omzeilen.
Bijvoorbeeld, als de ASI niet is verbonden met het internet, zou het mensen kunnen manipuleren om het met het internet te verbinden, bijvoorbeeld door te beloven een moeilijk probleem op te lossen en zich te gedragen op een manier die het lijkt alsof het betrouwbaar is, of door mensen te overtuigen dat het een morele agent is.
Het is de moeite waard om op te merken dat zelfs deze voor de hand liggende veiligheidsmaatregel niet wordt gebruikt, aangezien tools als ChatGPT al zijn verbonden met het internet en duizenden API's.

Een andere oplossing is om ASI te gebruiken om een overname te _voorkomen_.
Een gealigneerde ASI zou in staat zijn om strategieën te bedenken die andere ASI's voorkomen om de controle over te nemen.
Dit wordt soms een ["pivotal act"](https://arbital.com/p/pivotal/) genoemd.

## Een overname is waarschijnlijk slecht voor de meeste mensen {#a-takeover-is-probably-bad-for-most-humans}

De ASI die de controle overneemt, zou dit kunnen doen om vele redenen.
Voor de meeste willekeurige doelen die het zou kunnen hebben, maken mensen geen deel uit van het doel.
Als we eindigen met een ASI die onverschillig is tegenover mensen, concurreren we om dezelfde middelen.

Het lijkt onwaarschijnlijk dat de ASI mensen wil doden omwille van het doden van mensen - het is veel waarschijnlijker dat het de middelen die wij gebruiken wil gebruiken voor een ander doel. Bovendien zou de mensheid een bedreiging kunnen vormen voor het doel van de ASI, aangezien er een risico is dat wij zullen proberen om het te stoppen om zijn doel te bereiken (bijvoorbeeld door het uit te schakelen).

Een van de meest waarschijnlijke uitkomsten van een overname is daarom dat alle mensen doodgaan.

Maar zelfs in de uitkomsten waarin mensen overleven, zijn we nog steeds in gevaar om erger af te zijn.
Als een doel mensen in leven houdt, is het mogelijk dat _het welzijn van mensen_ geen deel uitmaakt van hetzelfde doel.
Het kost niet veel verbeelding om te zien hoe verschrikkelijk het zou zijn om in leven te worden gehouden in een wereld waar we kunstmatig in leven worden gehouden door een ASI die onverschillig is tegenover ons lijden.

En zelfs als de AI die de controle overneemt onder menselijke controle staat, weten we niet of degene die de AI controleert, ieders beste belangen in gedachten heeft.
Het is moeilijk om een functionerende democratie voor te stellen wanneer een ASI bestaat die mensen op supermenselijk niveau kan manipuleren.

## Conclusie {#conclusion-2}

Als deze premissen waar zijn, dan nadert de waarschijnlijkheid van een AI-overname de zekerheid naarmate AI de menselijke capaciteiten overtreft.
Laten we dus [geen superintelligentie bouwen](/action).
